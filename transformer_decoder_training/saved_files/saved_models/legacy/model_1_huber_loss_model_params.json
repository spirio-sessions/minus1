{"model_topology": "Transformer(\n  (embedding): Linear(in_features=24, out_features=256, bias=True)\n  (pos_emb): SinusoidalPosEmb()\n  (blocks): ModuleList(\n    (0-7): 8 x TransformerBlock(\n      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (multihead_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n      )\n      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=256, out_features=1024, bias=True)\n        (1): ELU(alpha=1.0)\n        (2): Linear(in_features=1024, out_features=256, bias=True)\n      )\n    )\n  )\n  (fc_out): Linear(in_features=256, out_features=12, bias=True)\n)", "num_emb": 24, "hidden_size": 256, "num_layers": 8, "num_heads": 8, "training_params": {"learning_rate": 0.001, "num_epochs": 25, "optimizer": "Adam", "loss_fn": "SmoothL1Loss"}, "training_data_params": {"sos_token": [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], "pad_token": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], "snapshot_intervall": 0.05, "batch_size": 64, "sequence_length": 512, "stride": 256}}