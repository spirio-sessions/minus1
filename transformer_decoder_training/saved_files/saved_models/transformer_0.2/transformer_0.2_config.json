{
    "model_project_name": "transformer_0.2",
    "model_params": {
        "model_topology": "Transformer(\n  (embedding): Linear(in_features=24, out_features=512, bias=True)\n  (pos_emb): SinusoidalPosEmb()\n  (blocks): ModuleList(\n    (0-7): 8 x TransformerBlock(\n      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (multihead_attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n      )\n      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=512, out_features=2048, bias=True)\n        (1): ELU(alpha=1.0)\n        (2): Linear(in_features=2048, out_features=512, bias=True)\n      )\n    )\n  )\n  (fc_out): Linear(in_features=512, out_features=24, bias=True)\n)",
        "num_emb": 24,
        "hidden_size": 512,
        "num_layers": 8,
        "num_heads": 16,
        "output_dim": null
    },
    "training_params": {
        "learning_rate": 0.0001,
        "num_epochs": 5,
        "optimizer": "Adam",
        "loss_fn": "MultiLabelSoftMarginLoss"
    },
    "training_data_params": {
        "sos_token": [
            [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1
            ]
        ],
        "pad_token": [
            [
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2,
                2
            ]
        ],
        "snapshot_interval": 0.1,
        "batch_size": 40,
        "sequence_length": 100,
        "stride": 16,
        "test_size": 0.2
    }
}