{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Jetzt nochmal ein Decoder only ohne tokenization zu int mit hoffentlich korrekter inferenz\n",
   "id": "24c38489ff10921a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:59:37.905025Z",
     "start_time": "2024-06-25T18:59:37.349584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from data_preperation import dataset_snapshot\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ],
   "id": "bcb65d87ee74abdb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# prepare data",
   "id": "23765b3ac55ffd27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:59:37.939564Z",
     "start_time": "2024-06-25T18:59:37.905620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "7fa52c1cc923a11e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:59:38.064173Z",
     "start_time": "2024-06-25T18:59:37.940995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set Special tokens, chunk size, etc.\n",
    "sos_token = np.full((1, 88), 1)\n",
    "pad_token = np.full((1, 88), 2)\n",
    "pad_token = torch.tensor(pad_token, device=device)\n",
    "\n",
    "seq_length = 512\n",
    "batch_size = 64"
   ],
   "id": "4c76896becf6ab2e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:59:39.788577Z",
     "start_time": "2024-06-25T18:59:38.065773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformer_decoder_training.dataprep_transformer import dataprep_1\n",
    "from sklearn.model_selection import train_test_split\n",
    "# prepare data for dataset\n",
    "\n",
    "#load data\n",
    "dataset_as_snapshots = dataset_snapshot.process_dataset_multithreaded(\"/home/falaxdb/Repos/minus1/datasets/maestro_v3_split/hands_split_into_seperate_midis\", 0.05, amount=20)\n",
    "# filter snapshots to 88 piano notes\n",
    "dataset_as_snapshots = dataset_snapshot.filter_piano_range(dataset_as_snapshots)\n",
    "\n",
    "# Convert data into Chunks and add special tokens\n",
    "data = dataprep_1.prepare_dataset(dataset_as_snapshots, seq_length, seq_length, sos_token)\n",
    "\n",
    "print(\"Ammount of sequence pairs:\", len(data))\n",
    "\n",
    "# Split the dataset using sklearn while maintaining pairs\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42, shuffle=True)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, shuffle=True)"
   ],
   "id": "20e83bfc1b5fcdb1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed dataset (40/40): 100%|██████████| 40/40 [00:01<00:00, 29.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40 of 40 files\n",
      "Ammount of sequence pairs: 349\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:59:39.832331Z",
     "start_time": "2024-06-25T18:59:39.789100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformer_decoder_training.dataset_transformer.dataset_1 import PianoDataset\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = PianoDataset(train_data)\n",
    "val_dataset = PianoDataset(val_data)\n",
    "test_dataset = PianoDataset(test_data)\n",
    "\n",
    "# Create DataLoaders for each subset with drop_last=True\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# Iterate over the DataLoader (example with train_loader)\n",
    "for batch in train_loader:\n",
    "    X, y = batch\n",
    "    print(X.shape, y.shape)\n",
    "    # X and y should both have shape (batch_size, chunk_size + 1, feature dimension) because of SOS (and EOS) tokens"
   ],
   "id": "5facbd248fd5eb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Initialize model",
   "id": "e92d52061114bca6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:59:39.834604Z",
     "start_time": "2024-06-25T18:59:39.833007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set parameters\n",
    "\n",
    "# Learning rate for the optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Number of epochs for training\n",
    "nepochs = 20\n",
    "\n",
    "# Embedding Size\n",
    "hidden_size = 256\n",
    "\n",
    "# Number of transformer blocks\n",
    "num_layers = 8\n",
    "\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 8"
   ],
   "id": "bfbeb949c8aaa4d2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:59:40.158280Z",
     "start_time": "2024-06-25T18:59:39.835056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformer_decoder_training.models.transformer_decoder_1 import Transformer\n",
    "\n",
    "# Create model\n",
    "# (num emb = wie viele verschiedene Tokens es geben kann bei 12 Tönen 2 ** 12 Möglichkeiten + 2 special tokens)\n",
    "# num_emb: Da ja keine int indexe mehr -> wahrscheinlich 88 wegen 88 Keys\n",
    "tf_generator = Transformer(num_emb=88, num_layers=num_layers, \n",
    "                           hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(tf_generator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.BCELoss"
   ],
   "id": "b4645664ec62fb7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:59:40.161057Z",
     "start_time": "2024-06-25T18:59:40.158973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check number of model parameters\n",
    "num_model_params = 0\n",
    "for param in tf_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ],
   "id": "d8b97651371e006a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-This Model Has 6363480 (Approximately 6 Million) Parameters!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "1891247df0d89ca1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:59:40.176087Z",
     "start_time": "2024-06-25T18:59:40.161442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader, pad_token, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Move data to GPU\n",
    "        input_melody, expected_harmony = batch\n",
    "        input_melody, expected_harmony = input_melody.to(device), expected_harmony.to(device)\n",
    "        \n",
    "        # shift input und output für das training zeug mit start token\n",
    "        input_melody = input_melody[:, 0:-1]\n",
    "        expected_harmony = expected_harmony[:, 1:]\n",
    "        \n",
    "        # Generate predictions\n",
    "        pred = tf_generator(input_melody, pad_token)\n",
    "        \n",
    "        print(\"Prediction shape:\", pred.shape)\n",
    "        print(pred)\n",
    "        print(\"expected harmony_shape:\", expected_harmony.shape)\n",
    "        print(expected_harmony)\n",
    "        \n",
    "        # Calculate loss with masked cross-entropy\n",
    "        # ich glaube 0 steht in vorlage für padding token index -> habe ich hier anders\n",
    "        #mask = (expected_harmony != pad_token).float() Maske verwenden, um Padding positions im output zu canceln\n",
    "        # masked_pred = pred * mask\n",
    "        loss = loss_fn(pred, expected_harmony)\n",
    "        \n",
    "        # Backpropagation\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "1f09bcbc20cb6ee4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:59:40.539631Z",
     "start_time": "2024-06-25T18:59:40.177276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_loop(tf_generator, optimizer, loss_fn, train_loader, pad_token, device)\n",
    "    end_time = timer()\n",
    "    # val_loss = validation_loop(model, loss_fn, val_loader)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ],
   "id": "9fc6d5036b353801",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: torch.Size([64, 512, 88])\n",
      "tensor([[[0.1289, 0.3501, 0.3525,  ..., 0.6268, 0.2488, 0.5487],\n",
      "         [0.2162, 0.4370, 0.5057,  ..., 0.4988, 0.3200, 0.6353],\n",
      "         [0.2042, 0.4149, 0.4802,  ..., 0.4825, 0.2878, 0.5799],\n",
      "         ...,\n",
      "         [0.1934, 0.4509, 0.8172,  ..., 0.5359, 0.3302, 0.4925],\n",
      "         [0.1756, 0.4465, 0.7997,  ..., 0.5464, 0.2746, 0.4799],\n",
      "         [0.1554, 0.4074, 0.7583,  ..., 0.5710, 0.2339, 0.4577]],\n",
      "\n",
      "        [[0.1661, 0.3225, 0.4541,  ..., 0.5199, 0.2542, 0.4806],\n",
      "         [0.2098, 0.4675, 0.5503,  ..., 0.5082, 0.2270, 0.5916],\n",
      "         [0.1660, 0.4058, 0.6206,  ..., 0.4679, 0.2460, 0.5936],\n",
      "         ...,\n",
      "         [0.2120, 0.4627, 0.8209,  ..., 0.5080, 0.3571, 0.4751],\n",
      "         [0.1888, 0.4437, 0.8029,  ..., 0.5497, 0.2998, 0.4663],\n",
      "         [0.1697, 0.3971, 0.7599,  ..., 0.5619, 0.2564, 0.4448]],\n",
      "\n",
      "        [[0.1188, 0.3945, 0.3913,  ..., 0.6225, 0.2506, 0.4615],\n",
      "         [0.2046, 0.4354, 0.4853,  ..., 0.4756, 0.3760, 0.4416],\n",
      "         [0.1666, 0.4223, 0.4970,  ..., 0.4149, 0.2691, 0.6080],\n",
      "         ...,\n",
      "         [0.2035, 0.3964, 0.7946,  ..., 0.5484, 0.3188, 0.5159],\n",
      "         [0.1851, 0.3937, 0.7748,  ..., 0.5647, 0.2617, 0.4998],\n",
      "         [0.1661, 0.3468, 0.7270,  ..., 0.5787, 0.2200, 0.4769]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1637, 0.3503, 0.3923,  ..., 0.6521, 0.3477, 0.5376],\n",
      "         [0.1716, 0.4055, 0.5534,  ..., 0.4065, 0.3241, 0.5913],\n",
      "         [0.1882, 0.3728, 0.5341,  ..., 0.4047, 0.2317, 0.5858],\n",
      "         ...,\n",
      "         [0.2080, 0.4286, 0.8056,  ..., 0.5594, 0.3250, 0.4997],\n",
      "         [0.1881, 0.4411, 0.7973,  ..., 0.5512, 0.3002, 0.4697],\n",
      "         [0.1676, 0.4028, 0.7437,  ..., 0.5788, 0.2464, 0.4411]],\n",
      "\n",
      "        [[0.1334, 0.3103, 0.4123,  ..., 0.5954, 0.2736, 0.5679],\n",
      "         [0.2374, 0.4038, 0.6448,  ..., 0.3840, 0.3030, 0.5376],\n",
      "         [0.2079, 0.3340, 0.5305,  ..., 0.3913, 0.2786, 0.5609],\n",
      "         ...,\n",
      "         [0.2084, 0.4299, 0.8076,  ..., 0.5149, 0.3785, 0.4787],\n",
      "         [0.1889, 0.4432, 0.8019,  ..., 0.5500, 0.2999, 0.4529],\n",
      "         [0.1644, 0.3993, 0.7581,  ..., 0.5688, 0.2596, 0.4380]],\n",
      "\n",
      "        [[0.2158, 0.2418, 0.3668,  ..., 0.4913, 0.2648, 0.5448],\n",
      "         [0.2060, 0.4320, 0.5256,  ..., 0.3978, 0.2231, 0.5506],\n",
      "         [0.2091, 0.3738, 0.4778,  ..., 0.3702, 0.2459, 0.5171],\n",
      "         ...,\n",
      "         [0.2057, 0.4542, 0.8141,  ..., 0.5216, 0.3402, 0.5101],\n",
      "         [0.1883, 0.4490, 0.7947,  ..., 0.5306, 0.2845, 0.4850],\n",
      "         [0.1663, 0.4052, 0.7522,  ..., 0.5582, 0.2449, 0.4699]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "expected harmony_shape: torch.Size([64, 512, 88])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, NUM_EPOCHS\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m      5\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[0;32m----> 6\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtf_generator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpad_token\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m# val_loss = validation_loop(model, loss_fn, val_loader)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[9], line 26\u001B[0m, in \u001B[0;36mtrain_loop\u001B[0;34m(model, opt, loss_fn, dataloader, pad_token, device)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(expected_harmony)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Calculate loss with masked cross-entropy\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# ich glaube 0 steht in vorlage für padding token index -> habe ich hier anders\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m#mask = (expected_harmony != pad_token).float() Maske verwenden, um Padding positions im output zu canceln\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# masked_pred = pred * mask\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mloss_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpected_harmony\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[1;32m     29\u001B[0m opt\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/Repos/minus1/venv/lib64/python3.12/site-packages/torch/nn/modules/loss.py:615\u001B[0m, in \u001B[0;36mBCELoss.__init__\u001B[0;34m(self, weight, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m    614\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, weight: Optional[Tensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, size_average\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, reduce\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, reduction: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 615\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize_average\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Repos/minus1/venv/lib64/python3.12/site-packages/torch/nn/modules/loss.py:30\u001B[0m, in \u001B[0;36m_WeightedLoss.__init__\u001B[0;34m(self, weight, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, weight: Optional[Tensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, size_average\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, reduce\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, reduction: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msize_average\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregister_buffer(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweight\u001B[39m\u001B[38;5;124m'\u001B[39m, weight)\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight: Optional[Tensor]\n",
      "File \u001B[0;32m~/Repos/minus1/venv/lib64/python3.12/site-packages/torch/nn/modules/loss.py:23\u001B[0m, in \u001B[0;36m_Loss.__init__\u001B[0;34m(self, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 23\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduction: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlegacy_get_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize_average\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduction \u001B[38;5;241m=\u001B[39m reduction\n",
      "File \u001B[0;32m~/Repos/minus1/venv/lib64/python3.12/site-packages/torch/nn/_reduction.py:35\u001B[0m, in \u001B[0;36mlegacy_get_string\u001B[0;34m(size_average, reduce, emit_warning)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     33\u001B[0m     reduce \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mand\u001B[39;00m reduce:\n\u001B[1;32m     36\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m reduce:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
