{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Versuch 2\n",
    "\n",
    "Ich versuche jetzt ein Transformer-decoder only zu Trainieren, darauf trainiert wird immer nur ein Token basierend auf einer vorhergehenden sequenz vorherzusagen\n",
    "\n",
    "bin mir noch nicht ganz sicher, ob alles passt mit der Architektur\n",
    "\n",
    "- bin mir nicht sicher, ob Casual mask und Padding mask funktionieren\n",
    "\n",
    "\n",
    "Idee:\n",
    "- Transformer soll auf einer Eingabesquenz nur ein nächstes Token vorhersagen.\n",
    "    - Bei Inferenz wird dann das ausgegeben token wieder an den input hinten drangehängt\n",
    "    - So lange machen, bis gewünschte sequenz erreicht ist\n",
    "- Daten:\n",
    "    - Bei Training wird als input die Rechte und linke hand genommen\n",
    "    - Loss wird dann auf den nächsten token von nur der rechten Hand berechnet\n",
    "    - Modell soll quasi aus bisher gespielter Melodie + Harmony das nächste harmony-token vorhersagen.\n",
    "    - Sequenzen sind fast immer gleich lang"
   ],
   "id": "ce1fbb5b86acc10f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 0 import",
   "id": "e9e57105a14e8a9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:02:18.030291Z",
     "start_time": "2024-06-22T17:02:18.028342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from data_preperation import dataset_snapshot\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "99cf62d8e008400",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:02:18.135297Z",
     "start_time": "2024-06-22T17:02:18.133140Z"
    }
   },
   "cell_type": "code",
   "source": "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "id": "2317db75e37cd359",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1 Data preperation",
   "id": "afbb21b47435d60e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "special start token",
   "id": "4676a9e23968b885"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:02:18.143579Z",
     "start_time": "2024-06-22T17:02:18.137363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define SOS token as global variables\n",
    "SOS_TOKEN = np.full((1, 88), 1)  # SOS token representation with ones (adjust for feature dimension)"
   ],
   "id": "7e81532050a9e19e",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "load datatset",
   "id": "950c726056a96621"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:02:19.371072Z",
     "start_time": "2024-06-22T17:02:18.144405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create snapshots\n",
    "dataset_as_snapshots = dataset_snapshot.process_dataset_multithreaded(\"/home/falaxdb/Repos/minus1/datasets/maestro_v3_split/hands_split_into_seperate_midis\", 0.1, amount=10)\n",
    "# filter snapshots to 88 piano notes\n",
    "dataset_as_snapshots = dataset_snapshot.filter_piano_range(dataset_as_snapshots)\n",
    "# compress data into one octave\n",
    "# dataset_as_snapshots =  dataset_snapshot.compress_existing_dataset_to_12keys(dataset_as_snapshots)\n",
    "\n",
    "for song in dataset_as_snapshots:\n",
    "    print(\"song:\")\n",
    "    for track in song:\n",
    "        print(track.shape)"
   ],
   "id": "d0b04d2bdf0a3420",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "batchen, splitten, etc",
   "id": "8a6ec102fb37623c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:02:19.375780Z",
     "start_time": "2024-06-22T17:02:19.372019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "\n",
    "# Function to add SOS token at the start of each song and create sequences\n",
    "# inputs are right and left hand combined, target is the one snapshot of the left hand\n",
    "def create_sequences_with_sos(data, seq_length):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for song in data: \n",
    "        right_h = song[0]\n",
    "        left_h = song[1]\n",
    "        \n",
    "        assert right_h.shape == left_h.shape, \"Tracks must have same dimensions\"\n",
    "        \n",
    "        combined_tracks = np.bitwise_xor(right_h, left_h)\n",
    "        \n",
    "        combined_tracks = np.vstack((sos_token, combined_tracks))\n",
    "        \n",
    "        for i in range(len(right_h) - seq_length):\n",
    "            inputs.append(combined_tracks[i:i + seq_length])\n",
    "            targets.append(left_h[i + seq_length])\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "\n",
    "\n",
    "# Dataset class\n",
    "class PianoDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.inputs[idx]\n",
    "        target_seq = self.targets[idx]\n",
    "        return {\n",
    "            'input_seq': torch.tensor(input_seq, dtype=torch.float32),\n",
    "            'target_seq': torch.tensor(target_seq, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Collate function to handle padding and creating attention masks\n",
    "def collate_fn(batch):\n",
    "    inputs = [item['input_seq'] for item in batch]\n",
    "    targets = [item['target_seq'] for item in batch]\n",
    "    \n",
    "    # Pad sequences to the same length in the batch dimension\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=padding_value)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=padding_value)\n",
    "    \n",
    "    # Create the attention mask\n",
    "    # inputs_padded: (batch_size, max_seq_length, feature_dim)\n",
    "    attention_mask = (inputs_padded != padding_value).float()\n",
    "    \n",
    "    # Reduce attention_mask to (batch_size, max_seq_length) by checking any padding in the feature_dim dimension\n",
    "    attention_mask = attention_mask.any(dim=-1).float()\n",
    "    \n",
    "    return {\n",
    "        'input_seq': inputs_padded,\n",
    "        'target_seq': targets_padded,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "\n",
    "    "
   ],
   "id": "a0d2cb9166f3fa3d",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:02:19.702456Z",
     "start_time": "2024-06-22T17:02:19.376389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sos_token = SOS_TOKEN\n",
    "padding_value = -1  # Use -1 for padding\n",
    "\n",
    "# Parameters\n",
    "seq_length = 100  # Example sequence length\n",
    "\n",
    "# Split the dataset using sklearn while maintaining pairs\n",
    "train_data, temp_data = train_test_split(dataset_as_snapshots, test_size=0.3, random_state=42, shuffle=True)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Create sequences for right and left hand data with SOS token\n",
    "train_inputs, train_targets = create_sequences_with_sos(train_data, seq_length)\n",
    "val_inputs, val_targets = create_sequences_with_sos(val_data, seq_length)\n",
    "test_inputs, test_targets = create_sequences_with_sos(test_data, seq_length)\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = PianoDataset(train_inputs, train_targets)\n",
    "val_dataset = PianoDataset(val_inputs, val_targets)\n",
    "test_dataset = PianoDataset(test_inputs, test_targets)\n",
    "\n",
    "# Create DataLoader with collate_fn\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ],
   "id": "48bd6798964b7b35",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:02:20.089081Z",
     "start_time": "2024-06-22T17:02:19.702954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_tensors(input_tensor, target_tensor, attention_mask):\n",
    "    \"\"\"\n",
    "    Visualizes the input tensor, target tensor, and attention mask.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_tensor: torch.Tensor of shape (seq_length, feature_dim)\n",
    "    - target_tensor: torch.Tensor of shape (feature_dim)\n",
    "    - attention_mask: torch.Tensor of shape (seq_length)\n",
    "    \"\"\"\n",
    "    input_np = input_tensor.numpy().T  # Transpose to get features on the y-axis\n",
    "    target_np = target_tensor.numpy()\n",
    "    attention_mask_np = attention_mask.numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "    axes[0].imshow(input_np, cmap='binary', aspect='auto')\n",
    "    axes[0].set_title('Input Tensor')\n",
    "    axes[0].set_xlabel('Snapshots')\n",
    "    axes[0].set_ylabel('Features')\n",
    "    \n",
    "    axes[1].imshow(target_np.reshape(-1, 1), cmap='binary', aspect='auto')\n",
    "    axes[1].set_title('Target Tensor')\n",
    "    axes[1].set_xlabel('Target Snapshot')\n",
    "    axes[1].set_yticks(np.arange(target_np.shape[0]))\n",
    "    axes[1].set_yticklabels(np.arange(target_np.shape[0]))\n",
    "    axes[1].set_ylabel('Features')\n",
    "    \n",
    "    axes[2].imshow(attention_mask_np.reshape(1, -1), cmap='binary', aspect='auto')\n",
    "    axes[2].set_title('Attention Mask')\n",
    "    axes[2].set_xlabel('Snapshots')\n",
    "    axes[2].set_yticks([])  # Hide y-axis\n",
    "\n",
    "    plt.tight_layout(pad=4.0)\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of data\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Extract the first example from the batch\n",
    "input_tensor = batch['input_seq'][0]\n",
    "target_tensor = batch['target_seq'][0]\n",
    "attention_mask = batch['attention_mask'][0]\n",
    "\n",
    "print(input_tensor)\n",
    "print(target_tensor)\n",
    "print(attention_mask)\n",
    "\n",
    "# Visualize the example\n",
    "visualize_tensors(input_tensor, target_tensor, attention_mask)"
   ],
   "id": "9909c472efe3e8ee",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2 Define model",
   "id": "cfc1b24301156835"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "positional and mask",
   "id": "4988f0422572a37a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:02:20.093591Z",
     "start_time": "2024-06-22T17:02:20.089835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Encoding - From formula -> This is basically applying the formula for Positional encoding (The one with Sinus and Cosinus)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # Baically a positions list 0, 1, 2, 3, 4, 5, ...\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        #  # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "         # Saving buffer (same as parameter without gradients needed)\n",
    "        # pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # Shape: (1, max_len, d_model) Änderung evtl. wegen batch first = True\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Residual connection + pos encoding\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    \n",
    "    # EX for size=5:\n",
    "    # [[0., -inf, -inf, -inf, -inf],\n",
    "    #  [0.,   0., -inf, -inf, -inf],\n",
    "    #  [0.,   0.,   0., -inf, -inf],\n",
    "    #  [0.,   0.,   0.,   0., -inf],\n",
    "    #  [0.,   0.,   0.,   0.,   0.]]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Die Funktion create_mask erstellt sowohl Quell- als auch Ziel-Pad-Masken, indem sie prüft, ob Elemente in der Quell- und Zielsequenz gleich dem Pad-Token sind. Diese Masken werden transponiert, um die richtige Dimension zu erhalten.\n",
    "def create_mask(tgt):\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    return tgt_mask"
   ],
   "id": "22ea5d0aae74c848",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Transformer",
   "id": "bc3bf095b843aafd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:02:20.108930Z",
     "start_time": "2024-06-22T17:02:20.094023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoderModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, nhead, num_decoder_layers, dim_feedforward, dropout=0.1, max_len=5000):\n",
    "        super(TransformerDecoderModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        # Hier werden die Layers des modells definiert\n",
    "        \n",
    "        # Input token werden zu embedding vektoren umgewandelt -> hier einfach mit linearer schicht\n",
    "        # input dimension wird zu embed_dim transformiert. embed dim ist die dimension meiner embeddings\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        # Positional encoding schicht für infos zu reihenfolge\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        \n",
    "        # hier werden die Decoder layers und der stack an layers (der transformer definiert)\n",
    "        decoder_layers = nn.TransformerDecoderLayer(embed_dim, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, num_decoder_layers)\n",
    "        \n",
    "        # output schicht, um die wahrscheinlichkeit für die features\n",
    "        self.generator = nn.Linear(embed_dim, input_dim)\n",
    "        \n",
    "        # Sigmoid für zwischen 0 und 1 \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, src, attention_mask, tgt_mask=None):\n",
    "        tgt = self.embedding(src)\n",
    "        tgt = self.pos_encoder(tgt)  # Apply positional encoding\n",
    "        \n",
    "        src_key_padding_mask = attention_mask == 0\n",
    "        \n",
    "        output = self.transformer_decoder(tgt, tgt, tgt_mask=tgt_mask, tgt_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.generator(output)\n",
    "        \n",
    "        output = self.sigmoid(output[:, -1, :])\n",
    "        return output"
   ],
   "id": "949244b84627249c",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3 train model",
   "id": "89c3112119d87a09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:02:20.117980Z",
     "start_time": "2024-06-22T17:02:20.109336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "input_dim = 88 # Dimension der eingehenden tokens\n",
    "embed_dim = 128  # Embedding dimension\n",
    "nhead = 4 # anzahl der attention heads\n",
    "num_decoder_layers = 3 # anzahl der decoder layer (die Blöcke übereinander)\n",
    "dim_feedforward = 128 # dimension des feedforward neural nets\n",
    "dropout = 0.1\n",
    "# batch_size = 32 oben festgelegt\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "max_len = 200  # Maximum length of the sequences -> abbruch ohne end token"
   ],
   "id": "b664156695f9eba8",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:02:20.153103Z",
     "start_time": "2024-06-22T17:02:20.120397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modell initialisieren\n",
    "model = TransformerDecoderModel(input_dim, embed_dim, nhead, num_decoder_layers, dim_feedforward, dropout, max_len)\n",
    "# auf GPU laden\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Kostenfunktion festlegen \n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# optimizer festlegen\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "id": "fed46f28c7fc3386",
   "execution_count": 33,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "training",
   "id": "72028798fd75d059"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:02:20.157209Z",
     "start_time": "2024-06-22T17:02:20.153872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def debug_shapes_and_ranges(X, y, output):\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Output range:\", torch.min(output).item(), torch.max(output).item())\n",
    "    print(\"y range:\", torch.min(y).item(), torch.max(y).item())\n",
    "\n",
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Move data to GPU\n",
    "        input_seq = batch['input_seq'].to(DEVICE)\n",
    "        target_seq = batch['target_seq'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        tgt_mask = create_mask(input_seq).to(DEVICE)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        # X ist unser Memory\n",
    "        logits = model(input_seq, attention_mask, tgt_mask=tgt_mask)\n",
    "        \n",
    "        #print(\"Training: prediction (model output) shape:\", logits.shape)\n",
    "        # Debug shapes and ranges\n",
    "        # debug_shapes_and_ranges(X, y, logits)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(logits, target_seq)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Move data to GPU\n",
    "        input_seq = batch['input_seq'].to(DEVICE)\n",
    "        target_seq = batch['target_seq'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        tgt_mask = create_mask(input_seq).to(DEVICE)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        # X ist unser Memory\n",
    "        logits = model(input_seq, attention_mask, tgt_mask=tgt_mask)\n",
    "        \n",
    "        #print(\"Training: prediction (model output) shape:\", logits.shape)\n",
    "        # Debug shapes and ranges\n",
    "        # debug_shapes_and_ranges(X, y, logits)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(logits, target_seq)\n",
    "        \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "b6e86bd9cad471f0",
   "execution_count": 34,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:03:33.080212Z",
     "start_time": "2024-06-22T17:02:20.157740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_loop(model, optimizer, loss_fn, train_loader)\n",
    "    end_time = timer()\n",
    "    val_loss = validation_loop(model, loss_fn, val_loader)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ],
   "id": "adff4c6704dfc3b9",
   "execution_count": 35,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4 Inferenz",
   "id": "671241f10b774392"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:03:33.083483Z",
     "start_time": "2024-06-22T17:03:33.080826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_sequence(model, input_seq, attention_mask, seq_length, device):\n",
    "    \"\"\"\n",
    "    Generate a sequence of predictions using the transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained transformer model\n",
    "    - input_seq: Initial input sequence (tensor of shape (1, seq_length, feature_dim))\n",
    "    - attention_mask: Attention mask for the input sequence\n",
    "    - seq_length: Length of the sequence to generate\n",
    "    - device: Device to perform inference on\n",
    "\n",
    "    Returns:\n",
    "    - generated_seq: Generated sequence of predictions (tensor of shape (generated_length, feature_dim))\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    generated_seq = []\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(seq_length):\n",
    "            # Generate target mask for the current sequence length\n",
    "            tgt_mask = generate_square_subsequent_mask(input_seq.size(1)).to(device)\n",
    "            \n",
    "            print(f\"Input step {i}:\", input_seq.shape)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(input_seq, attention_mask, tgt_mask=tgt_mask)\n",
    "            # Get the prediction for the last time step\n",
    "            next_token = output\n",
    "            \n",
    "\n",
    "            # Append the prediction to the generated sequence\n",
    "            generated_seq.append(next_token)\n",
    "\n",
    "            # Update the input sequence to include the new prediction\n",
    "            input_seq = torch.cat((input_seq, next_token.unsqueeze(1)), dim=1)\n",
    "\n",
    "            # Update the attention mask accordingly\n",
    "            new_mask = torch.ones(1, 1, device=device)\n",
    "            attention_mask = torch.cat((attention_mask, new_mask), dim=1)\n",
    "\n",
    "    generated_seq = torch.cat(generated_seq, dim=0)\n",
    "    return generated_seq"
   ],
   "id": "faba4ae5b818b756",
   "execution_count": 36,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:03:33.192290Z",
     "start_time": "2024-06-22T17:03:33.083879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def inference_single(model, test_loader, device, seq_length):\n",
    "    # Get the first batch from the test loader\n",
    "    for batch in test_loader:\n",
    "        input_seq = batch['input_seq'][0:1].to(device)  # Get the first sequence and keep batch dimension\n",
    "        attention_mask = batch['attention_mask'][0:1].to(device)  # Get the attention mask for the first sequence\n",
    "        break\n",
    "\n",
    "    print(attention_mask.shape)\n",
    "\n",
    "    # Generate sequence\n",
    "    generated_sequence = generate_sequence(model, input_seq, attention_mask, seq_length, device)\n",
    "    return generated_sequence\n",
    "\n",
    "generated_sequence = inference_single(model, test_loader, DEVICE, 100)\n",
    "\n",
    "print(generated_sequence.shape)"
   ],
   "id": "be9eac789ff9303f",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5 output analysieren",
   "id": "2fccf7f71b16f64e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:03:33.195002Z",
     "start_time": "2024-06-22T17:03:33.192717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def printHeatmap(predicted_harmony, center=0.1, vmin=0, vmax=1):\n",
    "    # Create the 'pictures' directory if it doesn't exist\n",
    "    if not os.path.exists('pictures'):\n",
    "        os.makedirs('pictures')\n",
    "\n",
    "    # Determine the next prefix number for saving files\n",
    "    existing_files = [f for f in os.listdir('pictures') if f.endswith('.png')]\n",
    "    if existing_files:\n",
    "        latest_file = max(existing_files)\n",
    "        latest_prefix = int(latest_file.split('_')[0])\n",
    "        prefix = f\"{latest_prefix + 1:02d}_\"\n",
    "    else:\n",
    "        prefix = \"00_\"\n",
    "\n",
    "    # Create and save a heatmap of Predicted Harmony Data\n",
    "    plt.figure(figsize=(20, 10))  # Adjust the size as necessary\n",
    "    sns.heatmap(predicted_harmony, cmap='coolwarm', center=center, vmin=vmin, vmax=vmax)  # Adjust color map and limits based on your data\n",
    "    plt.title('Heatmap of Predicted Harmony Data')\n",
    "    plt.xlabel('Keys on piano')\n",
    "    plt.ylabel('Probability of pressing (One-Hot-Encoding)')\n",
    "    plt.savefig(os.path.join('pictures', f'{prefix}heatmap_predicted_harmony.png'))\n",
    "    plt.show()"
   ],
   "id": "8ff7ef26511d6b9d",
   "execution_count": 38,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:03:33.840623Z",
     "start_time": "2024-06-22T17:03:33.195393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "threshold = 0.08\n",
    "\n",
    "output_sequence = generated_sequence\n",
    "\n",
    "print(output_sequence.shape)\n",
    "print(\"Output range:\", torch.min(output_sequence).item(), torch.max(output_sequence).item())\n",
    "\n",
    "# apply threshold value\n",
    "# Apply thresholding\n",
    "#output_sequence = (output_sequence >= threshold).float()\n",
    "\n",
    "# move to cpu for conversion\n",
    "output_sequence = output_sequence.cpu().numpy()\n",
    "print(type(output_sequence))\n",
    "\n",
    "printHeatmap(output_sequence, 0.002, 0, 1)"
   ],
   "id": "c3c2d2e395994477",
   "execution_count": 39,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Versuche rechte hand dazu zu simulieren",
   "id": "4d0f908a6527659b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:03:33.845128Z",
     "start_time": "2024-06-22T17:03:33.841347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_sequence_simulate_right_hand(model, input_seq, device):\n",
    "    \"\"\"\n",
    "    Generate a sequence of predictions using the transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained transformer model\n",
    "    - input_seq: a sequence of a song\n",
    "    - device: Device to perform inference on\n",
    "\n",
    "    Returns:\n",
    "    - generated_seq: Generated sequence of predictions (tensor of shape (generated_length, feature_dim))\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    generated_seq = []\n",
    "    \n",
    "    right_h = input_seq[0]\n",
    "    left_h = input_seq[1]\n",
    "    \n",
    "    \n",
    "    # add a batch dimension\n",
    "    right_h = np.expand_dims(right_h, axis=0)\n",
    "    left_h = np.expand_dims(left_h, axis=0)\n",
    "    \n",
    "    print(\"before slicing:\")\n",
    "    print(\"right hand tensor:\", right_h.shape)\n",
    "    print(\"left hand tensor:\", left_h.shape)\n",
    "    \n",
    "    # get the initial input sequence\n",
    "    initial_right_h = right_h[:, :100, :]\n",
    "    initial_left_h = left_h[:, :100, :]\n",
    "    \n",
    "    continuing_right_h = right_h[:, 100:, :]\n",
    "    \n",
    "    # build the initial sequence\n",
    "    init_sequence = np.bitwise_xor(initial_right_h, initial_left_h)\n",
    "    \n",
    "    # build attention mask for initial sequence.\n",
    "    attention_mask = torch.ones(init_sequence.shape[1]).to(device)\n",
    "    attention_mask = torch.unsqueeze(attention_mask, 0)\n",
    "    \n",
    "    print(\"initial input sequence:\", init_sequence.shape)\n",
    "    print(\"initial attention mask:\", attention_mask.shape)\n",
    "    \n",
    "    \n",
    "    init_sequence = torch.tensor(init_sequence, dtype=torch.float32).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(continuing_right_h.shape[1]):\n",
    "            # Generate target mask for the current sequence length\n",
    "            tgt_mask = generate_square_subsequent_mask(init_sequence.size(1)).to(device)\n",
    "            \n",
    "            print(f\"Input step {i}:\", init_sequence.shape)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(init_sequence, attention_mask, tgt_mask=tgt_mask)\n",
    "            # Get the prediction for the last time step\n",
    "            next_token = output\n",
    "            \n",
    "\n",
    "            # Append the prediction to the generated sequence\n",
    "            generated_seq.append(next_token)\n",
    "            \n",
    "            # Add the correct right hand contiuation\n",
    "            print(\"right hand continuing:\", continuing_right_h[0][i].shape)\n",
    "            print(\"next token\", next_token.shape)\n",
    "            \n",
    "            next_token = next_token.cpu()\n",
    "            # convert output token to binary array with threshold\n",
    "            threshold = 0.1\n",
    "            # Apply the threshold\n",
    "            next_token = (next_token >= threshold)\n",
    "            \n",
    "            next_token = np.bitwise_xor(next_token, continuing_right_h[0][i])\n",
    "            \n",
    "            next_token = torch.tensor(next_token, dtype=torch.float32).to(device)\n",
    "            \n",
    "            print(\"next token after adding right hand\", next_token.shape)\n",
    "            \n",
    "            # Update the input sequence to include the new prediction\n",
    "            init_sequence = torch.cat((init_sequence, next_token.unsqueeze(1)), dim=1)\n",
    "\n",
    "            # Update the attention mask accordingly\n",
    "            new_mask = torch.ones(1, 1, device=device)\n",
    "            attention_mask = torch.cat((attention_mask, new_mask), dim=1)\n",
    "\n",
    "    generated_seq = torch.cat(generated_seq, dim=0)\n",
    "    return generated_seq, init_sequence"
   ],
   "id": "91b059d70b56fe27",
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:03:34.522069Z",
     "start_time": "2024-06-22T17:03:33.845538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get a song from the original data\n",
    "song = dataset_as_snapshots[1]\n",
    "\n",
    "generated_sequence, last_input_sequence = generate_sequence_simulate_right_hand(model, song, DEVICE)"
   ],
   "id": "7ca4c1c8a1c42e2a",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:03:35.693067Z",
     "start_time": "2024-06-22T17:03:34.522639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"original harmony length:\", song[1].shape)\n",
    "\n",
    "print(generated_sequence.shape)\n",
    "\n",
    "generated_sequence = generated_sequence.cpu()\n",
    "last_input_sequence = last_input_sequence.cpu()\n",
    "\n",
    "printHeatmap(generated_sequence, 1)\n",
    "\n",
    "printHeatmap(last_input_sequence[0], 0.5)"
   ],
   "id": "d67338622473bf99",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "generate midi",
   "id": "9bab06b03f54322a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:03:35.715999Z",
     "start_time": "2024-06-22T17:03:35.693602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mido import MidiFile, MidiTrack, Message\n",
    "\n",
    "# Create a new MIDI file and two tracks\n",
    "mid = MidiFile()\n",
    "melody_track = MidiTrack()\n",
    "harmony_track = MidiTrack()\n",
    "mid.tracks.append(melody_track)\n",
    "mid.tracks.append(harmony_track)\n",
    "\n",
    "# Constants\n",
    "TIME_PER_SNAPSHOT = 0.1  # seconds\n",
    "TICKS_PER_BEAT = mid.ticks_per_beat\n",
    "TEMPO = 500000  # microseconds per beat, equivalent to 120 BPM\n",
    "TICKS_PER_SNAPSHOT = int(TICKS_PER_BEAT * (TIME_PER_SNAPSHOT / (60 / 120)))  # for 120 BPM\n",
    "\n",
    "# Initial states of the keys\n",
    "previous_melody_keys = [0] * 88\n",
    "previous_harmony_keys = [0] * 88\n",
    "\n",
    "# Iterate over each row (snapshot) in the dataframes\n",
    "for index in range(len(last_input_sequence[0])):\n",
    "    melody_keys = last_input_sequence[0][index].tolist()\n",
    "\n",
    "    for key in range(88):\n",
    "        # Handle melody track\n",
    "        if melody_keys[key] == 1 and previous_melody_keys[key] == 0:\n",
    "            # Note on\n",
    "            melody_track.append(Message('note_on', note=key + 21, velocity=64, time=0))\n",
    "        elif melody_keys[key] == 0 and previous_melody_keys[key] == 1:\n",
    "            # Note off\n",
    "            melody_track.append(Message('note_off', note=key + 21, velocity=64, time=0))\n",
    "\n",
    "        \n",
    "\n",
    "    previous_melody_keys = melody_keys\n",
    "\n",
    "    # Add time delay (advance time) for each track\n",
    "    melody_track.append(Message('note_on', note=0, velocity=0, time=TICKS_PER_SNAPSHOT))\n",
    "    harmony_track.append(Message('note_on', note=0, velocity=0, time=TICKS_PER_SNAPSHOT))\n",
    "\n",
    "# Save the MIDI file\n",
    "output_path = '/home/falaxdb/Repos/minus1/transformer_decoder_training/jupyter_notebooks/piano_tests/output/v3'\n",
    "output_data_name = 'last_input_tokens.mid'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "mid.save(f'{output_path}{output_data_name}')\n",
    "\n",
    "print(f'MIDI file saved to {output_path}')"
   ],
   "id": "f859ada6fca10a84",
   "execution_count": 43,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T17:03:35.915704Z",
     "start_time": "2024-06-22T17:03:35.716508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data_visualization import snapshot_to_midi\n",
    "\n",
    "input_data = []\n",
    "\n",
    "input_data.append(song[0])\n",
    "input_data.append(song[1])\n",
    "\n",
    "input_data.append(last_input_sequence[0].numpy())\n",
    "\n",
    "# print(\"last input sequnce:\", last_input_sequence[0].shape)\n",
    "\n",
    "for track in input_data:\n",
    "    print(track.shape)\n",
    "\n",
    "output_path = \"/home/falaxdb/Repos/minus1/transformer_decoder_training/jupyter_notebooks/piano_tests/output/v3\"\n",
    "\n",
    "output_name = \"combined.mid\"\n",
    "\n",
    "\n",
    "snapshot_to_midi.create_midi_from_snapshots(input_data, [\"original melody\", \"original harmony\", \"generated sequence\"], 0.1, output_path, output_name)\n",
    "\n"
   ],
   "id": "6960aa15b02b7a0f",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "- Spielt immer noch genre halbtöne direkt nebeneinander. \n",
    "- Auch für sehr lange sequenzen wird immer der gleiche ton gehalten (liegt evtl. daran, dass ich noch nicht die rechte Hand mit in die generierung einbeziehe)\n",
    "- Spielt einen Ton sehr lange."
   ],
   "id": "8c4c61b79bda4f63"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
