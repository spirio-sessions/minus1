{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# First test of using a Transformer decoder only\n",
    "\n",
    "Good guide: https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse\n",
    "\n",
    "pytorch api: https://pytorch.org/docs/stable/nn.html#transformer-layers"
   ],
   "id": "65cee26ca8ea0444"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 0 imports",
   "id": "163417bc420a671d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:52:23.340444Z",
     "start_time": "2024-06-19T14:52:22.732303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from data_preperation import dataset_snapshot\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ],
   "id": "11a7936b1ef6d9af",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:52:23.373389Z",
     "start_time": "2024-06-19T14:52:23.341297Z"
    }
   },
   "cell_type": "code",
   "source": "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "id": "6c366d202046f522",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1 Data Preperation",
   "id": "8905f2ac509475c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Special Tokens definieren",
   "id": "1469da2c468adf97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:52:23.388897Z",
     "start_time": "2024-06-19T14:52:23.373991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SOS und EOS Tokens definieren\n",
    "\n",
    "# Define SOS and EOS tokens as global variables\n",
    "SOS_TOKEN = np.full((1, 12), 1)  # SOS token representation with 1\n",
    "EOS_TOKEN = np.full((1, 12), 3)  # EOS token probably not neccessary since i use fixed length sequences\n",
    "\n",
    "UNK_IDX = 5 #Brauchen wir glaube ich nicht\n",
    "PAD_IDX = 4 #Brauchen wir auch nicht, weil die sequenzen alle genau richtig lang sind"
   ],
   "id": "710c9df879a350de",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## load dataset",
   "id": "d6873315a32f96a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:52:51.814280Z",
     "start_time": "2024-06-19T14:52:23.390990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create snapshots\n",
    "dataset_as_snapshots = dataset_snapshot.process_dataset_multithreaded(\"/home/falaxdb/Repos/minus1/datasets/maestro_v3_split/hands_split_into_seperate_midis\", 0.1)\n",
    "# filter snapshots to 88 piano notes\n",
    "dataset_as_snapshots = dataset_snapshot.filter_piano_range(dataset_as_snapshots)\n",
    "# compress data into one octave\n",
    "dataset_as_snapshots =  dataset_snapshot.compress_existing_dataset_to_12keys(dataset_as_snapshots)\n",
    "\n",
    "for song in dataset_as_snapshots:\n",
    "    print(\"song:\")\n",
    "    for track in song:\n",
    "        print(track.shape)"
   ],
   "id": "8ecd9ec462806990",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Daten batchen, splitten, etc",
   "id": "b5fd9b6f87a40eda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:52:51.995764Z",
     "start_time": "2024-06-19T14:52:51.814897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Function to add SOS and EOS tokens to each chunk\n",
    "def add_sos_eos_to_chunks(chunks):\n",
    "    new_chunks = []\n",
    "    for chunk in chunks:\n",
    "        # new_chunk = np.vstack([SOS_TOKEN, chunk, EOS_TOKEN]) eos token probably not neccessary\n",
    "        new_chunk = np.vstack([SOS_TOKEN, chunk])\n",
    "        new_chunks.append(new_chunk)\n",
    "    return new_chunks\n",
    "\n",
    "# Function to split sequences into chunks\n",
    "def split_into_chunks(sequence, chunk_size):\n",
    "    return [sequence[i:i + chunk_size] for i in range(0, len(sequence), chunk_size)]\n",
    "\n",
    "# Function to filter out short chunks while maintaining pairs\n",
    "def filter_short_chunks(chunks_1, chunks_2, min_length):\n",
    "    filtered_chunks_1 = []\n",
    "    filtered_chunks_2 = []\n",
    "    for chunk_1, chunk_2 in zip(chunks_1, chunks_2):\n",
    "        if len(chunk_1) >= min_length and len(chunk_2) >= min_length:\n",
    "            filtered_chunks_1.append(chunk_1)\n",
    "            filtered_chunks_2.append(chunk_2)\n",
    "    return filtered_chunks_1, filtered_chunks_2\n",
    "\n",
    "# Custom Dataset class\n",
    "class PianoDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Prepare the dataset with paired sequences and SOS/EOS tokens for each chunk\n",
    "def prepare_dataset(dataset_as_snapshots, chunk_size, min_length):\n",
    "    data = []\n",
    "    for song in dataset_as_snapshots:\n",
    "        track_1, track_2 = song\n",
    "        assert len(track_1) == len(track_2), \"Tracks must have the same length\"\n",
    "        \n",
    "        chunks_1 = split_into_chunks(track_1, chunk_size)\n",
    "        chunks_2 = split_into_chunks(track_2, chunk_size)\n",
    "        chunks_1, chunks_2 = filter_short_chunks(chunks_1, chunks_2, min_length)\n",
    "        \n",
    "        # Add SOS and EOS tokens to each chunk\n",
    "        chunks_1 = add_sos_eos_to_chunks(chunks_1)\n",
    "        chunks_2 = add_sos_eos_to_chunks(chunks_2)\n",
    "        \n",
    "        for x, y in zip(chunks_1, chunks_2):\n",
    "            data.append((x, y))\n",
    "    return data\n"
   ],
   "id": "b35d226b4e938d32",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:52:52.236747Z",
     "start_time": "2024-06-19T14:52:51.996306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define your dataset parameters\n",
    "chunk_size = 100  # Define the chunk size you want\n",
    "min_length = chunk_size  # Minimum length to keep a chunk\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Prepare the dataset\n",
    "data = prepare_dataset(dataset_as_snapshots, chunk_size, min_length)\n",
    "\n",
    "# Split the dataset using sklearn while maintaining pairs\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42, shuffle=True)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = PianoDataset(train_data)\n",
    "val_dataset = PianoDataset(val_data)\n",
    "test_dataset = PianoDataset(test_data)\n",
    "\n",
    "# Create DataLoaders for each subset with drop_last=True\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# Iterate over the DataLoader (example with train_loader)\n",
    "for batch in train_loader:\n",
    "    X, y = batch\n",
    "    print(X.shape, y.shape)\n",
    "    # X and y should both have shape (batch_size, chunk_size + 2, feature dimension) because of SOS and EOS tokens"
   ],
   "id": "95718b72c7bc39e7",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Modell definieren",
   "id": "c91d843f6f39422c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Positional encoding",
   "id": "f047730114fadb35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:52:52.240238Z",
     "start_time": "2024-06-19T14:52:52.237261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Encoding - From formula -> This is basically applying the formula for Positional encoding (The one with Sinus and Cosinus)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # Baically a positions list 0, 1, 2, 3, 4, 5, ...\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        #  # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "         # Saving buffer (same as parameter without gradients needed)\n",
    "        # pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # Shape: (1, max_len, d_model) Änderung evtl. wegen batch first = True\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Residual connection + pos encoding\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ],
   "id": "7835b65f3d71fe91",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Maske",
   "id": "9538a7d2676220f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:52:52.250077Z",
     "start_time": "2024-06-19T14:52:52.240686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    \n",
    "    # EX for size=5:\n",
    "    # [[0., -inf, -inf, -inf, -inf],\n",
    "    #  [0.,   0., -inf, -inf, -inf],\n",
    "    #  [0.,   0.,   0., -inf, -inf],\n",
    "    #  [0.,   0.,   0.,   0., -inf],\n",
    "    #  [0.,   0.,   0.,   0.,   0.]]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Die Funktion create_mask erstellt sowohl Quell- als auch Ziel-Pad-Masken, indem sie prüft, ob Elemente in der Quell- und Zielsequenz gleich dem Pad-Token sind. Diese Masken werden transponiert, um die richtige Dimension zu erhalten.\n",
    "def create_mask(tgt):\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    return tgt_mask"
   ],
   "id": "886e017310d481f3",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformer",
   "id": "d3e8dc3fcfcc4fbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:52:52.258512Z",
     "start_time": "2024-06-19T14:52:52.250675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoderModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, nhead, num_decoder_layers, dim_feedforward, dropout=0.1, max_len=5000):\n",
    "        super(TransformerDecoderModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        # Hier werden die Layers des modells definiert\n",
    "        \n",
    "        # Input token werden zu embedding vektoren umgewandelt -> hier einfach mit linearer schicht\n",
    "        # input dimension wird zu embed_dim transformiert. embed dim ist die dimension meiner embeddings\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        # Positional encoding schicht für infos zu reihenfolge\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        \n",
    "        # hier werden die Decoder layers und der stack an layers (der transformer definiert)\n",
    "        decoder_layers = nn.TransformerDecoderLayer(embed_dim, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, num_decoder_layers)\n",
    "        \n",
    "        # output schicht, um die wahrscheinlichkeit für die einzelnen Tokens festzulegen\n",
    "        self.generator = nn.Linear(embed_dim, input_dim)\n",
    "        \n",
    "        # Sigmoid für zwischen 0 und 1 \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask,):\n",
    "        tgt = self.embedding(tgt)\n",
    "        tgt = self.pos_encoder(tgt)  # Apply positional encoding\n",
    "        \n",
    "        # memory muss auch korrekt eingebettet/pos encoded werden werden\n",
    "        memory = self.embedding(memory)\n",
    "        memory = self.pos_encoder(memory)\n",
    "        \n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        output = self.generator(output)\n",
    "        \n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ],
   "id": "d4e2c4beb0f0e681",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3 Modell Laden und hyperparamter festlegen",
   "id": "67349dc9ca4a922a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "hyperparameter",
   "id": "4a8c66f23651d9b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:52:52.267113Z",
     "start_time": "2024-06-19T14:52:52.259819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "input_dim = 12 # Dimension der eingehenden tokens\n",
    "embed_dim = 64  # Embedding dimension\n",
    "nhead = 2 # anzahl der attention heads\n",
    "num_decoder_layers = 2 # anzahl der decoder layer (die Blöcke übereinander)\n",
    "dim_feedforward = 128 # dimension des feedforward neural nets\n",
    "dropout = 0.1\n",
    "# batch_size = 32 oben festgelegt\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "max_len = 200  # Maximum length of the sequences -> abbruch ohne end token"
   ],
   "id": "75ed49433327d8e0",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "modell initialisieren + Optimizer und Kostenfunktion",
   "id": "17c39c941edaef05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:52:52.792436Z",
     "start_time": "2024-06-19T14:52:52.268303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from lstm_training import music_theory_loss\n",
    "\n",
    "# Modell initialisieren\n",
    "model = TransformerDecoderModel(input_dim, embed_dim, nhead, num_decoder_layers, dim_feedforward, dropout, max_len)\n",
    "# auf GPU laden\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Kostenfunktion festlegen \n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# optimizer festlegen\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "id": "1501428fc25c917a",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4 Trainings und validation loop",
   "id": "7035ebc0d7bef3b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "for debugging",
   "id": "8af49f2ae0a47329"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:52:52.796056Z",
     "start_time": "2024-06-19T14:52:52.793550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def debug_shapes_and_ranges(X, y, output):\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Output range:\", torch.min(output).item(), torch.max(output).item())\n",
    "    print(\"y range:\", torch.min(y).item(), torch.max(y).item())"
   ],
   "id": "2719ffcbcf7c8956",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:52:52.810862Z",
     "start_time": "2024-06-19T14:52:52.796798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        X, y = batch\n",
    "        #print(X.shape, y.shape)\n",
    "        # X and y should both have shape (batch_size, chunk_size + 2, feature dim) because of SOS and EOS tokens\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1, :]\n",
    "        y_expected = y[:,1:, :]\n",
    "        #print(\"Training: X shape:\", X.shape)\n",
    "        #print(\"Training: y_input shape:\", y_input.shape)\n",
    "        #print(\"Training: y_expected shape:\", y_expected.shape)\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        tgt_mask = create_mask(y_input).to(DEVICE)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        # X ist unser Memory\n",
    "        logits = model(y_input, X, tgt_mask)\n",
    "        \n",
    "        #print(\"Training: prediction (model output) shape:\", logits.shape)\n",
    "        # Debug shapes and ranges\n",
    "        # debug_shapes_and_ranges(X, y, logits)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(logits, y_expected)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "549dfc5223f0eccd",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:52:52.819109Z",
     "start_time": "2024-06-19T14:52:52.811247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        X, y = batch\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1, :]\n",
    "        y_expected = y[:,1:, :]\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        tgt_mask = create_mask(y_input).to(DEVICE)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        # X ist unser Memory\n",
    "        logits = model(y_input, X, tgt_mask)\n",
    "        \n",
    "        #print(\"Training: prediction (model output) shape:\", logits.shape)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(logits, y_expected)\n",
    "        \n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "6c1b34dc43315f26",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5 Modell trainieren",
   "id": "92004f883387b8bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T14:53:41.143615Z",
     "start_time": "2024-06-19T14:52:52.819616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_loop(model, optimizer, loss_fn, train_loader)\n",
    "    end_time = timer()\n",
    "    val_loss = validation_loop(model, loss_fn, val_loader)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ],
   "id": "4f06331e0ddb3054",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inferenz",
   "id": "64c887a2edf39502"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T15:07:45.447580Z",
     "start_time": "2024-06-19T15:07:45.302616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def infer(model, memory, max_seq_len, start_token, device):\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"start inference\")\n",
    "    \n",
    "    # Initialize the output sequence with the start token\n",
    "    output_seq = torch.zeros((memory.size(0), max_seq_len, memory.size(2))).to(device)\n",
    "    output_seq[:, 0, :] = start_token\n",
    "    \n",
    "    for t in range(1, max_seq_len):\n",
    "        tgt_input = output_seq[:, :t, :]\n",
    "        \n",
    "        # Create target mask\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            output = model(tgt_input, memory, tgt_mask)\n",
    "        \n",
    "        # Get the next token prediction (taking the last token in the sequence)\n",
    "        next_token = output[:, -1, :]\n",
    "        \n",
    "        # Append the next token to the output sequence\n",
    "        output_seq[:, t, :] = next_token\n",
    "    \n",
    "    return output_seq\n",
    "\n",
    "\n",
    "# test inference\n",
    "# get a test batch for X_infer\n",
    "data_iter = iter(test_loader)\n",
    "X_infer, y_true = next(data_iter)\n",
    "X_infer, y_true = X_infer.to(DEVICE), y_true.to(DEVICE)\n",
    "\n",
    "start_token = torch.full((1, input_dim), 1.0).to(DEVICE)  # All ones\n",
    "predicted_sequence = infer(model, X_infer, max_seq_len=50, start_token=start_token, device=DEVICE)\n",
    "\n",
    "# remove start token\n",
    "predicted_sequence = predicted_sequence[:, 1:, :]\n",
    "\n",
    "print(\"Predicted sequence:\", predicted_sequence)\n",
    "print(\"Predicted sequence shape:\", predicted_sequence.shape)\n",
    "print(\"Output range:\", torch.min(predicted_sequence).item(), torch.max(predicted_sequence).item())"
   ],
   "id": "bf7295c95ed89dd5",
   "execution_count": 38,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notes\n",
    "\n",
    "Verwende sigmoid output layer, dass die outputs zu wahrscheinlichkeiten werden\n",
    "\n",
    "Ich habe die EOS tokens weggelassen -> brauchen wir ja nicht wirklich, ist ja kein satzt sondern immer feste abschnitte -> evtl besser, weil keine zahlen außerhalb von 0 und 1"
   ],
   "id": "83c6d921bc46ce29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualize outputs",
   "id": "e8b40cfa8c7a8609"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T15:07:54.406144Z",
     "start_time": "2024-06-19T15:07:54.387611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def printHeatmap(predicted_harmony, center=0.1, vmin=0, vmax=1):\n",
    "    # Create the 'pictures' directory if it doesn't exist\n",
    "    if not os.path.exists('pictures'):\n",
    "        os.makedirs('pictures')\n",
    "\n",
    "    # Determine the next prefix number for saving files\n",
    "    existing_files = [f for f in os.listdir('pictures') if f.endswith('.png')]\n",
    "    if existing_files:\n",
    "        latest_file = max(existing_files)\n",
    "        latest_prefix = int(latest_file.split('_')[0])\n",
    "        prefix = f\"{latest_prefix + 1:02d}_\"\n",
    "    else:\n",
    "        prefix = \"00_\"\n",
    "\n",
    "    # Create and save a heatmap of Predicted Harmony Data\n",
    "    plt.figure(figsize=(20, 10))  # Adjust the size as necessary\n",
    "    sns.heatmap(predicted_harmony, cmap='coolwarm', center=center, vmin=vmin, vmax=vmax)  # Adjust color map and limits based on your data\n",
    "    plt.title('Heatmap of Predicted Harmony Data')\n",
    "    plt.xlabel('Keys on piano')\n",
    "    plt.ylabel('Probability of pressing (One-Hot-Encoding)')\n",
    "    plt.savefig(os.path.join('pictures', f'{prefix}heatmap_predicted_harmony.png'))\n",
    "    plt.show()"
   ],
   "id": "bbf7c1847820b6a1",
   "execution_count": 39,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T15:21:38.879316Z",
     "start_time": "2024-06-19T15:21:28.130288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "threshold = 0.05 \n",
    "\n",
    "for output_sequence in predicted_sequence:\n",
    "    print(output_sequence.shape)\n",
    "    print(\"Output range:\", torch.min(output_sequence).item(), torch.max(output_sequence).item())\n",
    "    \n",
    "    # apply threshold value\n",
    "    # Apply thresholding\n",
    "    output_sequence = (output_sequence >= threshold).float()\n",
    "    \n",
    "    # move to cpu for conversion\n",
    "    output_sequence = output_sequence.cpu().numpy()\n",
    "    print(type(output_sequence))\n",
    "    \n",
    "    printHeatmap(output_sequence, 0.002, 0, 1)"
   ],
   "id": "e1d3754b47e9ebc6",
   "execution_count": 51,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "Funktioniert im generellen nicht wirklich gut.\n",
    "\n",
    "Probleme:\n",
    "\n",
    "- Es werden nur 0 bis 7 Tokens vorhergesagt, obwohl eig. 50 tokens vorhergesagt werden sollen und auch auf mehr trainiert wird.\n",
    "- Die Töne, die gespielt werden passen musikalisch nicht gut zusammen\n",
    "- Er ist sich generell für alle töne sehr unsicher: output range nach sigmoid ist meistens zwischen 0 und 0.15\n",
    "\n",
    "Potentielle Ursachen:\n",
    "\n",
    "- Ich mache sehr wahrscheinlich einen Fehler beim Training mit memory und Target, da beim Decoder only ja der output vom letzten Step an den output angehängt und als input für den nächsten step verwendet wird. -> Autoregressiv\n",
    "    - -> Wie soll ich dann die Vorhersage für die linke hand von der Rechten Hand machen? -> ist ja dann keine Übersetzung mehr, sondern Die Tokenvorhersage. Aber ich brauche ja die linke hand (+ evtl. rechte hand als input) -> und muss die Rechte hand vorhersagen. wie ist das dann mit der Vorhersage von nur der linken hand? -> oder doch lieber ganze sequenz vorhersagen \n",
    "- Der Decoder only Transformer generiert normalerweise nur 1 Token auf einmal -> ich habe evtl falschen Ansatz\n",
    "- Ich mache generell irgendwo einen Fehler in der Transformerstruktur\n",
    "- Das training passt nicht für einen Encoder-decoder Only -> Ich mache ja nicht wirklich eine sequentielle vorhersage sondern versuche quasi eher eine Übersetzung -> von rechter auf linke hand\n",
    "- Die Snapshots müssen irgendwie anders tokenized werden. -> in allen textgenerierungstasks ist die eingabesquenz vor dem embedding geringer -> nur zahlen anstatt einem Vektor -> evtl. eigenen Tokenizer schreiben?\n",
    "- **Keine eigene Kostenfunktion**\n",
    "- inferenz muss anders gemacht werden\n",
    "- Ist sigmoid-funktion im forward schritt wirklich notwendig oder am besten erst ganz am schluss"
   ],
   "id": "8ad1d5778207bb56"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
