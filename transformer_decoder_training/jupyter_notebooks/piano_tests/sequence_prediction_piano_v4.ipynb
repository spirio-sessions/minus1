{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Jetzt versuch mit Beispiel aus https://github.com/LukeDitria/pytorch_tutorials/blob/main/section14_transformers/solutions/Pytorch2_Transformer_Text_Generation.ipynb.\n",
    "\n",
    "Dazugehöriges Video: https://youtu.be/7J4Xn0LnnEA?list=PLyHaDji6oZkV4sRUVoJdvZm2Sk7ohQ9yD\n",
    "\n",
    "(Eventuelle Idee: ich baue einen Tokenizer für Die einzelnen Töne. -> Jeder snapshot wird getokenized. so wie die Einzelnen wörter bei einem Transformer für text. Sollte gut machbar sein bei 12 Tönen in einer Oktave -> könnten zu viele mögliche Tokens werden bei 88 Tönen. Aber es werden ja meißtens nicht alle töne gleichzeitig gespielt.)"
   ],
   "id": "23c599a4052f29a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from data_preperation import dataset_snapshot\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ],
   "id": "109e9f8643e1bdb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "SOS_TOKEN = np.full((1, 12), 1)  # SOS token representation with 1\n",
    "\n",
    "# Learning rate for the optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Number of epochs for training\n",
    "nepochs = 20\n",
    "\n",
    "# Batch size for data loaders\n",
    "batch_size = 128\n",
    "\n",
    "# Maximum sequence length for inputs\n",
    "max_len = 64\n",
    "\n",
    "# Root directory of the dataset\n",
    "data_set_root = \"/home/falaxdb/Repos/Learn-ml/Transformer-pytorch/piano_data/maestro_v3/hands_split_into_seperate_midis\""
   ],
   "id": "8b87d465bb85563a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create snapshots\n",
    "dataset_as_snapshots = dataset_snapshot.process_dataset_multithreaded(\"/home/falaxdb/Repos/minus1/datasets/maestro_v3_split/hands_split_into_seperate_midis\", 0.1)\n",
    "# filter snapshots to 88 piano notes\n",
    "dataset_as_snapshots = dataset_snapshot.filter_piano_range(dataset_as_snapshots)\n",
    "# compress data into one octave\n",
    "dataset_as_snapshots =  dataset_snapshot.compress_existing_dataset_to_12keys(dataset_as_snapshots)\n",
    "\n",
    "for song in dataset_as_snapshots:\n",
    "    print(\"song:\")\n",
    "    for track in song:\n",
    "        print(track.shape)"
   ],
   "id": "7a62b47498012796"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Function to add SOS and EOS tokens to each chunk\n",
    "def add_sos_eos_to_chunks(chunks):\n",
    "    new_chunks = []\n",
    "    for chunk in chunks:\n",
    "        # new_chunk = np.vstack([SOS_TOKEN, chunk, EOS_TOKEN]) eos token probably not neccessary\n",
    "        new_chunk = np.vstack([SOS_TOKEN, chunk])\n",
    "        new_chunks.append(new_chunk)\n",
    "    return new_chunks\n",
    "\n",
    "# Function to split sequences into chunks\n",
    "def split_into_chunks(sequence, chunk_size):\n",
    "    return [sequence[i:i + chunk_size] for i in range(0, len(sequence), chunk_size)]\n",
    "\n",
    "# Function to filter out short chunks while maintaining pairs\n",
    "def filter_short_chunks(chunks_1, chunks_2, min_length):\n",
    "    filtered_chunks_1 = []\n",
    "    filtered_chunks_2 = []\n",
    "    for chunk_1, chunk_2 in zip(chunks_1, chunks_2):\n",
    "        if len(chunk_1) >= min_length and len(chunk_2) >= min_length:\n",
    "            filtered_chunks_1.append(chunk_1)\n",
    "            filtered_chunks_2.append(chunk_2)\n",
    "    return filtered_chunks_1, filtered_chunks_2\n",
    "\n",
    "# Custom Dataset class\n",
    "class PianoDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Prepare the dataset with paired sequences and SOS/EOS tokens for each chunk\n",
    "def prepare_dataset(dataset_as_snapshots, chunk_size, min_length):\n",
    "    data = []\n",
    "    for song in dataset_as_snapshots:\n",
    "        track_1, track_2 = song\n",
    "        assert len(track_1) == len(track_2), \"Tracks must have the same length\"\n",
    "        \n",
    "        chunks_1 = split_into_chunks(track_1, chunk_size)\n",
    "        chunks_2 = split_into_chunks(track_2, chunk_size)\n",
    "        chunks_1, chunks_2 = filter_short_chunks(chunks_1, chunks_2, min_length)\n",
    "        \n",
    "        # Add SOS and EOS tokens to each chunk\n",
    "        chunks_1 = add_sos_eos_to_chunks(chunks_1)\n",
    "        chunks_2 = add_sos_eos_to_chunks(chunks_2)\n",
    "        \n",
    "        for x, y in zip(chunks_1, chunks_2):\n",
    "            data.append((x, y))\n",
    "    return data"
   ],
   "id": "9e4497f21b9fe814"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = prepare_dataset(dataset_as_snapshots, max_len, max_len)\n",
    "\n",
    "# Split the dataset using sklearn while maintaining pairs\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42, shuffle=True)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = PianoDataset(train_data)\n",
    "val_dataset = PianoDataset(val_data)\n",
    "test_dataset = PianoDataset(test_data)\n",
    "\n",
    "# Create DataLoaders for each subset with drop_last=True\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# Iterate over the DataLoader (example with train_loader)\n",
    "for batch in train_loader:\n",
    "    X, y = batch\n",
    "    print(X.shape, y.shape)\n",
    "    # X and y should both have shape (batch_size, chunk_size + 2, feature dimension) because of SOS and EOS tokens"
   ],
   "id": "cb31a759438e8e52"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
