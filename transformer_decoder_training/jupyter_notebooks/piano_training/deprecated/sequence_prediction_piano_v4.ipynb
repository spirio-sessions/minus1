{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Jetzt versuch mit Beispiel aus https://github.com/LukeDitria/pytorch_tutorials/blob/main/section14_transformers/solutions/Pytorch2_Transformer_Text_Generation.ipynb.\n",
    "\n",
    "Dazugehöriges Video: https://youtu.be/7J4Xn0LnnEA?list=PLyHaDji6oZkV4sRUVoJdvZm2Sk7ohQ9yD\n",
    "\n",
    "(Eventuelle Idee: ich baue einen Tokenizer für Die einzelnen Töne. -> Jeder snapshot wird getokenized. so wie die Einzelnen wörter bei einem Transformer für text. Sollte gut machbar sein bei 12 Tönen in einer Oktave -> könnten zu viele mögliche Tokens werden bei 88 Tönen. Aber es werden ja meißtens nicht alle töne gleichzeitig gespielt.)"
   ],
   "id": "23c599a4052f29a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:21:26.970010Z",
     "start_time": "2024-06-24T17:21:26.069627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from data_preperation import dataset_snapshot\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ],
   "id": "109e9f8643e1bdb6",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:21:26.972818Z",
     "start_time": "2024-06-24T17:21:26.970731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "#SOS_TOKEN = np.full((1,1), (2 ** 12))   # SOS token index (außerhalb was mit 12 binärstellen angezeigt werden kann)\n",
    "#PAD_TOKEN = np.full((1,1), ((2 ** 12) + 1)) # padding token\n",
    "SOS_TOKEN = (2 ** 12)   # SOS token index (außerhalb was mit 12 binärstellen angezeigt werden kann)\n",
    "PAD_TOKEN = ((2 ** 12) + 1) # padding token\n",
    "\n",
    "\n",
    "# Learning rate for the optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Number of epochs for training\n",
    "nepochs = 20\n",
    "\n",
    "# Batch size for data loaders\n",
    "batch_size = 128\n",
    "\n",
    "# Maximum sequence length for inputs\n",
    "max_len = 200\n",
    "\n",
    "# Root directory of the dataset\n",
    "data_set_root = \"/home/falaxdb/Repos/Learn-ml/Transformer-pytorch/piano_data/maestro_v3/hands_split_into_seperate_midis\""
   ],
   "id": "8b87d465bb85563a",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:22:20.241616Z",
     "start_time": "2024-06-24T17:21:26.973439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create snapshots\n",
    "dataset_as_snapshots = dataset_snapshot.process_dataset_multithreaded(\"/home/falaxdb/Repos/minus1/datasets/maestro_v3_split/hands_split_into_seperate_midis\", 0.05)\n",
    "# filter snapshots to 88 piano notes\n",
    "dataset_as_snapshots = dataset_snapshot.filter_piano_range(dataset_as_snapshots)\n",
    "# compress data into one octave\n",
    "dataset_as_snapshots =  dataset_snapshot.compress_existing_dataset_to_12keys(dataset_as_snapshots)\n",
    "\n",
    "for song in dataset_as_snapshots:\n",
    "    print(\"song:\")\n",
    "    for track in song:\n",
    "        print(track.shape)"
   ],
   "id": "7a62b47498012796",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Try converting the data into token indices (i think transformers work best with data in this format)\n",
    "\n",
    "For one octave: i think i can handle the multi one hot encoded Vektors like binary data and just convert it into base 10"
   ],
   "id": "5cf381c260b5e226"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:22:40.270428Z",
     "start_time": "2024-06-24T17:22:20.242553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def binary_to_base10(single_snapshot):\n",
    "    return int(\"\".join(map(lambda x: str(int(x)), single_snapshot)), 2)\n",
    "\n",
    "def base10_to_binary_one_hot(number, vector_length):\n",
    "    # Convert number to binary string without '0b' prefix\n",
    "    binary_str = bin(number)[2:]\n",
    "    \n",
    "    # Ensure the binary string is of the correct length by padding with leading zeros\n",
    "    binary_str = binary_str.zfill(vector_length)\n",
    "    \n",
    "    # Map each binary digit to an integer and store it in a list\n",
    "    one_hot_vector = list(map(lambda x: int(x), binary_str))\n",
    "    \n",
    "    return one_hot_vector\n",
    "\n",
    "def convert_snapshots_to_base10(dataset_as_snapshots):\n",
    "    converted_dataset = []\n",
    "    for song in dataset_as_snapshots:\n",
    "        track_1, track_2 = song\n",
    "        track_1_base10 = np.array([binary_to_base10(snapshot) for snapshot in track_1])\n",
    "        track_2_base10 = np.array([binary_to_base10(snapshot) for snapshot in track_2])\n",
    "        converted_dataset.append([track_1_base10, track_2_base10])\n",
    "    return converted_dataset\n",
    "\n",
    "dataset_as_snapshots = convert_snapshots_to_base10(dataset_as_snapshots)\n",
    "\n",
    "\n"
   ],
   "id": "e9c1bf50a845087b",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:22:40.276966Z",
     "start_time": "2024-06-24T17:22:40.271006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Shapes after converting:\")\n",
    "\n",
    "for song in dataset_as_snapshots:\n",
    "    print(\"song:\")\n",
    "    for track in song:\n",
    "        print(track.shape)\n",
    "\n",
    "print(\"One track:\")\n",
    "\n",
    "print(dataset_as_snapshots[0][0])\n",
    "print(\"Shape:\", dataset_as_snapshots[0][0].shape)\n",
    "    "
   ],
   "id": "3fd6a145484a8175",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:22:40.359571Z",
     "start_time": "2024-06-24T17:22:40.277367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Function to add SOS and EOS tokens to each chunk\n",
    "def add_sos_eos_to_chunks(chunks):\n",
    "    new_chunks = []\n",
    "    for chunk in chunks:\n",
    "        # new_chunk = np.vstack([SOS_TOKEN, chunk, EOS_TOKEN]) eos token probably not neccessary\n",
    "        print(chunk.shape)\n",
    "        \n",
    "        #new_chunk = np.vstack([SOS_TOKEN, chunk])\n",
    "        new_chunk = np.insert(chunk,0, SOS_TOKEN)\n",
    "        new_chunks.append(new_chunk)\n",
    "    return new_chunks\n",
    "\n",
    "# Function to split sequences into chunks\n",
    "def split_into_chunks(sequence, chunk_size):\n",
    "    print(\"sequence:\", sequence.shape)\n",
    "    return [sequence[i:i + chunk_size] for i in range(0, len(sequence), chunk_size)]\n",
    "\n",
    "# Function to filter out short chunks while maintaining pairs\n",
    "def filter_short_chunks(chunks_1, chunks_2, min_length):\n",
    "    filtered_chunks_1 = []\n",
    "    filtered_chunks_2 = []\n",
    "    for chunk_1, chunk_2 in zip(chunks_1, chunks_2):\n",
    "        if len(chunk_1) >= min_length and len(chunk_2) >= min_length:\n",
    "            filtered_chunks_1.append(chunk_1)\n",
    "            filtered_chunks_2.append(chunk_2)\n",
    "    return filtered_chunks_1, filtered_chunks_2\n",
    "\n",
    "# Custom Dataset class\n",
    "class PianoDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Prepare the dataset with paired sequences and SOS/EOS tokens for each chunk\n",
    "def prepare_dataset(dataset_as_snapshots, chunk_size, min_length):\n",
    "    data = []\n",
    "    for song in dataset_as_snapshots:\n",
    "        track_1, track_2 = song\n",
    "        assert len(track_1) == len(track_2), \"Tracks must have the same length\"\n",
    "        \n",
    "        chunks_1 = split_into_chunks(track_1, chunk_size)\n",
    "        chunks_2 = split_into_chunks(track_2, chunk_size)\n",
    "        chunks_1, chunks_2 = filter_short_chunks(chunks_1, chunks_2, min_length)\n",
    "        \n",
    "        print(\"chunks diemsion:\", chunks_1[0].size)\n",
    "        \n",
    "        # Add SOS and EOS tokens to each chunk\n",
    "        chunks_1 = add_sos_eos_to_chunks(chunks_1)\n",
    "        chunks_2 = add_sos_eos_to_chunks(chunks_2)\n",
    "        \n",
    "        for x, y in zip(chunks_1, chunks_2):\n",
    "            data.append((x, y))\n",
    "    return data"
   ],
   "id": "9e4497f21b9fe814",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:22:40.897642Z",
     "start_time": "2024-06-24T17:22:40.360070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = prepare_dataset(dataset_as_snapshots, max_len, max_len)\n",
    "\n",
    "# Split the dataset using sklearn while maintaining pairs\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42, shuffle=True)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = PianoDataset(train_data)\n",
    "val_dataset = PianoDataset(val_data)\n",
    "test_dataset = PianoDataset(test_data)\n",
    "\n",
    "# Create DataLoaders for each subset with drop_last=True\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# Iterate over the DataLoader (example with train_loader)\n",
    "for batch in train_loader:\n",
    "    X, y = batch\n",
    "    print(X.shape, y.shape)\n",
    "    # X and y should both have shape (batch_size, chunk_size + 2, feature dimension) because of SOS and EOS tokens"
   ],
   "id": "cb31a759438e8e52",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# model definition",
   "id": "1ccb4a859bdc15e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:22:40.902998Z",
     "start_time": "2024-06-24T17:22:40.898402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sinusoidal positional embeddings\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional embeddings module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate sinusoidal positional embeddings\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "    \n",
    "# Transformer block with Attention and causal masking\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block with self-attention and causal masking.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=128, num_heads=4):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        # Layer normalization for input\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Multi-head self-attention mechanism\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, \n",
    "                                                    num_heads=num_heads, \n",
    "                                                    batch_first=True,\n",
    "                                                    dropout=0.1)\n",
    "\n",
    "        # Layer normalization for attention output\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Feedforward neural network\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, padding_mask):\n",
    "        # Create causal mask for Attention\n",
    "        bs, l, h = x.shape\n",
    "        mask = torch.triu(torch.ones(l, l, device=x.device), 1).bool()\n",
    "\n",
    "        # Layer normalization\n",
    "        norm_x = self.norm1(x)\n",
    "\n",
    "        # Apply multi-head Attention\n",
    "        x = self.multihead_attn(norm_x, norm_x, norm_x, attn_mask=mask, key_padding_mask=padding_mask)[0] + x\n",
    "\n",
    "        # Layer normalization\n",
    "        norm_x = self.norm2(x)\n",
    "\n",
    "        # Apply feedforward neural network\n",
    "        x = self.mlp(norm_x) + x\n",
    "        return x\n",
    "\n",
    "    \n",
    "# \"Decoder-Only\" Style Transformer with Attention\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    \"Decoder-Only\" Style Transformer with self-attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Token embeddings\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "\n",
    "        # Positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "\n",
    "        # List of Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # Mask for padding tokens\n",
    "        input_key_mask = input_seq == 0\n",
    "\n",
    "        # Embedding input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to token embeddings\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "\n",
    "        # Pass through Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs, padding_mask=input_key_mask)\n",
    "\n",
    "        # Output predictions\n",
    "        return self.fc_out(embs)"
   ],
   "id": "b8837a7ee1526cfc",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# initialize model and optimizer",
   "id": "6af1cc97cc03743a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:22:42.165488Z",
     "start_time": "2024-06-24T17:22:40.903571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Embedding Size\n",
    "hidden_size = 256\n",
    "\n",
    "# Number of transformer blocks\n",
    "num_layers = 8\n",
    "\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 8\n",
    "\n",
    "# Create model\n",
    "# num emb = wie viele verschiedene Tokens es geben kann bei 12 Tönen 2 ** 12 Möglichkeiten + 2 special tokens\n",
    "tf_generator = Transformer(num_emb=4098, num_layers=num_layers, \n",
    "                           hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(tf_generator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Scaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "# Custom transform that will randomly replace a token with <pad>\n",
    "# td = TokenDrop(prob=0.2)\n",
    "\n",
    "# Initialize training loss logger and entropy logger\n",
    "training_loss_logger = []\n",
    "entropy_logger = []"
   ],
   "id": "d13efc788ce692cf",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:22:42.169239Z",
     "start_time": "2024-06-24T17:22:42.166607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in tf_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ],
   "id": "232b3ef3ab2821c9",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "32ec0667abdaad11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:22:42.177695Z",
     "start_time": "2024-06-24T17:22:42.169717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Move data to GPU\n",
    "        input_melody, expected_harmony = batch\n",
    "        input_melody, expected_harmony = input_melody.to(device), expected_harmony.to(device)\n",
    "        \n",
    "        # shift input und output für das training zeug mit start token\n",
    "        input_melody = input_melody[:, 0:-1]\n",
    "        expected_harmony = expected_harmony[:, 1:]\n",
    "        \n",
    "        # Generate predictions\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = tf_generator(input_melody)\n",
    "        \n",
    "        #print(\"Training: prediction (model output) shape:\", logits.shape)\n",
    "        # Debug shapes and ranges\n",
    "        # debug_shapes_and_ranges(X, y, logits)\n",
    "        \n",
    "        # Calculate loss with masked cross-entropy\n",
    "        # ich glaube 0 steht in vorlage für padding token index -> habe ich hier anders\n",
    "        mask = (expected_harmony != 4096).float()\n",
    "        loss = (loss_fn(pred.transpose(1, 2), expected_harmony) * mask).sum()/mask.sum()\n",
    "        \n",
    "        # Backpropagation\n",
    "        opt.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "6e684eda0c67f47a",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "for epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):    \n",
    "    tf_generator.train()\n",
    "    steps = 0\n",
    "    for batch in train_loader:\n",
    "        # Convert text to tokenized input\n",
    "        # text_tokens = train_tranform(list(text)).to(device)\n",
    "        # bs = text_tokens.shape[0]\n",
    "        \n",
    "        # Randomly drop input tokens\n",
    "        # input_text = td(text_tokens[:, 0:-1])\n",
    "        # output_text = text_tokens[:, 1:]\n",
    "        \n",
    "        input_melody, expected_harmony = batch\n",
    "        input_melody, expected_harmony = input_melody.to(device), expected_harmony.to(device)\n",
    "        \n",
    "        # print(\"Input melody:\", input_melody.shape)\n",
    "        \n",
    "        # shift input und output für das training zeug\n",
    "        input_melody = input_melody[:, 0:-1]\n",
    "        expected_harmony = expected_harmony[:, 1:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Generate predictions\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = tf_generator(input_melody)\n",
    "\n",
    "        # Calculate loss with masked cross-entropy\n",
    "        # ich glaube 0 steht in vorlage für padding token index -> habe ich hier anders\n",
    "        mask = (expected_harmony != 4096).float()\n",
    "        loss = (loss_fn(pred.transpose(1, 2), expected_harmony) * mask).sum()/mask.sum()\n",
    "        \n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Log training loss and entropy\n",
    "        print(\"loss:\", loss)\n",
    "        training_loss_logger.append(loss.item())\n",
    "        with torch.no_grad():\n",
    "            dist = Categorical(logits=pred)\n",
    "            entropy_logger.append(dist.entropy().mean().item())"
   ],
   "id": "ef9861ba88aaa5b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "training",
   "id": "ced53828ad980cb9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:24:30.680664Z",
     "start_time": "2024-06-24T17:22:42.178163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_loop(tf_generator, optimizer, loss_fn, train_loader, device)\n",
    "    end_time = timer()\n",
    "    # val_loss = validation_loop(model, loss_fn, val_loader)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ],
   "id": "caf45bfb33d7d396",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing",
   "id": "2b7b3e4f7bc5714c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:29:43.069118Z",
     "start_time": "2024-06-24T17:29:43.065429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Set temperature for sampling\n",
    "temp = 0.75\n",
    "\n",
    "X, y = next(iter(test_loader))\n",
    "\n",
    "print(f\"melody: {X.shape}, Harmony: {y.shape}\")\n",
    "\n",
    "# get single sequence\n",
    "X, y = X[1], y[1]\n",
    "\n",
    "print(f\"melody: {X}, Harmony: {y}\")"
   ],
   "id": "8e6ce66c1521ae35",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:29:45.162907Z",
     "start_time": "2024-06-24T17:29:45.159249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "log_tokens = [X.unsqueeze(0)]\n",
    "print(log_tokens)"
   ],
   "id": "4af01f24b950ca80",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:29:48.118855Z",
     "start_time": "2024-06-24T17:29:47.326944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "tf_generator.eval()\n",
    "\n",
    "# Generate tokens\n",
    "with torch.no_grad():    \n",
    "    for i in range(200):\n",
    "        # Concatenate tokens from previous iterations\n",
    "        input_tokens = torch.cat(log_tokens, 1)\n",
    "        input_tokens = input_tokens.to(device)\n",
    "        print(input_tokens)\n",
    "        print(input_tokens.shape)\n",
    "        \n",
    "        \n",
    "        # Get model predictions for the next token\n",
    "        data_pred = tf_generator(input_tokens)\n",
    "        \n",
    "        # Sample the next token from the distribution of probabilities\n",
    "        dist = Categorical(logits=data_pred[:, -1] / temp) #Model outputs all tokens plus the one new token-> so last token is the new one\n",
    "        next_tokens = dist.sample().reshape(1, 1)\n",
    "         \n",
    "        # Append the sampled token to the list of generated tokens\n",
    "        log_tokens.append(next_tokens.cpu())\n",
    "        \n",
    "        # Check for end-of-sequence token and stop generation\n",
    "        if next_tokens.item() == -1:\n",
    "            break\n"
   ],
   "id": "640f7964fb02d523",
   "execution_count": 43,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:29:53.018117Z",
     "start_time": "2024-06-24T17:29:53.015599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_tokens = log_tokens\n",
    "print(\"=========\")\n",
    "print()\n",
    "print(len(all_tokens))\n",
    "print(\"==================\")\n",
    "all_tokens = np.squeeze(torch.cat(all_tokens, 1).numpy())\n",
    "print(all_tokens)\n",
    "print(all_tokens.shape)"
   ],
   "id": "56e1a15f046f7117",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "convert generated tokens back to snapshots",
   "id": "fbd2e2be807cd65a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:29:56.018094Z",
     "start_time": "2024-06-24T17:29:56.007074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "harmony_tokens = all_tokens[-200:]\n",
    "melody_tokens = X.numpy()\n",
    "# remove start token\n",
    "melody_tokens = melody_tokens[1:]\n",
    "\n",
    "# Convert harmony to snapshots\n",
    "harmony_snapshots = []\n",
    "for token in harmony_tokens:\n",
    "    harmony_snapshots.append(base10_to_binary_one_hot(token, 12))\n",
    "\n",
    "# Convert melody to snapshots\n",
    "melody_snapshots = []\n",
    "for token in melody_tokens:\n",
    "    melody_snapshots.append(base10_to_binary_one_hot(token, 12))\n",
    "\n",
    "harmony_snapshots = np.array(harmony_snapshots)\n",
    "print(\"Harmony_tokens:\", harmony_tokens)\n",
    "\n",
    "print(\"harmony snapshots:\")\n",
    "for snapshot in harmony_snapshots:\n",
    "    print(snapshot)\n",
    "    \n",
    "print(\"===================\")\n",
    "\n",
    "melody_snapshots = np.array(melody_snapshots)\n",
    "print(\"melody tokens:\", melody_tokens)\n",
    "\n",
    "print(\"melody snapshots:\")\n",
    "for snapshot in melody_snapshots:\n",
    "    print(snapshot)"
   ],
   "id": "c308ad2a17a278ed",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "blow up to 88 keys and convert to midi",
   "id": "e3fc43357d84291b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:29:59.759291Z",
     "start_time": "2024-06-24T17:29:59.753186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_to_88_keys(one_hot_vector, start_key=21, octaves_higher=0, total_keys=88):\n",
    "    \"\"\"\n",
    "    Pad a one-hot encoded vector to fit 88 keys of a piano and place it a specified number of octaves higher.\n",
    "\n",
    "    Parameters:\n",
    "    one_hot_vector (np.ndarray): Input one-hot encoded vector.\n",
    "    start_key (int): The starting key in the 88-key piano.\n",
    "    octaves_higher (int): Number of octaves to shift the starting key higher.\n",
    "    total_keys (int): The total number of keys on the piano (default is 88).\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Padded one-hot encoded vector with 88 keys.\n",
    "    \"\"\"\n",
    "    # Calculate the new starting key based on the number of octaves higher\n",
    "    start_key = start_key + (octaves_higher * 12)\n",
    "    \n",
    "    # Initialize the full 88-key vector with zeros\n",
    "    padded_vector = np.zeros(total_keys, dtype=int)\n",
    "    end_key = start_key + len(one_hot_vector)\n",
    "    \n",
    "    if end_key > total_keys:\n",
    "        raise ValueError(\"The one-hot vector is too long to fit in the 88 keys starting from the given start_key.\")\n",
    "    \n",
    "    padded_vector[start_key:end_key] = one_hot_vector\n",
    "    return padded_vector\n",
    "\n",
    "def pad_sequence_of_one_hot_vectors(sequence, start_key=21, octaves_higher=0, total_keys=88):\n",
    "    \"\"\"\n",
    "    Pad a sequence of one-hot encoded vectors to fit 88 keys of a piano, placing each one a specified number of octaves higher.\n",
    "\n",
    "    Parameters:\n",
    "    sequence (list of np.ndarray): Sequence of one-hot encoded vectors.\n",
    "    start_key (int): The starting key in the 88-key piano.\n",
    "    octaves_higher (int): Number of octaves to shift the starting key higher.\n",
    "    total_keys (int): The total number of keys on the piano (default is 88).\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: 2D array where each row is a padded one-hot encoded vector with 88 keys.\n",
    "    \"\"\"\n",
    "    padded_vectors = [pad_to_88_keys(vector, start_key, octaves_higher, total_keys) for vector in sequence]\n",
    "    return np.stack(padded_vectors)\n"
   ],
   "id": "df0f19d60d59f1a7",
   "execution_count": 46,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T17:30:01.416276Z",
     "start_time": "2024-06-24T17:30:01.379685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data_visualization import snapshot_to_midi\n",
    "\n",
    "\n",
    "\n",
    "melody_snapshots = pad_sequence_of_one_hot_vectors(melody_snapshots, 21, 3)\n",
    "harmony_snapshots = pad_sequence_of_one_hot_vectors(harmony_snapshots, 21, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "melody_harmony = [melody_snapshots, harmony_snapshots]\n",
    "snapshot_to_midi.create_midi_from_snapshots(melody_harmony, [\"melody\", \"Generated harmony\"], 0.05,\n",
    "                                            \"/transformer_decoder_training/jupyter_notebooks/piano_training/outputs\", \"tokenized.mid\")"
   ],
   "id": "821b6a19adac399d",
   "execution_count": 47,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
