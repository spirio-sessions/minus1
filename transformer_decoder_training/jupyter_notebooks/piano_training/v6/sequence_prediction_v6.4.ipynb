{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model with MSE Loss",
   "id": "d69877fa8ba6d4ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T23:11:02.713131Z",
     "start_time": "2024-07-31T23:11:01.446673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "fcdf888398fb8767",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T23:11:03.000650Z",
     "start_time": "2024-07-31T23:11:02.714811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sos_token = np.full((1, 24), 1)\n",
    "pad_token = np.full((1, 24), 2)\n",
    "pad_token = torch.tensor(pad_token, device=device)\n",
    "\n",
    "dataset_dir = \"/home/falaxdb/Repos/minus1/datasets/maestro_v3_split/hands_split_into_seperate_midis\"\n",
    "snapshot_intervall = 0.05\n",
    "\n",
    "# Define other parameters\n",
    "batch_size = 64\n",
    "seq_length = 512\n",
    "stride = 256\n",
    "\n",
    "test_size=0.3"
   ],
   "id": "21962fade3b9a5ff",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T23:11:08.020325Z",
     "start_time": "2024-07-31T23:11:03.002987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformer_decoder_training.dataprep_transformer.prepare_dataloader_complete import prepare_dataset_as_dataloaders\n",
    "\n",
    "# Load Data\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_dataset_as_dataloaders(dataset_dir, snapshot_intervall, batch_size, seq_length, stride, test_size, sos_token)"
   ],
   "id": "41359a5db32731cf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed dataset (306/1038):  29%|██▉       | 306/1038 [00:04<00:09, 76.22it/s] Exception ignored in: \n",
      "<bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f51b80eb770>>Traceback (most recent call last):\n",
      "  File \"/home/falaxdb/Repos/minus1/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f51b80eb770>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/falaxdb/Repos/minus1/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformer_decoder_training\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataprep_transformer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprepare_dataloader_complete\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m prepare_dataset_as_dataloaders\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Load Data\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m train_loader, val_loader, test_loader \u001B[38;5;241m=\u001B[39m \u001B[43mprepare_dataset_as_dataloaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msnapshot_intervall\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msos_token\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Repos/minus1/transformer_decoder_training/dataprep_transformer/prepare_dataloader_complete.py:25\u001B[0m, in \u001B[0;36mprepare_dataset_as_dataloaders\u001B[0;34m(dataset_dir, snapshot_intervall, batch_size, seq_length, stride, test_size, sos_token, amount)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \n\u001B[1;32m     13\u001B[0m \u001B[38;5;124;03m:param dataset_dir:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;124;03m    then right hand index wise\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# load data\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m dataset_as_snapshots \u001B[38;5;241m=\u001B[39m \u001B[43mdataset_snapshot\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_dataset_multithreaded\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msnapshot_intervall\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m                                                                      \u001B[49m\u001B[43mamount\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamount\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# filter snapshots to 88 piano notes\u001B[39;00m\n\u001B[1;32m     28\u001B[0m dataset_as_snapshots \u001B[38;5;241m=\u001B[39m dataset_snapshot\u001B[38;5;241m.\u001B[39mfilter_piano_range(dataset_as_snapshots)\n",
      "File \u001B[0;32m~/Repos/minus1/data_preperation/dataset_snapshot.py:219\u001B[0m, in \u001B[0;36mprocess_dataset_multithreaded\u001B[0;34m(dataset_dir, interval, pattern, amount)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m concurrent\u001B[38;5;241m.\u001B[39mfutures\u001B[38;5;241m.\u001B[39mProcessPoolExecutor() \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[1;32m    216\u001B[0m     future_to_midi \u001B[38;5;241m=\u001B[39m {executor\u001B[38;5;241m.\u001B[39msubmit(__process_single_midi, midi_file, interval): midi_file\n\u001B[1;32m    217\u001B[0m                       \u001B[38;5;28;01mfor\u001B[39;00m group_files \u001B[38;5;129;01min\u001B[39;00m midi_files\u001B[38;5;241m.\u001B[39mvalues() \u001B[38;5;28;01mfor\u001B[39;00m midi_file \u001B[38;5;129;01min\u001B[39;00m group_files\u001B[38;5;241m.\u001B[39mvalues()}\n\u001B[0;32m--> 219\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfuture\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconcurrent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfutures\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_completed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfuture_to_midi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmidi_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msnapshots_array\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfiles_as_snapshots\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmidi_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msnapshots_array\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib64/python3.12/concurrent/futures/_base.py:243\u001B[0m, in \u001B[0;36mas_completed\u001B[0;34m(fs, timeout)\u001B[0m\n\u001B[1;32m    238\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m wait_timeout \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    239\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m(\n\u001B[1;32m    240\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m (of \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m) futures unfinished\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m (\n\u001B[1;32m    241\u001B[0m                 \u001B[38;5;28mlen\u001B[39m(pending), total_futures))\n\u001B[0;32m--> 243\u001B[0m \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwait_timeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m waiter\u001B[38;5;241m.\u001B[39mlock:\n\u001B[1;32m    246\u001B[0m     finished \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39mfinished_futures\n",
      "File \u001B[0;32m/usr/lib64/python3.12/threading.py:655\u001B[0m, in \u001B[0;36mEvent.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    653\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[1;32m    654\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[0;32m--> 655\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    656\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[0;32m/usr/lib64/python3.12/threading.py:355\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    353\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 355\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    356\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    357\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# set parameters\n",
    "# Learning rate for the optimizer\n",
    "learning_rate = 1e-3\n",
    "# Number of epochs for training\n",
    "num_epochs = 25\n",
    "# basically input dimension before embedding\n",
    "num_emb = 24\n",
    "# output dimension\n",
    "num_output_dim = 12\n",
    "# size after embedding for feed forward neural network\n",
    "hidden_size = 256\n",
    "# Number of transformer blocks\n",
    "num_layers = 8\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 8"
   ],
   "id": "44e7f395616b88a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformer_decoder_training.models.transformer_decoder_2 import Transformer\n",
    "\n",
    "model = Transformer(num_emb=num_emb, num_layers=num_layers, hidden_size=hidden_size, num_heads=num_heads, num_output_dim=num_output_dim).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "# loss function should be one that can handle multi one hot encoded vectors\n",
    "# Klammern nicht vergessen\n",
    "# Chat gpt says BCEWithLogitsLoss is more stable\n",
    "loss_fn = nn.SmoothL1Loss()"
   ],
   "id": "11261f45561a5a2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare directory for saving model state dict, parameters, loss, etc.\n",
    "# leave out \".pth\"\n",
    "model_state_dict_filepath = \"/home/falaxdb/Repos/minus1/transformer_decoder_training/saved_files/saved_models/model_1_huber_loss\""
   ],
   "id": "64bb06b6e9e4852c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "from transformer_decoder_training.training import training_1\n",
    "from IPython.display import clear_output\n",
    "from data_visualization.Visualization import plot_losses\n",
    "\n",
    "# Initialize lists to store loss values\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    start_time = timer()\n",
    "    train_loss = training_1.train_loop_harmony_only(model, optimizer, loss_fn, train_loader, pad_token, device)\n",
    "    end_time = timer()\n",
    "    val_loss = training_1.validation_loop_harmony_only(model, loss_fn, val_loader, pad_token, device)\n",
    "    \n",
    "    # Store the loss values\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "    plot_losses(train_losses, val_losses, model_state_dict_filepath + \"loss_plot.png\")"
   ],
   "id": "483132f9c6525c68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), model_state_dict_filepath + \".pth\")\n",
    "# save the parameters\n",
    "model_params = {\n",
    "    \"model_topology\": str(model),\n",
    "    \"num_emb\": num_emb,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"training_params\": {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"loss_fn\": loss_fn.__class__.__name__\n",
    "    },\n",
    "    \"training_data_params\": {\n",
    "        \"sos_token\": sos_token.tolist(),\n",
    "        \"pad_token\": pad_token.tolist(),\n",
    "        \"snapshot_intervall\": snapshot_intervall,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"sequence_length\": seq_length,\n",
    "        \"stride\": stride\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the model parameters separately as a JSON file\n",
    "params_path = model_state_dict_filepath + \"_model_params.json\"\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(model_params, f)"
   ],
   "id": "ee4a33fae79d3341",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
