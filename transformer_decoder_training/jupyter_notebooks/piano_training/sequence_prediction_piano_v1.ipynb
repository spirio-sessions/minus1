{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ist ein Transformer Encoder Decoder kein Decoder only Transformer",
   "id": "76cdad594e4accd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Jetzt der 1. Versuch für unsere eigenen Piano-Daten",
   "id": "197553aeb005392a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 0. Imports",
   "id": "f89af126ceadd9d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:28:05.396919Z",
     "start_time": "2024-06-17T17:28:05.395160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ],
   "id": "d5b1741eeb343552",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "gpu nutzen",
   "id": "14567bc9b1cf5791"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:28:05.441154Z",
     "start_time": "2024-06-17T17:28:05.438818Z"
    }
   },
   "cell_type": "code",
   "source": "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "id": "41ff7d21d02866ef",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Daten Vorbereiten\n",
    "\n",
    "## Ideen für Special Tokens\n",
    "\n",
    "Padding, Start of Stream (SOS), und End of Stream (EOS) Tokens sind wichtige Konzepte beim Arbeiten mit Sequenzdaten, insbesondere in Modellen wie Transformern. Sie helfen dabei, Sequenzen auf eine einheitliche Länge zu bringen und dem Modell zu signalisieren, wann eine Eingabe- oder Ausgabesequenz beginnt und endet.\n",
    "\n",
    "### Padding Token\n",
    "\n",
    "- **Padding Token** wird verwendet, um alle Sequenzen auf eine einheitliche Länge zu bringen.\n",
    "- Da Ihre Daten aus binären Vektoren bestehen, könnte ein Padding Token einfach ein Vektor aus Nullen sein (d.h. kein Tastendruck).\n",
    "\n",
    "### Start of Stream (SOS) Token\n",
    "\n",
    "- **Start of Stream Token** signalisiert den Beginn einer Sequenz.\n",
    "- Ein möglicher SOS-Token könnte ein Vektor sein, bei dem nur das erste Element auf 1 gesetzt ist und der Rest auf 0 (d.h. `[1, 0, 0, ..., 0]`).\n",
    "\n",
    "### End of Stream (EOS) Token\n",
    "\n",
    "- **End of Stream Token** signalisiert das Ende einer Sequenz.\n",
    "- Ein möglicher EOS-Token könnte ein Vektor sein, bei dem nur das letzte Element auf 1 gesetzt ist und der Rest auf 0 (d.h. `[0, 0, ..., 0, 1]`).\n",
    "\n",
    "\n",
    "## Datenaufteilung\n",
    "\n",
    "- Wie Sollen die Daten am besten aufgeteilt werden? nach den einzelnen Songs? erst in Sequenzen aufteilen und dann splitten?\n",
    "- Alle songs sind verschieden lang -> in Sequenzen aufteilen und zu kurze sequenzen wegwerfen? oder lieber padden\n",
    "- Wann sollen die Start-of stream und end of stream tokens eingeführt werden -> wahrscheinlich für jede Sequenz.\n",
    "    - Bei der Inferenz dann halt schauen, weil Modell hat ja nur die Bestimmte sequenzlänge gelernt.\n",
    "\n",
    "- Ich mach eine Sequenzlänge von 50 Snapshots -> bei 0.1 pro snapshot also 5 Sekunden"
   ],
   "id": "247bf734d58c4b04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1 Definiere Code zum laden der Daten",
   "id": "b77709120b8d7467"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:28:05.451771Z",
     "start_time": "2024-06-17T17:28:05.442016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import mido\n",
    "import os\n",
    "import fnmatch\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "def snapshot_active_notes_from_midi(file_path, interval):\n",
    "    \"\"\"\n",
    "    Processes a MIDI file and returns snapshots of active notes at specified intervals.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the MIDI file.\n",
    "        interval (float): The interval (in seconds) at which snapshots are taken.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of snapshots, where each snapshot is a list of active notes.\n",
    "    \"\"\"\n",
    "    mid = mido.MidiFile(file_path)\n",
    "    snapshots = []\n",
    "    active_notes = [0] * 128\n",
    "    current_time = 0\n",
    "    snapshot_time = 0\n",
    "    previous_event_time = 0\n",
    "\n",
    "    for msg in mid:\n",
    "        current_time += msg.time\n",
    "\n",
    "        while current_time >= snapshot_time + interval:\n",
    "            if current_time == previous_event_time:\n",
    "                break\n",
    "            snapshots.append(active_notes[:])\n",
    "            snapshot_time += interval\n",
    "\n",
    "        if msg.type == 'note_on':\n",
    "            if msg.velocity == 0:\n",
    "                active_notes[msg.note] = 0\n",
    "            else:\n",
    "                active_notes[msg.note] = 1\n",
    "        elif msg.type == 'note_off':\n",
    "            active_notes[msg.note] = 0\n",
    "\n",
    "        previous_event_time = current_time\n",
    "\n",
    "    return np.array(snapshots)\n",
    "\n",
    "\n",
    "def find_midi_files(root_dir, pattern=None):\n",
    "    \"\"\"\n",
    "    Recursively searches for MIDI files in the specified root directory and groups them\n",
    "    based on their base patterns, ensuring each group has exactly one 'rightH' and one 'leftH' file.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): The root directory to start the search.\n",
    "        pattern (str, optional): An optional pattern to filter the MIDI files. Only files\n",
    "                                 containing this pattern in their names will be included.\n",
    "\n",
    "    Returns:\n",
    "        defaultdict: A dictionary where each key is a base pattern and the value is a dictionary\n",
    "                     with 'rightH' and 'leftH' keys for corresponding file paths.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any group does not have both 'rightH' and 'leftH' MIDI files.\n",
    "\n",
    "    Example:\n",
    "        midi_files = find_midi_files('/path/to/root_dir')\n",
    "        for base_pattern, files in midi_files.items():\n",
    "            print(f\"Group: {base_pattern}\")\n",
    "            print(f\" - Right Hand: {files['rightH']}\")\n",
    "            print(f\" - Left Hand: {files['leftH']}\")\n",
    "    \"\"\"\n",
    "    midi_groups = defaultdict(dict)\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if fnmatch.fnmatch(filename.lower(), '*.midi') or fnmatch.fnmatch(filename.lower(), '*.mid'):\n",
    "                if pattern is None or fnmatch.fnmatch(filename.lower(), f'*{pattern.lower()}*'):\n",
    "                    filepath = os.path.join(dirpath, filename)\n",
    "                    parts = filename.lower().split('_')\n",
    "                    if 'righth' in parts[-1]:\n",
    "                        base_pattern = '_'.join(parts[:-1])\n",
    "                        midi_groups[base_pattern]['rightH'] = filepath\n",
    "                    elif 'lefth' in parts[-1]:\n",
    "                        base_pattern = '_'.join(parts[:-1])\n",
    "                        midi_groups[base_pattern]['leftH'] = filepath\n",
    "\n",
    "    # Ensure each group has exactly one 'rightH' and one 'leftH'\n",
    "    for base_pattern, files in midi_groups.items():\n",
    "        if 'rightH' not in files or 'leftH' not in files:\n",
    "            raise ValueError(f\"Group {base_pattern} does not have both 'rightH' and 'leftH' MIDI files.\")\n",
    "\n",
    "    return midi_groups\n",
    "\n",
    "\n",
    "def trim_snapshots(group_snapshots):\n",
    "    \"\"\"\n",
    "    Trims the leading and trailing empty snapshots for each group of snapshots.\n",
    "\n",
    "    Args:\n",
    "        group_snapshots (list): A list of numpy arrays where each array represents snapshots\n",
    "                                for a MIDI file in the group.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of trimmed numpy arrays with empty snapshots removed from the beginning and end.\n",
    "    \"\"\"\n",
    "    min_start = float('inf')\n",
    "    max_end = 0\n",
    "\n",
    "    for snapshots in group_snapshots:\n",
    "        non_empty_indices = np.where(snapshots.any(axis=1))[0]\n",
    "        if non_empty_indices.size > 0:\n",
    "            first_non_empty = non_empty_indices[0]\n",
    "            last_non_empty = non_empty_indices[-1]\n",
    "            if first_non_empty < min_start:\n",
    "                min_start = first_non_empty\n",
    "            if last_non_empty > max_end:\n",
    "                max_end = last_non_empty\n",
    "\n",
    "    trimmed_group_snapshots = [snapshots[min_start:max_end + 1] for snapshots in group_snapshots]\n",
    "\n",
    "    return trimmed_group_snapshots\n",
    "\n",
    "\n",
    "def __process_single_midi(midi_file, interval):\n",
    "    \"\"\"\n",
    "    Helper function to process a single MIDI file and return its snapshots.\n",
    "\n",
    "    Args:\n",
    "        midi_file (str): The path to the MIDI file.\n",
    "        interval (float): The interval (in seconds) at which snapshots are taken.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the MIDI file path and the array of snapshots.\n",
    "    \"\"\"\n",
    "    snapshots_array = snapshot_active_notes_from_midi(midi_file, interval)\n",
    "    return midi_file, snapshots_array\n",
    "\n",
    "\n",
    "def process_dataset(dataset_dir, interval, pattern=None, amount=0):\n",
    "    \"\"\"\n",
    "    Processes a dataset of MIDI files, taking snapshots of active notes at specified intervals\n",
    "    and grouping related files together.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str): The directory containing the dataset of MIDI files.\n",
    "        interval (float): The interval (in seconds) at which snapshots are taken.\n",
    "        pattern (str, optional): An optional pattern to filter the MIDI files.\n",
    "        amount (int, optional): An optional amount of how many songs should be processed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of snapshots for each group of MIDI files. The group will always have the right hand first [0]\n",
    "        and then the left hand [1]\n",
    "    \"\"\"\n",
    "    midi_files = find_midi_files(dataset_dir, pattern)\n",
    "\n",
    "    # limit amount of files\n",
    "    if amount > 0:\n",
    "        midi_files = {k: midi_files[k] for k in list(midi_files)[:amount]}\n",
    "\n",
    "    files_as_snapshots = []\n",
    "    filenames = []\n",
    "\n",
    "    total_files = sum(len(files) for files in midi_files.values())\n",
    "    progress_bar = tqdm(total=total_files)\n",
    "\n",
    "    for base_pattern, group_files in midi_files.items():\n",
    "        group_snapshots = []\n",
    "        for hand in ['rightH', 'leftH']:\n",
    "            midi_file = group_files[hand]\n",
    "            snapshots_array = snapshot_active_notes_from_midi(midi_file, interval)\n",
    "            group_snapshots.append(snapshots_array)\n",
    "            filenames.append(midi_file)\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_description(f\"Processed dataset ({progress_bar.n}/{progress_bar.total})\")\n",
    "\n",
    "        trimmed_group_snapshots = trim_snapshots(group_snapshots)\n",
    "        files_as_snapshots.append(trimmed_group_snapshots)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    return files_as_snapshots\n",
    "\n",
    "\n",
    "def process_dataset_multithreaded(dataset_dir, interval, pattern=None, amount=0):\n",
    "    \"\"\"\n",
    "    Processes a dataset of MIDI files, taking snapshots of active notes at specified intervals,\n",
    "    grouping related files together, and using multithreading for efficiency.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str): The directory containing the dataset of MIDI files.\n",
    "        interval (float): The interval (in seconds) at which snapshots are taken.\n",
    "        pattern (str, optional): An optional pattern to filter the MIDI files.\n",
    "        amount (int, optional): An optional amount of how many songs should be processed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of snapshots for each group of MIDI files. The group will always have the right hand first [0]\n",
    "        and then the left hand [1]\n",
    "    \"\"\"\n",
    "    midi_files = find_midi_files(dataset_dir, pattern)\n",
    "\n",
    "    # limit ammount of files\n",
    "    if amount > 0:\n",
    "        midi_files = {k: midi_files[k] for k in list(midi_files)[:amount]}\n",
    "\n",
    "    files_as_snapshots = []\n",
    "\n",
    "    total_files = sum(len(files) for files in midi_files.values())\n",
    "    progress_bar = tqdm(total=total_files)\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        future_to_midi = {executor.submit(__process_single_midi, midi_file, interval): midi_file\n",
    "                          for group_files in midi_files.values() for midi_file in group_files.values()}\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_midi):\n",
    "            midi_file, snapshots_array = future.result()\n",
    "            files_as_snapshots.append((midi_file, snapshots_array))\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_description(f\"Processed dataset ({progress_bar.n}/{progress_bar.total})\")\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    grouped_snapshots = defaultdict(dict)\n",
    "    for midi_file, snapshots_array in files_as_snapshots:\n",
    "        base_pattern = '_'.join(os.path.basename(midi_file).lower().split('_')[:-1])\n",
    "        if 'righth' in midi_file.lower():\n",
    "            grouped_snapshots[base_pattern]['rightH'] = snapshots_array\n",
    "        elif 'lefth' in midi_file.lower():\n",
    "            grouped_snapshots[base_pattern]['leftH'] = snapshots_array\n",
    "\n",
    "    final_grouped_snapshots = []\n",
    "    for group in grouped_snapshots.values():\n",
    "        if group['rightH'] is not None and group['leftH'] is not None:\n",
    "            final_grouped_snapshots.append(trim_snapshots([group['rightH'], group['leftH']]))\n",
    "\n",
    "    print(f\"Processed {len(files_as_snapshots)} of {total_files} files\")\n",
    "\n",
    "    return final_grouped_snapshots\n",
    "\n",
    "\n",
    "def filter_piano_range(grouped_snapshots):\n",
    "    \"\"\"\n",
    "    Filters the snapshots to keep only the notes in the piano range (MIDI notes 21 to 108).\n",
    "\n",
    "    Args:\n",
    "        grouped_snapshots (list): A list of lists, where each sublist contains numpy arrays of snapshots\n",
    "                                  for a group of MIDI files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists, where each sublist contains numpy arrays of filtered snapshots\n",
    "              for a group of MIDI files, keeping only the piano range notes.\n",
    "    \"\"\"\n",
    "    filtered_groups = []\n",
    "\n",
    "    for group in grouped_snapshots:\n",
    "        filtered_group = []\n",
    "        for snapshots in group:\n",
    "            filtered_snapshots = [snapshot[21:109] for snapshot in snapshots]\n",
    "            filtered_group.append(np.array(filtered_snapshots))\n",
    "        filtered_groups.append(filtered_group)\n",
    "\n",
    "    return filtered_groups\n",
    "\n",
    "# Function to map MIDI note to octave position\n",
    "def map_to_octave(note):\n",
    "    return note % 12\n",
    "\n",
    "# Function to compress a single track\n",
    "def compress_track(track):\n",
    "    compressed_track = np.zeros((track.shape[0], 12))\n",
    "    for i, snapshot in enumerate(track):\n",
    "        for note_index, is_active in enumerate(snapshot):\n",
    "            if is_active:\n",
    "                octave_position = map_to_octave(note_index)\n",
    "                compressed_track[i][octave_position] = 1\n",
    "    return compressed_track\n",
    "\n",
    "def compress_dataset(dataset):\n",
    "    compressed_dataset = []\n",
    "    for song in dataset_as_snapshots:\n",
    "        compressed_song = []\n",
    "        for track in song:\n",
    "            compressed_track = compress_track(track)\n",
    "            compressed_song.append(compressed_track)\n",
    "        compressed_dataset.append(compressed_song)\n",
    "    return compressed_dataset"
   ],
   "id": "7fbd94f8a37edac8",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.2 Lade die Snapshots aus dem Midi datensatz\n",
    "\n",
    "jeder song hat: \n",
    "- zuerst die rechte hand [0]\n",
    "- dann die linke hand [1]\n"
   ],
   "id": "3ed93f64623e2635"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:28:36.395909Z",
     "start_time": "2024-06-17T17:28:05.452375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create snapshots\n",
    "dataset_as_snapshots = process_dataset_multithreaded(\"/home/falaxdb/Repos/Learn-ml/Transformer-pytorch/piano_data/maestro_v3/hands_split_into_seperate_midis\", 0.1)\n",
    "# filter snapshots to 88 piano notes\n",
    "dataset_as_snapshots = filter_piano_range(dataset_as_snapshots)\n",
    "# compress data into one octave\n",
    "dataset_as_snapshots = compress_dataset(dataset_as_snapshots)\n",
    "\n",
    "for song in dataset_as_snapshots:\n",
    "    print(\"song:\")\n",
    "    for track in song:\n",
    "        print(track.shape)"
   ],
   "id": "409c6c9eafcb35de",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### The 88 Keys are now compressed into one oktave 12 keys",
   "id": "c3bd983d02358436"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.3 Daten Batchen, aufteilen und in Richtigen Dataloader packen",
   "id": "2f1a03da39cf1cf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:28:36.398567Z",
     "start_time": "2024-06-17T17:28:36.396779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SOS und EOS Tokens definieren\n",
    "\n",
    "# Define SOS and EOS tokens as global variables\n",
    "SOS_TOKEN = np.full((1, 12), 2)  # SOS token representation with 2\n",
    "EOS_TOKEN = np.full((1, 12), 3)  # EOS token representation with 3\n",
    "\n",
    "UNK_IDX = 5 #Brauchen wir glaube ich nicht\n",
    "PAD_IDX = 4 #Brauchen wir auch nicht, weil die sequenzen alle genau richtig lang sind"
   ],
   "id": "1a8cf5d825cfcc6a",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:28:36.944853Z",
     "start_time": "2024-06-17T17:28:36.399008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Function to add SOS and EOS tokens to each chunk\n",
    "def add_sos_eos_to_chunks(chunks):\n",
    "    new_chunks = []\n",
    "    for chunk in chunks:\n",
    "        new_chunk = np.vstack([SOS_TOKEN, chunk, EOS_TOKEN])\n",
    "        new_chunks.append(new_chunk)\n",
    "    return new_chunks\n",
    "\n",
    "# Function to split sequences into chunks\n",
    "def split_into_chunks(sequence, chunk_size):\n",
    "    return [sequence[i:i + chunk_size] for i in range(0, len(sequence), chunk_size)]\n",
    "\n",
    "# Function to filter out short chunks while maintaining pairs\n",
    "def filter_short_chunks(chunks_1, chunks_2, min_length):\n",
    "    filtered_chunks_1 = []\n",
    "    filtered_chunks_2 = []\n",
    "    for chunk_1, chunk_2 in zip(chunks_1, chunks_2):\n",
    "        if len(chunk_1) >= min_length and len(chunk_2) >= min_length:\n",
    "            filtered_chunks_1.append(chunk_1)\n",
    "            filtered_chunks_2.append(chunk_2)\n",
    "    return filtered_chunks_1, filtered_chunks_2\n",
    "\n",
    "# Custom Dataset class\n",
    "class PianoDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Prepare the dataset with paired sequences and SOS/EOS tokens for each chunk\n",
    "def prepare_dataset(dataset_as_snapshots, chunk_size, min_length):\n",
    "    data = []\n",
    "    for song in dataset_as_snapshots:\n",
    "        track_1, track_2 = song\n",
    "        assert len(track_1) == len(track_2), \"Tracks must have the same length\"\n",
    "        \n",
    "        chunks_1 = split_into_chunks(track_1, chunk_size)\n",
    "        chunks_2 = split_into_chunks(track_2, chunk_size)\n",
    "        chunks_1, chunks_2 = filter_short_chunks(chunks_1, chunks_2, min_length)\n",
    "        \n",
    "        # Add SOS and EOS tokens to each chunk\n",
    "        chunks_1 = add_sos_eos_to_chunks(chunks_1)\n",
    "        chunks_2 = add_sos_eos_to_chunks(chunks_2)\n",
    "        \n",
    "        for x, y in zip(chunks_1, chunks_2):\n",
    "            data.append((x, y))\n",
    "    return data\n",
    "\n",
    "# Define your dataset parameters\n",
    "chunk_size = 50  # Define the chunk size you want\n",
    "min_length = chunk_size  # Minimum length to keep a chunk\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Prepare the dataset\n",
    "data = prepare_dataset(dataset_as_snapshots, chunk_size, min_length)\n",
    "\n",
    "# Split the dataset using sklearn while maintaining pairs\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42, shuffle=True)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = PianoDataset(train_data)\n",
    "val_dataset = PianoDataset(val_data)\n",
    "test_dataset = PianoDataset(test_data)\n",
    "\n",
    "# Create DataLoaders for each subset with drop_last=True\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# Iterate over the DataLoader (example with train_loader)\n",
    "for batch in train_loader:\n",
    "    X, y = batch\n",
    "    print(X.shape, y.shape)\n",
    "    # X and y should both have shape (batch_size, chunk_size + 2, 88) because of SOS and EOS tokens\n"
   ],
   "id": "ededfd5b09434ff3",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Explanation of the Code\n",
    "\n",
    "1. **Splitting and Filtering Chunks**:\n",
    "   - The `split_into_chunks` function splits each sequence into fixed-length chunks.\n",
    "   - The `filter_short_chunks` function ensures that only pairs of chunks (from `src` and `tgt`) that both meet the minimum length requirement are kept. This maintains the alignment of the sequences.\n",
    "\n",
    "2. **Preparing the Dataset**:\n",
    "   - The `prepare_dataset` function processes each song, splits the tracks into chunks, and applies the filtering function to ensure that the chunks are correctly paired.\n",
    "\n",
    "3. **Creating the Datasets**:\n",
    "   - The data is split into training, validation, and test sets using `train_test_split` from `sklearn`, ensuring that the pairs are maintained.\n",
    "\n",
    "4. **Creating the DataLoaders**:\n",
    "   - DataLoaders are created for each subset with `drop_last=True` to ensure that only complete batches are processed.\n",
    "\n",
    "By following this approach, the original alignment between `src` and `tgt` sequences is preserved, ensuring that the data is correctly prepared for training your Transformer model."
   ],
   "id": "f5344c7640278043"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Explenation Batch iteration\n",
    "\n",
    "The line `X, y = batch` is unpacking the batch of data retrieved from the DataLoader into two variables, `X` and `y`. Here’s a detailed explanation of what happens at this line:\n",
    "\n",
    "### Explanation of `X, y = batch`\n",
    "\n",
    "1. **DataLoader Output**:\n",
    "   - When you iterate over a DataLoader in PyTorch, it yields batches of data. Each batch is typically a tuple containing the inputs and the targets (labels) for a single batch.\n",
    "   - In this specific context, each batch contains pairs of sequences: `X` (the source sequence) and `y` (the target sequence).\n",
    "\n",
    "2. **Batch Structure**:\n",
    "   - In the `PianoDataset` class, the `__getitem__` method returns a tuple `(x, y)`, where `x` is a tensor representing the input sequence and `y` is a tensor representing the corresponding target sequence.\n",
    "   - The DataLoader combines these tuples into a single batch. Therefore, a batch is a tuple containing two tensors: one for all the input sequences in the batch and one for all the target sequences in the batch.\n",
    "\n",
    "3. **Unpacking the Batch**:\n",
    "   - The line `X, y = batch` is unpacking the batch into two separate variables: `X` for the input sequences and `y` for the target sequences.\n",
    "   - If `batch` is a tuple of two tensors, `X` and `y` will be assigned the values of these two tensors, respectively.\n",
    "\n",
    "4. **Shape of `X` and `y`**:\n",
    "   - After unpacking, `X` and `y` will both have the shape `(batch_size, chunk_size, 88)`, where:\n",
    "     - `batch_size` is the number of sequences in the batch (in this case, 16).\n",
    "     - `chunk_size` is the length of each sequence (in this case, 512).\n",
    "     - `88` is the number of features (in this case, representing the 88 keys of a piano).\n",
    "\n",
    "### Example\n",
    "\n",
    "Here’s a simplified example to illustrate the concept:\n",
    "\n",
    "```python\n",
    "# Assume each element in the dataset is a tuple (x, y)\n",
    "# Example data with 2 elements, where each element is a tuple of two sequences\n",
    "example_data = [\n",
    "    (torch.ones(512, 88), torch.zeros(512, 88)),\n",
    "    (torch.ones(512, 88) * 2, torch.zeros(512, 88) * 2)\n",
    "]\n",
    "\n",
    "# Create a DataLoader\n",
    "example_loader = DataLoader(example_data, batch_size=2)\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for batch in example_loader:\n",
    "    X, y = batch  # Unpack the batch into X (inputs) and y (targets)\n",
    "    print(\"X:\", X)\n",
    "    print(\"y:\", y)\n",
    "```\n",
    "\n",
    "In this example, `batch` would be a tuple containing two tensors, each of shape `(2, 512, 88)` (since `batch_size=2`). The line `X, y = batch` unpacks these tensors into `X` and `y` for further processing.\n",
    "\n",
    "By using this approach, you can easily access the input and target sequences separately within each batch, facilitating their use in training or evaluation loops."
   ],
   "id": "2d5d71e861f9227a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Modell definieren",
   "id": "87f7038bbc5e3968"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.1 Positional Encoding",
   "id": "6a0e9371d631bd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:28:36.949435Z",
     "start_time": "2024-06-17T17:28:36.945962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Encoding - From formula -> This is basically applying the formula for Positional encoding (The one with Sinus and Cosinus)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # Baically a positions list 0, 1, 2, 3, 4, 5, ...\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        #  # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "         # Saving buffer (same as parameter without gradients needed)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Residual connection + pos encoding\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ],
   "id": "5baa47ca2c6fc5fe",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2 Transformer definieren",
   "id": "257b2f3b1559ade8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TODO: Am schluss wahrscheinlichkeiten für jedes feature in einem Token noch ausgeben",
   "id": "ab19ba674d7481d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:30:33.244086Z",
     "start_time": "2024-06-17T17:30:33.227939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Hier werden glaube ich die Layer definiert. Ist Im guide glaube ich in anderer Reihenfolge -> hab sie jetzt in die gleiche Reihenfolge wie im guide gepackt\n",
    "        \n",
    "        ## Layers des Gesamten Modells\n",
    "        \n",
    "        # Positional Encoding zur Hinzufügung von Positionsinformationen zu den Token-Einbettungen\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        # Token-Einbettung für Quell- und Zielvokabular\n",
    "        # I use a nn.Embedding instead of the self defined TokenEmbedding\n",
    "        # brauche ich evtl nicht, da die Daten ja schon quasi \"embedded\" sind\n",
    "        self.src_tok_emb = nn.Linear(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = nn.Linear(tgt_vocab_size, emb_size)\n",
    "        \n",
    "        # Initialisierung des nn.Transformer Moduls mit den gegebenen Hyperparametern\n",
    "        self.transformer = nn.Transformer(d_model=emb_size,\n",
    "                                          nhead=nhead,\n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers,\n",
    "                                          dim_feedforward=dim_feedforward,\n",
    "                                          dropout=dropout,\n",
    "                                          batch_first=True)\n",
    "        \n",
    "        # Linearer Layer zur Projektion der Ausgabedimensionen auf die Zielvokabulargröße\n",
    "        # Generator ist also glaube ich die Outputlayer, die die Ausgabe in die Wahrscheinlichkeiten für die einzelnen Tokens übersetzt\n",
    "        \n",
    "        # hier Wichtig, bin mir nicht sicher, was hier gelten soll: \n",
    "        # ich glaube 88 ist richtig. muss mir aber noch anschauen, was genau hier passiert\n",
    "        self.generator = nn.Linear(emb_size, 12)  # Output should match the number of piano keys \n",
    "        # self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                # src_mask: Tensor,\n",
    "                tgt_mask=None,\n",
    "                src_padding_mask=None,\n",
    "                tgt_padding_mask=None):\n",
    "        \n",
    "        \n",
    "        # Einbettung und Positional Encoding für die Quellsequenz\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        #src_emb = self.positional_encoding(src)\n",
    "        \n",
    "        # Einbettung und Positional Encoding für die Zielsequenz\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        #tgt_emb = self.positional_encoding(trg)\n",
    "        \n",
    "        # Hier bin ich noch etwas verwirrt, warum die dimensionen Permutiert werden müssen\n",
    "        # Aus der Erklärung für die batch_first variable von nn.Transformer:\n",
    "        # If True, then the input and output tensors are provided as (batch, seq, feature). Default: False (seq, batch, feature).\n",
    "        \n",
    "        # (deprecated) src_emb = src_emb.permute(1,0,2)\n",
    "        # (deprecated) tgt_emb = tgt_emb.permute(1,0,2)\n",
    "        #print(\"src_emb shape:\", src_emb.shape)\n",
    "        #print(\"tgt_emb shape:\", tgt_emb.shape)\n",
    "        \n",
    "        # Durchführen der Transformationsoperation\n",
    "        # src_emb und tgt_emb sind die eingebetteten Sequenzen mit Positionsinformationen\n",
    "        # src_mask und tgt_mask sind die Masken, die verhindern, dass zukünftige Tokens betrachtet werden\n",
    "        # src_padding_mask, tgt_padding_mask und memory_key_padding_mask sind die Masken für Padding-Tokens\n",
    "        outs = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask, src_key_padding_mask=src_padding_mask, tgt_key_padding_mask=tgt_padding_mask)\n",
    "        \n",
    "        # Projektion der Ausgabe auf die Zielvokabulargröße\n",
    "        return self.generator(outs)"
   ],
   "id": "48abfa2135adc5e8",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.3 Masken definieren",
   "id": "f7d6bcbbbae5a237"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:28:36.973789Z",
     "start_time": "2024-06-17T17:28:36.965434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    \n",
    "    # EX for size=5:\n",
    "    # [[0., -inf, -inf, -inf, -inf],\n",
    "    #  [0.,   0., -inf, -inf, -inf],\n",
    "    #  [0.,   0.,   0., -inf, -inf],\n",
    "    #  [0.,   0.,   0.,   0., -inf],\n",
    "    #  [0.,   0.,   0.,   0.,   0.]]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Die Funktion create_mask erstellt sowohl Quell- als auch Ziel-Pad-Masken, indem sie prüft, ob Elemente in der Quell- und Zielsequenz gleich dem Pad-Token sind. Diese Masken werden transponiert, um die richtige Dimension zu erhalten.\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ],
   "id": "8f4adc50ab23f66d",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ?.? Embeddings erstellen\n",
    "\n",
    "nn.Embedding modul von pytorch wird evtl nicht benötigt.\n",
    "\n",
    "Aus chat GPT:\n",
    "\n",
    "- Die Eingabesequenzen (src) und die Zielsequenzen (tgt) haben die Form (batch_size, seq_len, feature_dim), was in Ihrem Fall (batch_size, 100, 88) ist.\n",
    "- Das positional embedding wird trotzdem benötigt\n",
    "- Da die Eingabedaten bereits in einer geeigneten Form vorliegen (dichte Vektoren), benötigen Sie kein nn.Embedding Modul. Sie können direkt die Sequenzen und Positional Encodings verwenden."
   ],
   "id": "f31a783e48c2cc12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Training/Validation definieren",
   "id": "3203525cd895548f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.1 Hyperparameter festlegen",
   "id": "c7dc325becfbcf39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:28:36.982340Z",
     "start_time": "2024-06-17T17:28:36.974298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SRC_VOCAB_SIZE = 12 # Ist glaube ich das num_tokens aus dem Guide (Also wie viele Verschiedene Tokens es insgesamt gibt\n",
    "TGT_VOCAB_SIZE = 12 # Auch 4, da die eingabe und zielsequenz die Gleichen möglichkeiten für Tokens haben\n",
    "EMB_SIZE = 32 #die Dimesnion des Modells Die anzahl der Erwarteten features der inputs/outputs also quasi die anzahl der Wörter in einer sequenz glaube ich -> also 8 bei uns (hier werden die spezial-tokens nicht gezählt ?)\n",
    "NHEAD = 4 # Anzahl der heads in einem Attention block\n",
    "FFN_HID_DIM = 512 # Anzahl der hidden layers des Feed-forward networks \n",
    "BATCH_SIZE = 32 # wird nicht ans Modell weitergegeben. evtl für uns nicht wichtig, weil wir die Daten schon gebatcht haben?\n",
    "NUM_ENCODER_LAYERS = 4 # wie viele Encoder blöcke\n",
    "NUM_DECODER_LAYERS = 4 # wie viele Decoder Blöcke"
   ],
   "id": "481044c71e2911c3",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.2 Modell initialisieren",
   "id": "c4681d35daa933b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:28:37.021098Z",
     "start_time": "2024-06-17T17:28:36.983225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modell initialisiern\n",
    "transformer = Transformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "# auf GPU laden\n",
    "transformer = transformer.to(DEVICE)\n",
    "# Kostenfunktion als CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Optimizer als Adam optimizer festlegen\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ],
   "id": "b8024c8f1cefe77f",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.3 Trainingsloop definieren",
   "id": "8bea243e08455c4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:28:37.024179Z",
     "start_time": "2024-06-17T17:28:37.021730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        X, y = batch\n",
    "        #print(X.shape, y.shape)\n",
    "        # X and y should both have shape (batch_size, chunk_size + 2, 88) because of SOS and EOS tokens\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1]\n",
    "        y_expected = y[:,1:]\n",
    "        \n",
    "        #print(\"Training: X shape:\", X.shape)\n",
    "        #print(\"Training: y_input shape:\", y_input.shape)\n",
    "        #print(\"Training: y_expected shape:\", y_expected.shape)\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = y_input.size(1)\n",
    "        tgt_mask = generate_square_subsequent_mask(sequence_length).to(DEVICE)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        logits = model(X, y_input, tgt_mask)\n",
    "        \n",
    "        #print(\"Training: prediction (model output) shape:\", logits.shape)\n",
    "        \n",
    "        #(deprecated) Permute pred to have batch size first again\n",
    "        #(deprecated) pred = pred.permute(1, 2, 0)\n",
    "        # logits ist die Ausgabe des modells, y_expected ist die erwartete ausgabe\n",
    "        # Die dimensionen müssen verändert werden, da die loss funktion die Tensoren in anderer Form erwartet\n",
    "        \n",
    "        \n",
    "        # Reshape logits and y_expected for loss calculation\n",
    "        #logits = logits.reshape(-1, logits.shape[-1])\n",
    "        #y_expected = y_expected.reshape(-1, y_expected.shape[-1])\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(logits, y_expected)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "374220b7c4917510",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.4 Validation loop definieren",
   "id": "1cb16d91be65f248"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:28:37.032971Z",
     "start_time": "2024-06-17T17:28:37.024882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            #print(X.shape, y.shape)\n",
    "            # X and y should both have shape (batch_size, chunk_size + 2, 88) because of SOS and EOS tokens\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = y[:,:-1]\n",
    "            y_expected = y[:,1:]\n",
    "            \n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = y_input.size(1)\n",
    "            tgt_mask = generate_square_subsequent_mask(sequence_length).to(DEVICE)\n",
    "\n",
    "            # Standard training except we pass in y_input and src_mask\n",
    "            logits = model(X, y_input, tgt_mask)\n",
    "\n",
    "            # Permute pred to have batch size first again\n",
    "            #pred = pred.permute(1, 2, 0)\n",
    "            # logits ist die Ausgabe des modells, y_expected ist die erwartete ausgabe\n",
    "            \n",
    "            # Reshape logits and y_expected for loss calculation\n",
    "            #logits = logits.reshape(-1, logits.shape[-1])\n",
    "            #y_expected = y_expected.reshape(-1, y_expected.shape[-1])\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(logits, y_expected)\n",
    "    \n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(logits, y_expected)\n",
    "            total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "e2644c1791731b63",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Modell Trainieren",
   "id": "32d3631ed0fb71e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:30:42.829520Z",
     "start_time": "2024-06-17T17:30:42.742983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_loop(transformer, optimizer, loss_fn, train_loader)\n",
    "    end_time = timer()\n",
    "    val_loss = validation_loop(transformer, loss_fn, val_loader)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ],
   "id": "e7fe3fe6e90de130",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Inferenz",
   "id": "a4ab2907222264af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T15:08:02.978525Z",
     "start_time": "2024-06-17T15:08:02.970005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(model, input_sequence, max_length, start_token, end_token, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input sequence (add batch dimension if necessary and move to device)\n",
    "    if input_sequence.dim() == 2:  # if shape is [seq_len, num_features]\n",
    "        input_sequence = input_sequence.unsqueeze(0)  # add batch dimension\n",
    "    \n",
    "    input_sequence = input_sequence.to(device)  # move to device\n",
    "    #print(\"Input Melody:\", input_sequence.shape)\n",
    "    #print(\"=====================\")\n",
    "    \n",
    "    # Prepare start token for target sequence\n",
    "    y_input = torch.tensor(start_token, dtype=torch.float, device=device).unsqueeze(0)  # shape: [1, 1, 12]\n",
    "    #print(\"y_input:\", y_input.shape)\n",
    "    #print(\"=================\")\n",
    "    \n",
    "    prediction_tensor_list =[]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Get target mask\n",
    "        tgt_mask = generate_square_subsequent_mask(y_input.size(1)).to(device)\n",
    "        \n",
    "        # Perform prediction\n",
    "        pred = model(input_sequence, y_input, tgt_mask)\n",
    "        \n",
    "        prediction_tensor_list.append(pred)\n",
    "        \n",
    "        # Apply a threshold to convert logits to binary values\n",
    "        next_item = (pred[:, -1, :] > 0).float().unsqueeze(1)  # Shape: [1, 1, num_features]\n",
    "        \n",
    "        # Concatenate previous input with predicted token\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "        \n",
    "        # Stop if model predicts end of sequence\n",
    "        if torch.equal(next_item.squeeze(), torch.tensor(end_token, dtype=torch.float, device=device).squeeze()):\n",
    "            #print(\"end token found\")\n",
    "            break\n",
    "    \n",
    "    return y_input.squeeze(0).tolist(), prediction_tensor_list  # Remove batch dimension"
   ],
   "id": "26bc4ea14903c90a",
   "execution_count": 53,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T15:13:26.922768Z",
     "start_time": "2024-06-17T15:13:26.428622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch in test_loader:\n",
    "    X, y = batch\n",
    "    X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "    \n",
    "    melody = X[0]\n",
    "    real_harmony = y[0]\n",
    "    \n",
    "    \n",
    "    prediction, prediction_tensor_list = predict(transformer, melody, 100, SOS_TOKEN, EOS_TOKEN, DEVICE)\n",
    "    \n",
    "    #print(\"Prediction:\", prediction)\n",
    "    #print(\"=====================\")\n",
    "    #print(\"real harmony:\", real_harmony)\n",
    "    \n",
    "    #prediction_tensor_list = [item for sublist in prediction_tensor_list for item in sublist]\n",
    "    max_values = []\n",
    "    min_values = []\n",
    "    \n",
    "    \n",
    "    # print(\"prediciton tensor list:\", prediction_tensor_list)\n",
    "    for tensor in prediction_tensor_list:\n",
    "        max_values.append(torch.max(tensor))\n",
    "        min_values.append(torch.min(tensor))\n",
    "    \n",
    "    print(\"max value of all tensors:\", max(max_values))\n",
    "    print(\"min value of all tensors:\", min(min_values))\n",
    "    \n",
    "    break"
   ],
   "id": "ed1ef0866a4b2012",
   "execution_count": 57,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Output passt nicht wirklich. ich muss mir nohcmal anschauen, wie ich das modell am besten trainieren kann und wie man den Output am besten nochmal macht.",
   "id": "7f7180919d2e7ac2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T15:17:16.285872Z",
     "start_time": "2024-06-17T15:17:16.278487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def printHeatmap(predicted_harmony, center=0.2, vmin=0, vmax=0.4):\n",
    "    # Create the 'pictures' directory if it doesn't exist\n",
    "    if not os.path.exists('pictures'):\n",
    "        os.makedirs('pictures')\n",
    "\n",
    "    # Determine the next prefix number for saving files\n",
    "    existing_files = [f for f in os.listdir('pictures') if f.endswith('.png')]\n",
    "    if existing_files:\n",
    "        latest_file = max(existing_files)\n",
    "        latest_prefix = int(latest_file.split('_')[0])\n",
    "        prefix = f\"{latest_prefix + 1:02d}_\"\n",
    "    else:\n",
    "        prefix = \"00_\"\n",
    "\n",
    "    # Create and save a heatmap of Predicted Harmony Data\n",
    "    plt.figure(figsize=(20, 10))  # Adjust the size as necessary\n",
    "    sns.heatmap(predicted_harmony, cmap='coolwarm', center=center, vmin=vmin, vmax=vmax)  # Adjust color map and limits based on your data\n",
    "    plt.title('Heatmap of Predicted Harmony Data')\n",
    "    plt.xlabel('Keys on piano')\n",
    "    plt.ylabel('Probability of pressing (One-Hot-Encoding)')\n",
    "    plt.savefig(os.path.join('pictures', f'{prefix}heatmap_predicted_harmony.png'))\n",
    "    plt.show()"
   ],
   "id": "ba06c1cc041145ba",
   "execution_count": 59,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T15:19:00.789401Z",
     "start_time": "2024-06-17T15:19:00.313391Z"
    }
   },
   "cell_type": "code",
   "source": "printHeatmap(prediction, 0, -0.7, 1.1)",
   "id": "ec6441e3cd44266a",
   "execution_count": 62,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
