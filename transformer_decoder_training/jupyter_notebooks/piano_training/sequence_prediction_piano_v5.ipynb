{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Jetzt nochmal ein Decoder only ohne tokenization zu int\n",
    "\n",
    "TODO:\n",
    "train test split nochmal überarbeiten, da hier ja nur trainiert wird. Daten müssen extern getrennt werden, wenn woanders getestet wird -> Test soll auf dem Modell unbekannten Daten passieren\n"
   ],
   "id": "24c38489ff10921a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:22:29.012638Z",
     "start_time": "2024-06-26T17:22:28.274771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ],
   "id": "bcb65d87ee74abdb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# prepare data",
   "id": "23765b3ac55ffd27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:22:29.049865Z",
     "start_time": "2024-06-26T17:22:29.013378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "7fa52c1cc923a11e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:22:29.213259Z",
     "start_time": "2024-06-26T17:22:29.052270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set Special tokens, chunk size, etc.\n",
    "sos_token = np.full((1, 88), 1)\n",
    "pad_token = np.full((1, 88), 2)\n",
    "pad_token = torch.tensor(pad_token, device=device)\n",
    "\n",
    "seq_length = 512\n",
    "batch_size = 64"
   ],
   "id": "4c76896becf6ab2e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:22:49.117904Z",
     "start_time": "2024-06-26T17:22:29.215311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data_preperation import dataset_snapshot\n",
    "from transformer_decoder_training.dataprep_transformer import dataprep_1\n",
    "from sklearn.model_selection import train_test_split\n",
    "# prepare data for dataset\n",
    "\n",
    "#load data\n",
    "dataset_as_snapshots = dataset_snapshot.process_dataset_multithreaded(\"/home/falaxdb/Repos/minus1/datasets/maestro_v3_split/hands_split_into_seperate_midis\", 0.05)\n",
    "# filter snapshots to 88 piano notes\n",
    "dataset_as_snapshots = dataset_snapshot.filter_piano_range(dataset_as_snapshots)\n",
    "\n",
    "# Convert data into Chunks and add special tokens\n",
    "data = dataprep_1.prepare_dataset(dataset_as_snapshots, seq_length, seq_length, sos_token)\n",
    "\n",
    "print(\"Ammount of sequence pairs:\", len(data))\n",
    "\n",
    "# Split the dataset using sklearn while maintaining pairs\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42, shuffle=True)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, shuffle=True)"
   ],
   "id": "20e83bfc1b5fcdb1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed dataset (1038/1038): 100%|██████████| 1038/1038 [00:14<00:00, 72.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1038 of 1038 files\n",
      "Ammount of sequence pairs: 10069\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:22:49.603104Z",
     "start_time": "2024-06-26T17:22:49.118401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformer_decoder_training.dataset_transformer.dataset_1 import PianoDataset\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = PianoDataset(train_data)\n",
    "val_dataset = PianoDataset(val_data)\n",
    "test_dataset = PianoDataset(test_data)\n",
    "\n",
    "# Create DataLoaders for each subset with drop_last=True\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# Iterate over the DataLoader (example with train_loader)\n",
    "for batch in train_loader:\n",
    "    X, y = batch\n",
    "    print(X.shape, y.shape)\n",
    "    # X and y should both have shape (batch_size, chunk_size + 1, feature dimension) because of SOS (and EOS) tokens"
   ],
   "id": "5facbd248fd5eb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n",
      "torch.Size([64, 513, 88]) torch.Size([64, 513, 88])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Initialize model",
   "id": "e92d52061114bca6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:22:49.605415Z",
     "start_time": "2024-06-26T17:22:49.603721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set parameters\n",
    "\n",
    "# Learning rate for the optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Number of epochs for training\n",
    "nepochs = 20\n",
    "\n",
    "# Embedding Size\n",
    "hidden_size = 256\n",
    "\n",
    "# Number of transformer blocks\n",
    "num_layers = 8\n",
    "\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 8"
   ],
   "id": "bfbeb949c8aaa4d2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:22:49.902600Z",
     "start_time": "2024-06-26T17:22:49.605891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformer_decoder_training.models.transformer_decoder_1 import Transformer\n",
    "\n",
    "# Create model\n",
    "# (num emb = wie viele verschiedene Tokens es geben kann bei 12 Tönen 2 ** 12 Möglichkeiten + 2 special tokens)\n",
    "# num_emb: Da ja keine int indexe mehr -> wahrscheinlich 88 wegen 88 Keys\n",
    "tf_generator = Transformer(num_emb=88, num_layers=num_layers, \n",
    "                           hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(tf_generator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "# Klammern nicht vergessen\n",
    "loss_fn = nn.BCELoss()"
   ],
   "id": "b4645664ec62fb7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:22:49.905547Z",
     "start_time": "2024-06-26T17:22:49.903137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check number of model parameters\n",
    "num_model_params = 0\n",
    "for param in tf_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ],
   "id": "d8b97651371e006a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-This Model Has 6363480 (Approximately 6 Million) Parameters!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "1891247df0d89ca1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:22:49.914138Z",
     "start_time": "2024-06-26T17:22:49.905928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader, pad_token, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Move data to GPU\n",
    "        input_melody, expected_harmony = batch\n",
    "        input_melody, expected_harmony = input_melody.to(device), expected_harmony.to(device)\n",
    "        \n",
    "        # shift input und output für das training zeug mit start token\n",
    "        input_melody = input_melody[:, 0:-1]\n",
    "        expected_harmony = expected_harmony[:, 1:]\n",
    "        \n",
    "        # Generate predictions\n",
    "        pred = tf_generator(input_melody, pad_token)\n",
    "        \n",
    "        #print(\"Prediction shape:\", pred.shape)\n",
    "        #print(pred)\n",
    "        #print(\"expected harmony_shape:\", expected_harmony.shape)\n",
    "        #print(expected_harmony)\n",
    "        \n",
    "        # Calculate loss with masked cross-entropy\n",
    "        # ich glaube 0 steht in vorlage für padding token index -> habe ich hier anders\n",
    "        #mask = (expected_harmony != pad_token).float() Maske verwenden, um Padding positions im output zu canceln\n",
    "        # masked_pred = pred * mask\n",
    "        loss = loss_fn(pred, expected_harmony)\n",
    "        \n",
    "        # Backpropagation\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "1f09bcbc20cb6ee4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:26:40.561463Z",
     "start_time": "2024-06-26T17:22:49.914846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_loop(tf_generator, optimizer, loss_fn, train_loader, pad_token, device)\n",
    "    end_time = timer()\n",
    "    # val_loss = validation_loop(model, loss_fn, val_loader)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
   ],
   "id": "9fc6d5036b353801",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 0.074 Epoch time = 21.647s\n",
      "Epoch: 2, Train loss: 0.049 Epoch time = 22.403s\n",
      "Epoch: 3, Train loss: 0.049 Epoch time = 23.284s\n",
      "Epoch: 4, Train loss: 0.049 Epoch time = 23.285s\n",
      "Epoch: 5, Train loss: 0.049 Epoch time = 23.294s\n",
      "Epoch: 6, Train loss: 0.048 Epoch time = 23.298s\n",
      "Epoch: 7, Train loss: 0.048 Epoch time = 23.295s\n",
      "Epoch: 8, Train loss: 0.046 Epoch time = 23.294s\n",
      "Epoch: 9, Train loss: 0.046 Epoch time = 23.530s\n",
      "Epoch: 10, Train loss: 0.045 Epoch time = 23.307s\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:26:40.581075Z",
     "start_time": "2024-06-26T17:26:40.562357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# see: https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html#save-and-load-the-model\n",
    "\n",
    "torch.save(tf_generator.state_dict(), \"./saved_models/model_1_notebook_v5.pth\")"
   ],
   "id": "364f0e3645599da",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "5c40661c3664a326"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:26:40.588895Z",
     "start_time": "2024-06-26T17:26:40.581562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = next(iter(test_loader))\n",
    "\n",
    "print(f\"melody: {X.shape}, Harmony: {y.shape}\")\n",
    "\n",
    "# get single sequence\n",
    "X, y = X[1], y[1]\n",
    "\n",
    "\n",
    "# Input should be of the shape: (batch_size, seq_length, feature dimension)\n",
    "X, y = torch.unsqueeze(X, dim=0), torch.unsqueeze(y, dim=0)\n",
    "print(f\"melody: {X.shape}, Harmony: {y.shape}\")"
   ],
   "id": "4a61b8e967d00d40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melody: torch.Size([64, 513, 88]), Harmony: torch.Size([64, 513, 88])\n",
      "melody: torch.Size([1, 513, 88]), Harmony: torch.Size([1, 513, 88])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T17:26:40.772418Z",
     "start_time": "2024-06-26T17:26:40.589396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformer_decoder_training.inference.inference_1 import inference\n",
    "\n",
    "output_tokens = inference(tf_generator, X, 512, 0.1, pad_token, device)"
   ],
   "id": "508e8d7bf482e310",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformer_decoder_training\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minference\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minference_1\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m inference\n\u001B[0;32m----> 3\u001B[0m output_tokens \u001B[38;5;241m=\u001B[39m \u001B[43minference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtf_generator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpad_token\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Repos/minus1/transformer_decoder_training/inference/inference_1.py:10\u001B[0m, in \u001B[0;36minference\u001B[0;34m(model, sequence, num_of_tokens_to_generate, threshold, pad_token, device)\u001B[0m\n\u001B[1;32m      7\u001B[0m sequence\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# model prediction for new token\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m data_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpad_token\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data_pred\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m>\u001B[39m (sequence\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel might have generated more than one token\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Repos/minus1/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Repos/minus1/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Repos/minus1/transformer_decoder_training/models/transformer_decoder_1.py:105\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, input_seq, pad_token)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_seq, pad_token):\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;66;03m# Mask for padding tokens\u001B[39;00m\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;66;03m# input_key_mask = input_seq == 0\u001B[39;00m\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;66;03m# input key mask should be (batch_size, seq_Length)\u001B[39;00m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;66;03m# .all(dim=-1) to compare the token in every dimension\u001B[39;00m\n\u001B[0;32m--> 105\u001B[0m     input_key_mask \u001B[38;5;241m=\u001B[39m (\u001B[43minput_seq\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mpad_token\u001B[49m)\u001B[38;5;241m.\u001B[39mall(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;66;03m# i think i need to set padding token here for mask\u001B[39;00m\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;66;03m#print(input_key_mask.shape)\u001B[39;00m\n\u001B[1;32m    107\u001B[0m \n\u001B[1;32m    108\u001B[0m     \u001B[38;5;66;03m# Embedding input sequence\u001B[39;00m\n\u001B[1;32m    109\u001B[0m     input_embs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(input_seq)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
