{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "decoder only transformer with \n",
    "\n",
    "- 24 keys\n",
    "- loss: CrossEntropy"
   ],
   "id": "51aa462e42955fb5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-25T15:46:30.681894Z",
     "start_time": "2024-07-25T15:46:29.244074Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T15:46:32.375172Z",
     "start_time": "2024-07-25T15:46:32.346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "538a853ec338c53d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T15:46:34.043279Z",
     "start_time": "2024-07-25T15:46:33.874977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sos_token = np.full((1, 24), 1)\n",
    "pad_token = np.full((1, 24), 2)\n",
    "pad_token = torch.tensor(pad_token, device=device)\n",
    "\n",
    "dataset_dir = \"/home/falaxdb/Repos/minus1/datasets/maestro_v3_split/hands_split_into_seperate_midis\"\n",
    "snapshot_intervall = 0.05\n",
    "\n",
    "# Define other parameters\n",
    "batch_size = 64\n",
    "seq_length = 512\n",
    "stride = 256\n",
    "\n",
    "test_size=0.3"
   ],
   "id": "e04b779ecc0a58e7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T15:47:32.160805Z",
     "start_time": "2024-07-25T15:46:36.933670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformer_decoder_training.dataprep_transformer.prepare_dataloader_complete import prepare_dataset_as_dataloaders\n",
    "\n",
    "# Load Data\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_dataset_as_dataloaders(dataset_dir, snapshot_intervall, batch_size, seq_length, stride, test_size, sos_token)"
   ],
   "id": "7582d12d1bad5ef6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed dataset (1038/1038): 100%|██████████| 1038/1038 [00:15<00:00, 67.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1038 of 1038 files\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T15:47:52.640962Z",
     "start_time": "2024-07-25T15:47:52.632380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set parameters\n",
    "# Learning rate for the optimizer\n",
    "learning_rate = 1e-3\n",
    "# Number of epochs for training\n",
    "num_epochs = 20\n",
    "# basically input dimension before embedding\n",
    "num_emb = 24\n",
    "# size after embedding for feed forward neural network\n",
    "hidden_size = 256\n",
    "# Number of transformer blocks\n",
    "num_layers = 8\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 8"
   ],
   "id": "62cf13f41adc8098",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T15:47:57.531227Z",
     "start_time": "2024-07-25T15:47:56.043112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformer_decoder_training.models.transformer_decoder_2 import Transformer\n",
    "\n",
    "model = Transformer(num_emb=num_emb, num_layers=num_layers, hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "# loss function should be one that can handle multi one hot encoded vectors\n",
    "# Klammern nicht vergessen\n",
    "# Chat gpt says BCEWithLogitsLoss is more stable\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "id": "a106af948ed4577b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare directory for saving model state dict, parameters, loss, etc.\n",
    "model_state_dict_filepath = \"/home/falaxdb/Repos/minus1/transformer_decoder_training/saved_files/saved_models/model_1_notebook_v6.2\""
   ],
   "id": "d710fbee997d980a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T16:04:52.684142Z",
     "start_time": "2024-07-25T15:48:04.023693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "from transformer_decoder_training.training import training_1\n",
    "from data_visualization.Visualization import plot_losses\n",
    "\n",
    "# Initialize lists to store loss values\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    start_time = timer()\n",
    "    train_loss = training_1.train_loop(model, optimizer, loss_fn, train_loader, pad_token, device)\n",
    "    end_time = timer()\n",
    "    val_loss = training_1.validation_loop(model, loss_fn, val_loader, pad_token, device)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "    plot_losses(train_losses, val_losses, model_state_dict_filepath + \"loss_plot.png\")"
   ],
   "id": "7633cba1a227de58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 206.190, Val loss: 200.803, Epoch time = 48.315s\n",
      "Epoch: 2, Train loss: 203.712, Val loss: 199.703, Epoch time = 48.034s\n",
      "Epoch: 3, Train loss: 202.751, Val loss: 198.824, Epoch time = 48.008s\n",
      "Epoch: 4, Train loss: 201.969, Val loss: 197.926, Epoch time = 48.029s\n",
      "Epoch: 5, Train loss: 201.314, Val loss: 197.254, Epoch time = 48.019s\n",
      "Epoch: 6, Train loss: 200.749, Val loss: 196.726, Epoch time = 48.011s\n",
      "Epoch: 7, Train loss: 200.120, Val loss: 195.946, Epoch time = 47.987s\n",
      "Epoch: 8, Train loss: 199.437, Val loss: 195.258, Epoch time = 47.992s\n",
      "Epoch: 9, Train loss: 198.721, Val loss: 194.464, Epoch time = 48.002s\n",
      "Epoch: 10, Train loss: 197.996, Val loss: 193.661, Epoch time = 47.799s\n",
      "Epoch: 11, Train loss: 197.342, Val loss: 193.035, Epoch time = 47.754s\n",
      "Epoch: 12, Train loss: 196.802, Val loss: 192.582, Epoch time = 47.743s\n",
      "Epoch: 13, Train loss: 196.357, Val loss: 192.153, Epoch time = 47.493s\n",
      "Epoch: 14, Train loss: 195.930, Val loss: 191.870, Epoch time = 46.239s\n",
      "Epoch: 15, Train loss: 195.591, Val loss: 191.608, Epoch time = 46.308s\n",
      "Epoch: 16, Train loss: 195.277, Val loss: 191.355, Epoch time = 47.412s\n",
      "Epoch: 17, Train loss: 195.010, Val loss: 191.161, Epoch time = 47.978s\n",
      "Epoch: 18, Train loss: 194.739, Val loss: 191.044, Epoch time = 47.975s\n",
      "Epoch: 19, Train loss: 194.502, Val loss: 190.814, Epoch time = 47.925s\n",
      "Epoch: 20, Train loss: 194.261, Val loss: 190.778, Epoch time = 47.977s\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T16:05:38.325372Z",
     "start_time": "2024-07-25T16:05:38.302471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "torch.save(model.state_dict(), model_state_dict_filepath + \".pth\")\n",
    "# save the parameters\n",
    "model_params = {\n",
    "    \"num_emb\": num_emb,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"training_params\": {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"loss_fn\": loss_fn.__class__.__name__\n",
    "    },\n",
    "    \"training_data_params\": {\n",
    "        \"sos_token\": sos_token,\n",
    "        \"pad_token\": pad_token,\n",
    "        \"snapshot_intervall\": snapshot_intervall,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"sequence_length\": seq_length,\n",
    "        \"stride\": stride\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the model parameters separately as a JSON file\n",
    "params_path = model_state_dict_filepath + \"_model_params.json\"\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(model_params, f)"
   ],
   "id": "c58572e951397b83",
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
