{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# now with new dataset preparation\n",
    "\n",
    "I now want to do correct sequence prediction and not just Right hand to left hand\n",
    "\n",
    "## for training\n",
    "\n",
    "I just use one Sequence with sos token as input and the same sequence shifted 1 to the right as prediction target.\n",
    "\n",
    "For this i use the new dataset"
   ],
   "id": "411f36fa549f937d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T19:37:35.219262Z",
     "start_time": "2024-07-03T19:37:34.049911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ],
   "id": "c46b14c4f7c68393",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T19:37:35.251153Z",
     "start_time": "2024-07-03T19:37:35.219980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "a7798988d4673283",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Data",
   "id": "42c602882b26f0bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T19:38:28.230156Z",
     "start_time": "2024-07-03T19:37:35.252020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data_preperation import dataset_snapshot\n",
    "from transformer_decoder_training.dataprep_transformer import dataprep_1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load data\n",
    "dataset_as_snapshots = dataset_snapshot.process_dataset_multithreaded(\"/home/falaxdb/Repos/minus1/datasets/maestro_v3_split/hands_split_into_seperate_midis\", 0.05)\n",
    "# filter snapshots to 88 piano notes\n",
    "dataset_as_snapshots = dataset_snapshot.filter_piano_range(dataset_as_snapshots)\n",
    "\n",
    "dataset_as_snapshots = dataset_snapshot.compress_existing_dataset_to_12keys(dataset_as_snapshots)"
   ],
   "id": "a08e73d4cf433d23",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed dataset (1038/1038): 100%|██████████| 1038/1038 [00:14<00:00, 72.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1038 of 1038 files\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T19:38:28.234993Z",
     "start_time": "2024-07-03T19:38:28.231240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split songs into train, test and val\n",
    "train_data, temp_data = train_test_split(dataset_as_snapshots, test_size=0.3, random_state=42, shuffle=True)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# see if split is correct\n",
    "print(\"Train data:\", len(train_data))\n",
    "print(\"test data:\", len(test_data))\n",
    "print(\"val data:\", len(val_data))\n",
    "\n",
    "for song in train_data:\n",
    "    for track in song:\n",
    "        print(track.shape)\n",
    "    break"
   ],
   "id": "d45c18af7cf1fdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 363\n",
      "test data: 78\n",
      "val data: 78\n",
      "(6088, 12)\n",
      "(6088, 12)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Dataset",
   "id": "bdb5da987313b461"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T19:38:28.571687Z",
     "start_time": "2024-07-03T19:38:28.235398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define special Tokens\n",
    "# Token dimension needs to fit Data\n",
    "sos_token = np.full((1, 24), 1)\n",
    "pad_token = np.full((1, 24), 2)\n",
    "pad_token = torch.tensor(pad_token, device=device)\n",
    "\n",
    "# Define other parameters\n",
    "batch_size = 64\n",
    "seq_length = 512\n",
    "stride = 256"
   ],
   "id": "4590f1961ea7b2e7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T19:38:29.278925Z",
     "start_time": "2024-07-03T19:38:28.573490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create dataset + dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_decoder_training.dataset_transformer.dataset_2 import AdvancedPianoDataset\n",
    "\n",
    "train_dataset = AdvancedPianoDataset(train_data, seq_length, stride, sos_token)\n",
    "val_dataset = AdvancedPianoDataset(val_data, seq_length, stride, sos_token)\n",
    "test_dataset = AdvancedPianoDataset(test_data, seq_length, stride, sos_token)\n",
    "\n",
    "print(\"Check length of datasets. should roughly match split ratio\")\n",
    "print(\"train dataset:\", len(train_dataset))\n",
    "print(\"val dataset:\", len(val_dataset))\n",
    "print(\"test dataset:\", len(test_dataset))\n",
    "print(\"\")\n",
    "\n",
    "# Create DataLoaders for each subset with drop_last=True\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# Test if data looks correct\n",
    "# sos token should be at beginning of every sequence\n",
    "# sequence should be 2 times the size of a track snapshot\n",
    "for batch in train_loader:\n",
    "    print(\"Visualize shape of batch:\")\n",
    "    print(\"shape of one batch:\", batch.shape)\n",
    "    print(\"==============\")\n",
    "    \n",
    "    print(\"Test for sos token as first token in sequence\")\n",
    "    print(\"First token in seq:\", batch[0][0])\n",
    "    print(\"=============\")\n",
    "    \n",
    "    print(\"Test print one snapshot:\")\n",
    "    print(\"First half of values should be left hand, second half should be right hand\")\n",
    "    print(batch[0][1])\n",
    "    break\n"
   ],
   "id": "367d0a2cbef1f12a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check length of datasets. should roughly match split ratio\n",
      "train dataset: 13799\n",
      "val dataset: 2848\n",
      "test dataset: 3209\n",
      "\n",
      "Visualize shape of batch:\n",
      "shape of one batch: torch.Size([64, 513, 24])\n",
      "==============\n",
      "Test for sos token as first token in sequence\n",
      "First token in seq: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1.])\n",
      "=============\n",
      "Test print one snapshot:\n",
      "First half of values should be left hand, second half should be right hand\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Initialize Model",
   "id": "5704fd4de9b558bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T19:38:29.281482Z",
     "start_time": "2024-07-03T19:38:29.279440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set parameters\n",
    "# Learning rate for the optimizer\n",
    "learning_rate = 1e-3\n",
    "# Number of epochs for training\n",
    "nepochs = 20\n",
    "# Embedding Size\n",
    "hidden_size = 256\n",
    "# Number of transformer blocks\n",
    "num_layers = 8\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 8"
   ],
   "id": "de9627712b370dd8",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T19:38:29.890601Z",
     "start_time": "2024-07-03T19:38:29.282320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformer_decoder_training.models.transformer_decoder_1 import Transformer\n",
    "\n",
    "model = Transformer(num_emb=24, num_layers=num_layers, hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "# loss function should be one that can handle multi one hot encoded vectors\n",
    "# Klammern nicht vergessen\n",
    "loss_fn = nn.BCELoss()"
   ],
   "id": "28b992c114f19f58",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T19:38:29.893471Z",
     "start_time": "2024-07-03T19:38:29.891018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check number of model parameters\n",
    "num_model_params = 0\n",
    "for param in model.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ],
   "id": "a61d396ea6a203e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-This Model Has 6330648 (Approximately 6 Million) Parameters!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "51c6bd13ca69ffcf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T19:38:29.901589Z",
     "start_time": "2024-07-03T19:38:29.894195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader, pad_token, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Move data to GPU\n",
    "        src_sequence = batch.to(device)\n",
    "        \n",
    "        # create input and expected sequence -> move expected sequence one to the right\n",
    "        input_sequences = src_sequence[:, :-1]\n",
    "        expected_sequence = src_sequence[:, 1:]\n",
    "        \n",
    "        # Generate predictions\n",
    "        pred = model(input_sequences, pad_token)\n",
    "        \n",
    "        #print(\"Prediction shape:\", pred.shape)\n",
    "        #print(pred)\n",
    "        #print(\"expected harmony_shape:\", expected_harmony.shape)\n",
    "        #print(expected_harmony)\n",
    "        \n",
    "        # Calculate loss with masked cross-entropy\n",
    "        # ich glaube 0 steht in vorlage für padding token index -> habe ich hier anders\n",
    "        #mask = (expected_harmony != pad_token).float() Maske verwenden, um Padding positions im output zu canceln\n",
    "        # masked_pred = pred * mask\n",
    "        loss = loss_fn(pred, expected_sequence)\n",
    "        \n",
    "        # Backpropagation\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validation_loop(model, loss_fn, dataloader,pad_token, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move data to GPU\n",
    "            src_sequence = batch.to(device)\n",
    "            \n",
    "            # Create input and expected sequences\n",
    "            input_sequences = src_sequence[:, :-1, :]\n",
    "            expected_sequence = src_sequence[:, 1:, :]\n",
    "            \n",
    "            # Generate predictions\n",
    "            pred = model(input_sequences, pad_token)\n",
    "            \n",
    "            # Calculate loss without flattening\n",
    "            loss = loss_fn(pred, expected_sequence)\n",
    "            \n",
    "            total_loss += loss.detach().item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "57b52fad83974ad2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T19:50:11.208829Z",
     "start_time": "2024-07-03T19:38:29.902024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_loop(model, optimizer, loss_fn, train_loader, pad_token, device)\n",
    "    end_time = timer()\n",
    "    val_loss = validation_loop(model, loss_fn, val_loader, pad_token, device)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ],
   "id": "c930c228bd38cd58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 0.180, Val loss: 0.129, Epoch time = 44.243s\n",
      "Epoch: 2, Train loss: 0.126, Val loss: 0.126, Epoch time = 43.879s\n",
      "Epoch: 3, Train loss: 0.123, Val loss: 0.122, Epoch time = 43.895s\n",
      "Epoch: 4, Train loss: 0.120, Val loss: 0.121, Epoch time = 43.887s\n",
      "Epoch: 5, Train loss: 0.118, Val loss: 0.119, Epoch time = 44.041s\n",
      "Epoch: 6, Train loss: 0.117, Val loss: 0.118, Epoch time = 43.881s\n",
      "Epoch: 7, Train loss: 0.115, Val loss: 0.116, Epoch time = 43.871s\n",
      "Epoch: 8, Train loss: 0.114, Val loss: 0.115, Epoch time = 43.873s\n",
      "Epoch: 9, Train loss: 0.112, Val loss: 0.113, Epoch time = 43.864s\n",
      "Epoch: 10, Train loss: 0.111, Val loss: 0.112, Epoch time = 43.849s\n",
      "Epoch: 11, Train loss: 0.110, Val loss: 0.111, Epoch time = 43.848s\n",
      "Epoch: 12, Train loss: 0.108, Val loss: 0.109, Epoch time = 43.855s\n",
      "Epoch: 13, Train loss: 0.107, Val loss: 0.109, Epoch time = 43.855s\n",
      "Epoch: 14, Train loss: 0.106, Val loss: 0.108, Epoch time = 43.860s\n",
      "Epoch: 15, Train loss: 0.105, Val loss: 0.107, Epoch time = 43.850s\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Last training output:\n",
    "Learning rate: 1e-4\n",
    "\n",
    "Epoch: 1, Train loss: 0.126, Val loss: 0.133, Epoch time = 44.974s  \n",
    "Epoch: 2, Train loss: 0.124, Val loss: 0.132, Epoch time = 44.998s  \n",
    "Epoch: 3, Train loss: 0.123, Val loss: 0.130, Epoch time = 44.866s  \n",
    "Epoch: 4, Train loss: 0.121, Val loss: 0.129, Epoch time = 44.857s  \n",
    "Epoch: 5, Train loss: 0.120, Val loss: 0.127, Epoch time = 44.856s  \n",
    "Epoch: 6, Train loss: 0.118, Val loss: 0.126, Epoch time = 44.866s  \n",
    "Epoch: 7, Train loss: 0.118, Val loss: 0.125, Epoch time = 44.853s  \n",
    "Epoch: 8, Train loss: 0.117, Val loss: 0.125, Epoch time = 44.867s  \n",
    "Epoch: 9, Train loss: 0.116, Val loss: 0.124, Epoch time = 44.860s  \n",
    "Epoch: 10, Train loss: 0.115, Val loss: 0.123, Epoch time = 44.859s \n",
    "\n",
    "learning rate: 1e-3\n",
    "\n",
    "Epoch: 1, Train loss: 0.180, Val loss: 0.129, Epoch time = 44.243s  \n",
    "Epoch: 2, Train loss: 0.126, Val loss: 0.126, Epoch time = 43.879s  \n",
    "Epoch: 3, Train loss: 0.123, Val loss: 0.122, Epoch time = 43.895s  \n",
    "Epoch: 4, Train loss: 0.120, Val loss: 0.121, Epoch time = 43.887s  \n",
    "Epoch: 5, Train loss: 0.118, Val loss: 0.119, Epoch time = 44.041s  \n",
    "Epoch: 6, Train loss: 0.117, Val loss: 0.118, Epoch time = 43.881s  \n",
    "Epoch: 7, Train loss: 0.115, Val loss: 0.116, Epoch time = 43.871s  \n",
    "Epoch: 8, Train loss: 0.114, Val loss: 0.115, Epoch time = 43.873s  \n",
    "Epoch: 9, Train loss: 0.112, Val loss: 0.113, Epoch time = 43.864s  \n",
    "Epoch: 10, Train loss: 0.111, Val loss: 0.112, Epoch time = 43.849s     \n",
    "Epoch: 11, Train loss: 0.110, Val loss: 0.111, Epoch time = 43.848s     \n",
    "Epoch: 12, Train loss: 0.108, Val loss: 0.109, Epoch time = 43.855s     \n",
    "Epoch: 13, Train loss: 0.107, Val loss: 0.109, Epoch time = 43.855s     \n",
    "Epoch: 14, Train loss: 0.106, Val loss: 0.108, Epoch time = 43.860s     \n",
    "Epoch: 15, Train loss: 0.105, Val loss: 0.107, Epoch time = 43.850s     \n",
    "\n"
   ],
   "id": "b7ec398714dd8912"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T19:50:11.232708Z",
     "start_time": "2024-07-03T19:50:11.209435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# see: https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html#save-and-load-the-model\n",
    "\n",
    "torch.save(model.state_dict(), \"/home/falaxdb/Repos/minus1/transformer_decoder_training/saved_files/saved_models/model_1_notebook_v6.pth\")"
   ],
   "id": "25be820f644f6aa0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "afc1ca1cbff3be56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T20:20:21.663324Z",
     "start_time": "2024-07-03T20:20:21.648590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformer_decoder_training.inference.inference_2 import inference\n",
    "\n",
    "# just do one single sequence\n",
    "for batch in test_loader:\n",
    "    # get single sequence\n",
    "    # blow it up to one batch again\n",
    "    sequence = torch.unsqueeze(batch[0], 0)\n",
    "    print(sequence.shape)\n",
    "    \n",
    "    # split into context sequence and truth sequence\n",
    "    context_seq = sequence[: ,:200]\n",
    "    continuing_seq = sequence[:, 200:]\n",
    "    \n",
    "    output_tokens, melody_output_tokens = inference(model, context_seq, continuing_seq, 0.25, pad_token, device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ],
   "id": "a77852c333e34a93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 513, 24])\n",
      "Next token shape: torch.Size([1, 24])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 14\u001B[0m\n\u001B[1;32m     11\u001B[0m context_seq \u001B[38;5;241m=\u001B[39m sequence[: ,:\u001B[38;5;241m200\u001B[39m]\n\u001B[1;32m     12\u001B[0m continuing_seq \u001B[38;5;241m=\u001B[39m sequence[:, \u001B[38;5;241m200\u001B[39m:]\n\u001B[0;32m---> 14\u001B[0m output_tokens, melody_output_tokens \u001B[38;5;241m=\u001B[39m \u001B[43minference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext_seq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontinuing_seq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.25\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpad_token\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Repos/minus1/transformer_decoder_training/inference/inference_2.py:38\u001B[0m, in \u001B[0;36minference\u001B[0;34m(model, context_sequence, true_continuing_sequence, threshold, pad_token, device)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Get the ground truth right hand:\u001B[39;00m\n\u001B[1;32m     37\u001B[0m ground_truth \u001B[38;5;241m=\u001B[39m true_continuing_sequence[\u001B[38;5;241m0\u001B[39m][i]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 38\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m ground_truth\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m next_token\u001B[38;5;241m.\u001B[39mshape  \u001B[38;5;66;03m# Ensure same dimensions\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Get right hand truth\u001B[39;00m\n\u001B[1;32m     41\u001B[0m right_hand_truth \u001B[38;5;241m=\u001B[39m ground_truth[:, midpoint:]\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
