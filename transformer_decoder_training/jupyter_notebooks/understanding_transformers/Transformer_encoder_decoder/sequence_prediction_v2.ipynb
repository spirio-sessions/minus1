{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Der Guide angepasst:\n",
    "\n",
    "https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "\n",
    "\n",
    "Die Schritte sind:\n",
    "\n",
    "1. Daten Generieren/Vorbereiten -> Auf die Modellhyperparameter achten -> müssen zu Daten passen\n",
    "    - Daten Sind sentences(sequenzen) zu bestimmten längen (z.B. 8), und sind in Batches vorliegend\n",
    "    - Daten Brauchen Start of Stream/ End of Stream tokens oder beide\n",
    "2. Modell Definieren\n",
    "    - Positional encoding selber definieren (Vorlage nehmen)\n",
    "    - Die Transformerstruktur definieren (Hier viele Bauteile von Pytorch verwenden)\n",
    "    - Das Masking-zeug selber definieren\n",
    "    - Das Padding zeug evtl selber definieren\n",
    "3. Training/Validation definieren\n",
    "    - Modell initialisieren\n",
    "    - Optimizer Festlegen\n",
    "    - Kostenfunktion festlegen\n",
    "    - Trainingsfunktion festlegen (Muss nicht alles in der Trainingsfunktion direkt passieren, aber es muss passieren)\n",
    "        - Es muss beim Training diese Verschiebung der Tokens passieren, dass der nächste output für eine Sequenz ausgegeben wird\n",
    "        - Target-tensor wird während der Prediction ans Modell gegeben\n",
    "        - Target-Maske muss Generiert werden \n",
    "        - Padding maske muss evtl auch generiert werden\n",
    "    - Validationfunktion festlegen\n",
    "        - Ist das gleiche wie im Training, nur werden hier keine Gradienten geupdated oder gelesen\n",
    "4. Training/Validation ausführen\n",
    "5. Inferenz ausführen"
   ],
   "id": "6b54b702e821cad5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 0. Imports",
   "id": "d71c4900d4f5afbd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:02:33.843311Z",
     "start_time": "2024-06-16T20:02:33.035180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ],
   "id": "6e79767c390e7b9",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "gpu nutzen",
   "id": "c92302f10ecfc911"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:02:33.876173Z",
     "start_time": "2024-06-16T20:02:33.844417Z"
    }
   },
   "cell_type": "code",
   "source": "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "id": "afd4112fac62e99a",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Daten Generieren/Vorbereiten",
   "id": "29921b4fb3c2a8b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:02:33.970325Z",
     "start_time": "2024-06-16T20:02:33.877039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_random_data(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "    length = 8\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 1,1,1,1,1,1 -> 1,1,1,1,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 0,0,0,0 -> 0,0,0,0\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1,0,1,0 -> 1,0,1,0,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.zeros(length)\n",
    "        start = random.randint(0, 1)\n",
    "\n",
    "        X[start::2] = 1\n",
    "\n",
    "        y = np.zeros(length)\n",
    "        if X[-1] == 0:\n",
    "            y[::2] = 1\n",
    "        else:\n",
    "            y[1::2] = 1\n",
    "\n",
    "        X = np.concatenate((SOS_token, X, EOS_token))\n",
    "        y = np.concatenate((SOS_token, y, EOS_token))\n",
    "\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
    "    batches = []\n",
    "    for idx in range(0, len(data), batch_size):\n",
    "        # We make sure we dont get the last bit if its not batch_size size\n",
    "        if idx + batch_size < len(data):\n",
    "            # Here you would need to get the max length of the batch,\n",
    "            # and normalize the length with the PAD token.\n",
    "            if padding:\n",
    "                max_batch_length = 0\n",
    "\n",
    "                # Get longest sentence in batch\n",
    "                for seq in data[idx : idx + batch_size]:\n",
    "                    if len(seq) > max_batch_length:\n",
    "                        max_batch_length = len(seq)\n",
    "\n",
    "                # Append X padding tokens until it reaches the max length\n",
    "                for seq_idx in range(batch_size):\n",
    "                    remaining_length = max_batch_length - len(data[idx + seq_idx])\n",
    "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
    "\n",
    "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
    "\n",
    "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "train_data = generate_random_data(9000)\n",
    "val_data = generate_random_data(3000)\n",
    "\n",
    "train_dataloader = batchify_data(train_data)\n",
    "val_dataloader = batchify_data(val_data)"
   ],
   "id": "54c6bd4ebbeb9915",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Modell Definieren",
   "id": "a801175386882075"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.1 Positional Encoding",
   "id": "997f1e95a52c0d10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:02:33.974015Z",
     "start_time": "2024-06-16T20:02:33.971024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Encoding - From formula -> This is basically applying the formula for Positional encoding (The one with Sinus and Cosinus)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # Baically a positions list 0, 1, 2, 3, 4, 5, ...\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        #  # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "         # Saving buffer (same as parameter without gradients needed)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Residual connection + pos encoding\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ],
   "id": "79ec44b1439ae451",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2 Transformer Definieren",
   "id": "c24ae8bf6c8c9952"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:02:33.982711Z",
     "start_time": "2024-06-16T20:02:33.974503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Hier werden glaube ich die Layer definiert. Ist Im guide glaube ich in anderer Reihenfolge -> hab sie jetzt in die gleiche Reihenfolge wie im guide gepackt\n",
    "        \n",
    "        ## Layers des Gesamten Modells\n",
    "        \n",
    "        # Positional Encoding zur Hinzufügung von Positionsinformationen zu den Token-Einbettungen\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        # Token-Einbettung für Quell- und Zielvokabular\n",
    "        # I use a nn.Embedding instead of the self defined TokenEmbedding\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, emb_size) \n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, emb_size)\n",
    "        \n",
    "        # Initialisierung des nn.Transformer Moduls mit den gegebenen Hyperparametern\n",
    "        self.transformer = nn.Transformer(d_model=emb_size,\n",
    "                                          nhead=nhead,\n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers,\n",
    "                                          dim_feedforward=dim_feedforward,\n",
    "                                          dropout=dropout,\n",
    "                                          batch_first=True)\n",
    "        \n",
    "        # Linearer Layer zur Projektion der Ausgabedimensionen auf die Zielvokabulargröße\n",
    "        # Generator ist also glaube ich die Outputlayer, die die Ausgabe in die Wahrscheinlichkeiten für die einzelnen Tokens übersetzt\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                # src_mask: Tensor,\n",
    "                tgt_mask=None,\n",
    "                src_padding_mask=None,\n",
    "                tgt_padding_mask=None):\n",
    "        \n",
    "        \n",
    "        # Einbettung und Positional Encoding für die Quellsequenz\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        \n",
    "        # Einbettung und Positional Encoding für die Zielsequenz\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        \n",
    "        # Hier bin ich noch etwas verwirrt, warum die dimensionen Permutiert werden müssen\n",
    "        # Aus der Erklärung für die batch_first variable von nn.Transformer:\n",
    "        # If True, then the input and output tensors are provided as (batch, seq, feature). Default: False (seq, batch, feature).\n",
    "        \n",
    "        # (deprecated) src_emb = src_emb.permute(1,0,2)\n",
    "        # (deprecated) tgt_emb = tgt_emb.permute(1,0,2)\n",
    "        #print(\"src_emb shape:\", src_emb.shape)\n",
    "        #print(\"tgt_emb shape:\", tgt_emb.shape)\n",
    "        \n",
    "        # Durchführen der Transformationsoperation\n",
    "        # src_emb und tgt_emb sind die eingebetteten Sequenzen mit Positionsinformationen\n",
    "        # src_mask und tgt_mask sind die Masken, die verhindern, dass zukünftige Tokens betrachtet werden\n",
    "        # src_padding_mask, tgt_padding_mask und memory_key_padding_mask sind die Masken für Padding-Tokens\n",
    "        outs = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask, src_key_padding_mask=src_padding_mask, tgt_key_padding_mask=tgt_padding_mask)\n",
    "        \n",
    "        # Projektion der Ausgabe auf die Zielvokabulargröße\n",
    "        return self.generator(outs)"
   ],
   "id": "c22aff0f9bcf307e",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.3 Masken definieren",
   "id": "ddb2a2c90deeb1ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:02:33.990871Z",
     "start_time": "2024-06-16T20:02:33.983119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    \n",
    "    # EX for size=5:\n",
    "    # [[0., -inf, -inf, -inf, -inf],\n",
    "    #  [0.,   0., -inf, -inf, -inf],\n",
    "    #  [0.,   0.,   0., -inf, -inf],\n",
    "    #  [0.,   0.,   0.,   0., -inf],\n",
    "    #  [0.,   0.,   0.,   0.,   0.]]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Die Funktion create_mask erstellt sowohl Quell- als auch Ziel-Pad-Masken, indem sie prüft, ob Elemente in der Quell- und Zielsequenz gleich dem Pad-Token sind. Diese Masken werden transponiert, um die richtige Dimension zu erhalten.\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ],
   "id": "e1ecc12a9ad3678f",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ".type(torch.bool)# 3. Training/Validation definieren",
   "id": "31a8c5ff665e2280"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.1 Hyperparameter und special tokens festlegen und Modell laden",
   "id": "1b8700e1acef26c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:02:34.002372Z",
     "start_time": "2024-06-16T20:02:33.991284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SRC_VOCAB_SIZE = 4 # Ist glaube ich das num_tokens aus dem Guide (Also wie viele Verschiedene Tokens es insgesamt gibt\n",
    "TGT_VOCAB_SIZE = 4 # Auch 4, da die eingabe und zielsequenz die Gleichen möglichkeiten für Tokens haben\n",
    "EMB_SIZE = 8 #die Dimesnion des Modells Die anzahl der Erwarteten features der inputs/outputs also quasi die anzahl der Wörter in einer sequenz glaube ich -> also 8 bei uns (hier werden die spezial-tokens nicht gezählt ?)\n",
    "NHEAD = 2 # Anzahl der heads in einem Attention block\n",
    "FFN_HID_DIM = 512 # Anzahl der hidden layers des Feed-forward networks \n",
    "BATCH_SIZE = 16 # wird nicht ans Modell weitergegeben. evtl für uns nicht wichtig, weil wir die Daten schon gebatcht haben?\n",
    "NUM_ENCODER_LAYERS = 3 # wie viele Encoder blöcke\n",
    "NUM_DECODER_LAYERS = 3 # wie viele Decoder Blöcke"
   ],
   "id": "ba77c2e5500cf758",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Die ganzen Tokens müssen auf die Daten abgestimmt werden",
   "id": "32d869ec1507f0dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:02:34.019681Z",
     "start_time": "2024-06-16T20:02:34.002879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "UNK_IDX = 5 #Brauchen wir glaube ich nicht\n",
    "PAD_IDX = 4 #Brauchen wir auch nicht, weil die sequenzen alle genau richtig lang sind\n",
    "BOS_IDX = 2 # So gesetzt wie im Guide beispiel mit den sequenzen\n",
    "EOS_IDX = 3 #auch gesetz wie im guide (hoffe ich)"
   ],
   "id": "8a0d245e69b7095a",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:02:34.492259Z",
     "start_time": "2024-06-16T20:02:34.020187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modell initialisiern\n",
    "transformer = Transformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "# auf GPU laden\n",
    "transformer = transformer.to(DEVICE)\n",
    "# Kostenfunktion als CrossEntropyLoss\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "# Optimizer als Adam optimizer festlegen\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ],
   "id": "8bc16b913f81df13",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.2 Trainingsloop defineren",
   "id": "e5d2b5021a977b80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:02:34.496360Z",
     "start_time": "2024-06-16T20:02:34.493474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        X, y = batch[:, 0], batch[:, 1]\n",
    "        X, y = torch.tensor(X).to(DEVICE), torch.tensor(y).to(DEVICE)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1]\n",
    "        y_expected = y[:,1:]\n",
    "        \n",
    "        #print(\"Training: X shape:\", X.shape)\n",
    "        #print(\"Training: y_input shape:\", y_input.shape)\n",
    "        #print(\"Training: y_expected shape:\", y_expected.shape)\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = y_input.size(1)\n",
    "        tgt_mask = generate_square_subsequent_mask(sequence_length).to(DEVICE)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        logits = model(X, y_input, tgt_mask)\n",
    "        \n",
    "        #print(\"Training: prediction (model output) shape:\", logits.shape)\n",
    "        \n",
    "        #(deprecated) Permute pred to have batch size first again\n",
    "        #(deprecated) pred = pred.permute(1, 2, 0)\n",
    "        # logits ist die Ausgabe des modells, y_expected ist die erwartete ausgabe\n",
    "        # Die dimensionen müssen verändert werden, da die loss funktion die Tensoren in anderer Form erwartet\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), y_expected.reshape(-1))\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "61c39ed83203c930",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.3 Validation loop definieren",
   "id": "7f9135e4fac3327e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:02:34.505002Z",
     "start_time": "2024-06-16T20:02:34.496848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch[:, 0], batch[:, 1]\n",
    "            X, y = torch.tensor(X, dtype=torch.long, device=DEVICE), torch.tensor(y, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = y[:,:-1]\n",
    "            y_expected = y[:,1:]\n",
    "            \n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = y_input.size(1)\n",
    "            tgt_mask = generate_square_subsequent_mask(sequence_length).to(DEVICE)\n",
    "\n",
    "            # Standard training except we pass in y_input and src_mask\n",
    "            logits = model(X, y_input, tgt_mask)\n",
    "\n",
    "            # Permute pred to have batch size first again\n",
    "            #pred = pred.permute(1, 2, 0)\n",
    "            # logits ist die Ausgabe des modells, y_expected ist die erwartete ausgabe\n",
    "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), y_expected.reshape(-1))\n",
    "            total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "c55ef43ed775f815",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Modell Trainieren",
   "id": "b4f2839bac2c2dc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:03:52.082219Z",
     "start_time": "2024-06-16T20:02:34.505561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_loop(transformer, optimizer, loss_fn, train_dataloader)\n",
    "    end_time = timer()\n",
    "    val_loss = validation_loop(transformer, loss_fn, val_dataloader)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ],
   "id": "fe08a8dbd2a7359e",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training: X shape: torch.Size([16, 10])\n",
    "- **X:** Dies ist die Eingabesequenz (Quelle) des Modells.\n",
    "  - `16`: Batchgröße (Anzahl der Sequenzen in einem Batch).\n",
    "  - `10`: Sequenzlänge (Anzahl der Token in jeder Sequenz).\n",
    "\n",
    "### Training: y_input shape: torch.Size([16, 9])\n",
    "- **y_input:** Dies ist die Zielsequenz, die als Eingabe für den Decoder verwendet wird, aber ohne das letzte Token (da das letzte Token nur zum Vergleich verwendet wird).\n",
    "  - `16`: Batchgröße.\n",
    "  - `9`: Sequenzlänge der Zielsequenz (eine weniger als die Quellsequenz, da das letzte Token entfernt wurde).\n",
    "\n",
    "### Training: y_expected shape: torch.Size([16, 9])\n",
    "- **y_expected:** Dies ist die Zielsequenz, gegen die das Modell bewertet wird. Es ist die Zielsequenz ohne das erste Token (typischerweise das Starttoken) und wird zum Berechnen des Verlusts verwendet.\n",
    "  - `16`: Batchgröße.\n",
    "  - `9`: Sequenzlänge der Zielsequenz (wie bei `y_input`).\n",
    "\n",
    "### src_emb shape: torch.Size([16, 10, 8])\n",
    "- **src_emb:** Dies ist die eingebettete Quellsequenz nach Anwendung des Positional Encodings.\n",
    "  - `16`: Batchgröße.\n",
    "  - `10`: Sequenzlänge der Quellsequenz.\n",
    "  - `8`: Einbettungsgröße (Anzahl der Merkmale pro Token nach der Einbettung).\n",
    "\n",
    "### tgt_emb shape: torch.Size([16, 9, 8])\n",
    "- **tgt_emb:** Dies ist die eingebettete Zielsequenz nach Anwendung des Positional Encodings.\n",
    "  - `16`: Batchgröße.\n",
    "  - `9`: Sequenzlänge der Zielsequenz.\n",
    "  - `8`: Einbettungsgröße.\n",
    "\n",
    "### Training: prediction logits (model output) shape: torch.Size([16, 9, 4])\n",
    "- **prediction:** Dies ist die Ausgabe des Modells nach der Transformation und Projektion auf die Zielvokabulargröße.\n",
    "  - `16`: Batchgröße.\n",
    "  - `9`: Sequenzlänge der Zielsequenz.\n",
    "  - `4`: Größe des Zielvokabulars (Anzahl der möglichen Token im Zielvokabular).\n",
    "\n",
    "### Erklärung der Tensorformen\n",
    "\n",
    "- **Batchgröße (16):** Anzahl der Sequenzen, die gleichzeitig verarbeitet werden.\n",
    "- **Sequenzlänge (10 für Quelle, 9 für Ziel):** Anzahl der Token in jeder Sequenz. Die Zielsequenz ist um eins kürzer, da das letzte Token entfernt wird, um als y_expected verwendet zu werden.\n",
    "- **Einbettungsgröße (8):** Dimension der eingebetteten Vektoren nach der Positional Encoding.\n",
    "- **Zielvokabulargröße (4):** Anzahl der möglichen Token im Zielvokabular. Dies ist die Anzahl der Klassen, die das Modell vorherzusagen versucht.\n",
    "\n",
    "### Zusammenfassung\n",
    "\n",
    "Die Batchgröße bleibt über alle Tensoren hinweg konsistent. Die Sequenzlänge der Eingabesequenzen (Quellsequenzen) ist etwas länger als die der Zielsequenzen, da die Zielsequenzen angepasst werden, um `y_input` und `y_expected` zu erzeugen. Die Einbettungsgröße bleibt gleich für Quell- und Zielsequenzen, während die Modellvorhersage die Form `(batch_size, seq_len, vocab_size)` hat, wobei `vocab_size` die Anzahl der Klassen im Zielvokabular ist."
   ],
   "id": "bf310e16b464b6d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Erklärung Kostenfunktion\n",
    "\n",
    "1. **Vorhersagen des Modells (logits):**\n",
    "   - `logits = model(X, y_input, tgt_mask)`\n",
    "   - Dies ist die Ausgabe des Modells mit der Form `(batch_size, seq_len, vocab_size)`.\n",
    "\n",
    "2. **Erwartete Zielwerte (y_expected):**\n",
    "   - `y_expected` ist die erwartete Ausgabe mit der Form `(batch_size, seq_len)`.\n",
    "\n",
    "3. **Anpassen der Dimensionen:**\n",
    "   - Um die `CrossEntropyLoss`-Funktion zu verwenden, müssen die Vorhersagen (`logits`) und die Zielwerte (`y_expected`) in bestimmten Formen vorliegen.\n",
    "\n",
    "### Berechnung der Kostenfunktion\n",
    "\n",
    "#### 1. Ausgabe des Modells reshapen:\n",
    "\n",
    "- `logits.reshape(-1, logits.shape[-1])`\n",
    "  - `logits` hat die Form `(batch_size, seq_len, vocab_size)`.\n",
    "  - `logits.reshape(-1, logits.shape[-1])` ändert die Form zu `(batch_size * seq_len, vocab_size)`.\n",
    "  - Dies ist notwendig, da die `CrossEntropyLoss`-Funktion die Form `(N, C)` erwartet, wobei `N` die Anzahl der Datenpunkte und `C` die Anzahl der Klassen ist.\n",
    "\n",
    "#### 2. Zielwerte reshapen:\n",
    "\n",
    "- `y_expected.reshape(-1)`\n",
    "  - `y_expected` hat die Form `(batch_size, seq_len)`.\n",
    "  - `y_expected.reshape(-1)` ändert die Form zu `(batch_size * seq_len)`.\n",
    "  - Dies ist notwendig, da die `CrossEntropyLoss`-Funktion die Form `(N)` erwartet, wobei `N` die Anzahl der Datenpunkte ist."
   ],
   "id": "63f4622e6882aa24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Inferenz",
   "id": "8288c26751e1fd8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:04:31.388720Z",
     "start_time": "2024-06-16T20:04:31.208144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(model, input_sequence, max_length=15, SOS_token=2, EOS_token=3):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=DEVICE)\n",
    "    \n",
    "    print(\"input sequence:\", input_sequence.shape)\n",
    "    print(\"y_input shape:\", y_input.shape)\n",
    "    \n",
    "    num_tokens = len(input_sequence[0])\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Get source mask\n",
    "        tgt_mask = generate_square_subsequent_mask(y_input.size(1)).to(DEVICE)\n",
    "        \n",
    "        pred = model(input_sequence, y_input, tgt_mask)\n",
    "        \n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
    "        next_item = torch.tensor([[next_item]], device=DEVICE)\n",
    "\n",
    "        # Concatenate previous input with predicted best word\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.view(-1).tolist()\n",
    "  \n",
    "  \n",
    "# Here we test some examples to observe how the model predicts\n",
    "examples = [\n",
    "    torch.tensor([[2, 1, 0, 0, 0, 1, 0, 0, 0, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 0, 1, 3]], dtype=torch.long, device=DEVICE)\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(transformer, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Continuation: {result[1:-1]}\")\n",
    "    print()"
   ],
   "id": "98c5c1c26f468619",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chat gpt erwartung wenn mit unseren Daten gearbeitet werden soll\n",
    "\n",
    "Wenn Ihre Eingangsdaten nun die Form `(16, 2, 10, 88)` haben, bedeutet dies, dass Sie nicht mehr nur Sequenzen von Nullen und Einsen (binares Format) verwenden, sondern Sequenzen von Arrays mit Länge 88. Diese Art von Datenstruktur könnte darauf hinweisen, dass Sie mit eingebetteten Sequenzen arbeiten, bei denen jedes Token in der Sequenz durch einen Vektor der Länge 88 dargestellt wird.\n",
    "\n",
    "### Anpassungen am Modell\n",
    "\n",
    "In diesem Fall müssen Sie sicherstellen, dass das Modell in der Lage ist, mit den eingebetteten Sequenzen der Länge 88 korrekt umzugehen. Hier sind die notwendigen Anpassungen:\n",
    "\n",
    "1. **Anpassung der Eingangsdatenform**: \n",
    "    - Die Eingangsdaten haben nun die Form `(batch_size, 2, seq_len, feature_dim)`, also `(16, 2, 10, 88)`.\n",
    "\n",
    "2. **Extrahieren der Quell- und Zielsequenzen**:\n",
    "    - Sie müssen sicherstellen, dass Sie die Quell- und Zielsequenzen korrekt extrahieren.\n",
    "\n",
    "3. **Anpassung der Einbettungsschichten**:\n",
    "    - Da die Eingabedaten bereits eingebettet sind, benötigen Sie möglicherweise keine zusätzlichen Einbettungsschichten (`nn.Embedding`) mehr.\n",
    "\n",
    "4. **Anpassung der linearen Schicht**:\n",
    "    - Die lineare Schicht muss entsprechend der Größe der eingebetteten Dimensionen angepasst werden.\n"
   ],
   "id": "561112803e8cfbac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
