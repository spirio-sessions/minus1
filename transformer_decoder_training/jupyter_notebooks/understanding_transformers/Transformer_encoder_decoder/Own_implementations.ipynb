{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Update\n",
    "\n",
    "Es ist Echt schwer die Vergleiche zwischen der Repo und dem Guide zu ziehen. Ich probiere jetzt nochmal Das gleiche wie im guide zu erreichen mit dem Vergleich aus dem Guide zur language translation -> siehe anderes Notebook"
   ],
   "id": "f31c1f7607907183"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# First own Implementation Test for Transformer Generally (not Transformer-decoder only)\n",
    "\n",
    "Ich Richte mich nach Folgenden Guides:\n",
    "\n",
    "- Eher Beginner Guide: https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "- Pytorch official word_language_model example: https://github.com/pytorch/examples/blob/86591de88a25944c2d1d72506f04af79b4301cc3/word_language_model\n",
    "- (Pytorch official Language Translation: https://pytorch.org/tutorials/beginner/translation_transformer.html) the other two are the main sources\n",
    "\n",
    "\n",
    "Ich benötige Folgende Dinge, um ein Modell zu trainieren:\n",
    "\n",
    "## Daten\n",
    "\n",
    "## Modell\n",
    "\n",
    "Ich glaube so dinge wie Das positional embedding und da Vektorembedding muss selbst implementiert werden. Es gibt kein Vorgefertigtes pytorch modul\n",
    "\n",
    "- Die Outputs müssen für den Decoder eins nach rechts geschoben werden (hier sind dann auch die Special Tokens wichtig\n",
    "- Die Die Eingaben in die Attention-Blöcke müssen von Vektoren zu Embedding vektoren umgewandelt werden\n",
    "- Dann das ganze Transformer zeug\n",
    "- Dann Linear Schicht\n",
    "- Dann Softmax schicht für die Wahrscheinlichkeiten"
   ],
   "id": "38497ec0f3d03860"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "bee9dc5131ca1095"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T15:48:31.515351Z",
     "start_time": "2024-06-12T15:48:31.513005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ],
   "id": "f25bd9655fa64322",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Wir Prauchen das Positional embedding, das den Tokens informationen über ihre Reihenfolge dazugibt\n",
    "\n",
    "This part is basically the same in the Guide and the official github repo. Repo has very slight differences in Code, Guide has comments:\n",
    "\n",
    "I think there is no implementation in Pytorch for thsi\n",
    "\n",
    "- Guide: https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "- pytorch example: https://github.com/pytorch/examples/blob/main/word_language_model/model.py"
   ],
   "id": "7a297e286ffb30b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T15:48:31.577781Z",
     "start_time": "2024-06-12T15:48:31.571170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Encoding - From formula -> This is basically applying the formula for Positional encoding (The one with Sinus and Cosinus)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # Baically a positions list 0, 1, 2, 3, 4, 5, ...\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        #  # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "         # Saving buffer (same as parameter without gradients needed)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Residual connection + pos encoding\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ],
   "id": "a7f3500cedfd5455",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Jetzt die Basic Transformer-Struktur Definieren\n",
    "\n",
    "Here are Big differnces between the Guide and the example repo im using the Code from the Repo and trying to understand and comment what it does with the help of the guide\n",
    "\n",
    "Ich habe keine Ahnung, ob hier das masking und Padding gemacht wird oder nicht\n",
    "\n",
    "Zumindest das masking muss selber gemacht werden glaube ich \n",
    "-> Wir müssen Das masking selber definieren"
   ],
   "id": "e3847c0a98ebdbc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T15:48:31.586831Z",
     "start_time": "2024-06-12T15:48:31.579900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerModel(nn.Transformer):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    # ntoken -> number of tokens -> Wahrscheinlich wie viele Verschiedene Tokens es zur eingabe gibt.\n",
    "    # z.B. wenn wir die Tokens 0 und 1 als daten haben und <EOS> (End of stream) und ein Padding token, dann haben wir 4 Tokens\n",
    "    \n",
    "    # d_model =  the number of expected features in the encoder/decoder inputs (default=512).\n",
    "    # -> variable nimp steht wahrscheinlich für number_of_inputs\n",
    "    # Wenn ich eine Sequenz von 8 Wörtern habe, dann muss mein d_model 8  sein. z.b. bei den Trainingsdaten 1, 0, 1, 0, 1, 0, 1, 0 → 1, 0, 1, 0, 1, 0, 1, 0\n",
    "    \n",
    "    # nhead (int) – the number of heads in the multiheadattention models (default=8).\n",
    "    \n",
    "    # dim_feedforward (int) – the dimension of the feedforward network model (default=2048).\n",
    "    # -> nhid steht wahrscheinlich für n_hidden_layers (welche dimension das feed forward network hat)\n",
    "    \n",
    "    # num_encoder_layers (int) – the number of sub-encoder-layers in the encoder (default=6).\n",
    "    # -> wie viele Encoder-blöcke es geben soll\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers)\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        \n",
    "        ## Hier werden die Layers definiert glaube ich\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout) # This is probably the Positional encoding defined earlier\n",
    "\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        # Hier wird evtl die Maske für Masked attention generiert?\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, mask=self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        "
   ],
   "id": "d5e131e81e70eb8b",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Getting Data\n",
    "\n",
    "Ich Versuche die Daten aus dem Guide zu nehmen, um zu schauen ob es mit dem Transformer aus dem Example aus der Repo funktioniert"
   ],
   "id": "c8d4f13b685a1388"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T15:48:31.659200Z",
     "start_time": "2024-06-12T15:48:31.588727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def generate_random_data(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "    length = 8\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 1,1,1,1,1,1 -> 1,1,1,1,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 0,0,0,0 -> 0,0,0,0\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1,0,1,0 -> 1,0,1,0,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.zeros(length)\n",
    "        start = random.randint(0, 1)\n",
    "\n",
    "        X[start::2] = 1\n",
    "\n",
    "        y = np.zeros(length)\n",
    "        if X[-1] == 0:\n",
    "            y[::2] = 1\n",
    "        else:\n",
    "            y[1::2] = 1\n",
    "\n",
    "        X = np.concatenate((SOS_token, X, EOS_token))\n",
    "        y = np.concatenate((SOS_token, y, EOS_token))\n",
    "\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
    "    batches = []\n",
    "    for idx in range(0, len(data), batch_size):\n",
    "        # We make sure we dont get the last bit if its not batch_size size\n",
    "        if idx + batch_size < len(data):\n",
    "            # Here you would need to get the max length of the batch,\n",
    "            # and normalize the length with the PAD token.\n",
    "            if padding:\n",
    "                max_batch_length = 0\n",
    "\n",
    "                # Get longest sentence in batch\n",
    "                for seq in data[idx : idx + batch_size]:\n",
    "                    if len(seq) > max_batch_length:\n",
    "                        max_batch_length = len(seq)\n",
    "\n",
    "                # Append X padding tokens until it reaches the max length\n",
    "                for seq_idx in range(batch_size):\n",
    "                    remaining_length = max_batch_length - len(data[idx + seq_idx])\n",
    "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
    "\n",
    "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
    "\n",
    "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "train_data = generate_random_data(9000)\n",
    "val_data = generate_random_data(3000)\n",
    "\n",
    "train_dataloader = batchify_data(train_data)\n",
    "val_dataloader = batchify_data(val_data)"
   ],
   "id": "b5eb48711b189194",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Jetzt das Modell Erstellen und trainieren\n",
    "\n",
    "gpu nutzen wenn möglich"
   ],
   "id": "7d01b86e2f63496d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T15:48:31.661202Z",
     "start_time": "2024-06-12T15:48:31.659647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)"
   ],
   "id": "dfbf5acc2393e568",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Die Parameter des Modells festlegen (Muss sehr wahrscheinlich an daten angepasst werden)\n",
    "\n",
    "hier muss model importiert werden? -> nein ist in der Repo nur in verschiedene Dateien aufgeteilt\n",
    "\n",
    "Im repo beispiel werden viele der Parameter als Argumente beim starten des Scripts übergeben. Deshalb die args.something_here im Code in der Repo\n",
    "\n",
    "Die Ganzen Hyperparameter für das Modell müssen noch evtl noch angepasst werden. Dafür evtl. Guide, Repo und offizielle Dokumentation vergleichen. Genau nachprüfen, was die Hyperparameter alle bedeuten.\n",
    "\n",
    "### Siehe Modelldefinition Kommentare für eventuelle Erklärung\n",
    "\n",
    "`args.emsize` ist in der Repo die size of word embeddings. Ist das das gleiche wie dim_model im Guide ?"
   ],
   "id": "677ba5c62fa19458"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T15:48:31.690910Z",
     "start_time": "2024-06-12T15:48:31.661723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_tokens = 4 # in der Repo als len(corpus.dictionary) -> so viele Wörter gibt es\n",
    "dim_model = 8\n",
    "num_heads=2 \n",
    "num_encoder_layers=3\n",
    "num_decoder_layers=3\n",
    "dropout_p=0.1\n",
    "\n",
    "num_hid = 128\n",
    "\n",
    "model = TransformerModel(num_tokens, dim_model, num_heads, nhid=num_hid, nlayers=num_encoder_layers, dropout=dropout_p).to(device)"
   ],
   "id": "7b758d4978deb326",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Jetzt ist das Modell erstellt und jetzt müssen Kostenfunktion und ein Optimizer festgelegt werden\n",
    "\n",
    "wird beides im Guide direkt nach erstellen des Modells festgelegt. \n",
    "\n",
    "ist das hier die Kostenfunktion in der Repo? `criterion = nn.NLLLoss()`\n",
    "\n",
    "wo ist der optimizer in der Repo?\n",
    "\n",
    "ich nehm mal das aus dem Guide"
   ],
   "id": "51342ffab12be878"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T15:48:32.274550Z",
     "start_time": "2024-06-12T15:48:31.691469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "id": "d45e697918d1e788",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Jetzt müssen Trainings und validation loop erstellt werden.",
   "id": "afbdc3fe2124d3ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Ich definiere zuerst den Trainings loop (ich halte mich an den aus dem Guide) -> Das repo model hat kein get_tgt_mask \n",
    "\n",
    "Wie wird das masking gemacht? \n",
    "\n",
    "ich probiere jetzt mal eine mischung aus dem guide und der Repo"
   ],
   "id": "d4c00c9162ff5d67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T15:48:37.218568Z",
     "start_time": "2024-06-12T15:48:37.215981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        X, y = batch[:, 0], batch[:, 1]\n",
    "        X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1]\n",
    "        y_expected = y[:,1:]\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = y_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "        # Permute pred to have batch size first again\n",
    "        pred = pred.permute(1, 2, 0)      \n",
    "        loss = loss_fn(pred, y_expected)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "ead2bc9b1490dac7",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dann wird der Validation-loop definiert",
   "id": "fb489ebe0e72aaae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T15:48:44.076686Z",
     "start_time": "2024-06-12T15:48:44.068774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch[:, 0], batch[:, 1]\n",
    "            X, y = torch.tensor(X, dtype=torch.long, device=device), torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = y[:,:-1]\n",
    "            y_expected = y[:,1:]\n",
    "            \n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = y_input.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "            # Standard training except we pass in y_input and src_mask\n",
    "            pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "            # Permute pred to have batch size first again\n",
    "            pred = pred.permute(1, 2, 0)      \n",
    "            loss = loss_fn(pred, y_expected)\n",
    "            total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "fabc31056087fa98",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Jetzt das Modell trainieren",
   "id": "9fac583325779125"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T15:50:15.422221Z",
     "start_time": "2024-06-12T15:50:15.282373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        \n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return train_loss_list, validation_loss_list\n",
    "    \n",
    "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_dataloader, val_dataloader, 10)"
   ],
   "id": "dd270d34ec1cfd14",
   "execution_count": 18,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
