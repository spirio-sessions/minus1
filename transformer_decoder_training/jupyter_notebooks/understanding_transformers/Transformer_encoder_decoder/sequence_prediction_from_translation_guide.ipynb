{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Versuch für Sequenze prediction Transformer\n",
    "\n",
    "Ich versuche jetzt das objective aus dem Guide: https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "\n",
    "mit der referenz aus dem offiziellen pytorch Tutorial: https://pytorch.org/tutorials/beginner/translation_transformer.html\n",
    "\n",
    "Die Schritte sind:\n",
    "\n",
    "1. Daten Generieren/Vorbereiten -> Auf die Modellhyperparameter achten -> müssen zu Daten passen\n",
    "    - Daten Sind sentences(sequenzen) zu bestimmten längen (z.B. 8), und sind in Batches vorliegend\n",
    "    - Daten Brauchen Start of Stream/ End of Stream tokens oder beide\n",
    "2. Modell Definieren\n",
    "    - Positional encoding selber definieren (Vorlage nehmen)\n",
    "    - Die Transformerstruktur definieren (Hier viele Bauteile von Pytorch verwenden)\n",
    "    - Das Masking-zeug selber definieren\n",
    "    - Das Padding zeug evtl selber definieren\n",
    "3. Training/Validation definieren\n",
    "    - Modell initialisieren\n",
    "    - Optimizer Festlegen\n",
    "    - Kostenfunktion festlegen\n",
    "    - Trainingsfunktion festlegen (Muss nicht alles in der Trainingsfunktion direkt passieren, aber es muss passieren)\n",
    "        - Es muss beim Training diese Verschiebung der Tokens passieren, dass der nächste output für eine Sequenz ausgegeben wird\n",
    "        - Target-tensor wird während der Prediction ans Modell gegeben\n",
    "        - Target-Maske muss Generiert werden \n",
    "        - Padding maske muss evtl auch generiert werden\n",
    "    - Validationfunktion festlegen\n",
    "        - Ist das gleiche wie im Training, nur werden hier keine Gradienten geupdated oder gelesen\n",
    "4. Training/Validation ausführen\n",
    "5. Inferenz ausführen"
   ],
   "id": "8c1fde1cb5cbe871"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0. Imports",
   "id": "bf34ff3fae0296e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:48.877064Z",
     "start_time": "2024-06-13T18:33:48.869529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ],
   "id": "9b6b7d8b3b7f5846",
   "execution_count": 133,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Gpu nutzen wenn möglich",
   "id": "2e3f5d778d8a1cb2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.070921Z",
     "start_time": "2024-06-13T18:33:49.069410Z"
    }
   },
   "cell_type": "code",
   "source": "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "id": "5f91607aebd7eb5",
   "execution_count": 134,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Daten Generieren/Vorbereiten\n",
    "\n",
    "Wir generieren Sequenzes für die Sequenze prediction. Sequenzen sollen so aussehen:\n",
    "\n",
    "- 1, 1, 1, 1, 1, 1, 1, 1 → 1, 1, 1, 1, 1, 1, 1, 1\n",
    "- 0, 0, 0, 0, 0, 0, 0, 0 → 0, 0, 0, 0, 0, 0, 0, 0\n",
    "- 1, 0, 1, 0, 1, 0, 1, 0 → 1, 0, 1, 0, 1, 0, 1, 0\n",
    "- 0, 1, 0, 1, 0, 1, 0, 1 → 0, 1, 0, 1, 0, 1, 0, 1\n",
    "\n",
    "\n",
    "\n",
    "- Alle Sequenzen haben die länge 8 -> kein Padding nötig\n",
    "- Sequenzen werden Zufällig in Batches der Größe 16 eingeteilt\n",
    "- Hier Werden auch die Start of Stream und End of stream Tokens Vorne und hinten and Die Sequenzen gehängt\n",
    "\n",
    "**Obacht: Unsere Daten sind anders angeordnet als die Daten aus dem Übersetzer Tutorial**"
   ],
   "id": "991fa218936d7ea0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.154526Z",
     "start_time": "2024-06-13T18:33:49.104460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_random_data(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "    length = 8\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 1,1,1,1,1,1 -> 1,1,1,1,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 0,0,0,0 -> 0,0,0,0\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1,0,1,0 -> 1,0,1,0,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.zeros(length)\n",
    "        start = random.randint(0, 1)\n",
    "\n",
    "        X[start::2] = 1\n",
    "\n",
    "        y = np.zeros(length)\n",
    "        if X[-1] == 0:\n",
    "            y[::2] = 1\n",
    "        else:\n",
    "            y[1::2] = 1\n",
    "\n",
    "        X = np.concatenate((SOS_token, X, EOS_token))\n",
    "        y = np.concatenate((SOS_token, y, EOS_token))\n",
    "\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
    "    batches = []\n",
    "    for idx in range(0, len(data), batch_size):\n",
    "        # We make sure we dont get the last bit if its not batch_size size\n",
    "        if idx + batch_size < len(data):\n",
    "            # Here you would need to get the max length of the batch,\n",
    "            # and normalize the length with the PAD token.\n",
    "            if padding:\n",
    "                max_batch_length = 0\n",
    "\n",
    "                # Get longest sentence in batch\n",
    "                for seq in data[idx : idx + batch_size]:\n",
    "                    if len(seq) > max_batch_length:\n",
    "                        max_batch_length = len(seq)\n",
    "\n",
    "                # Append X padding tokens until it reaches the max length\n",
    "                for seq_idx in range(batch_size):\n",
    "                    remaining_length = max_batch_length - len(data[idx + seq_idx])\n",
    "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
    "\n",
    "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
    "\n",
    "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "train_data = generate_random_data(9000)\n",
    "val_data = generate_random_data(3000)\n",
    "\n",
    "train_dataloader = batchify_data(train_data)\n",
    "val_dataloader = batchify_data(val_data)"
   ],
   "id": "9e5a330d99b547cf",
   "execution_count": 135,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Daten Anschauen\n",
    "\n",
    "train data:\n",
    "- Liste der Länge 9000\n",
    "    - Jedes Element ist weiter liste mit 2 Elementen\n",
    "        - Das eine Element ist Numpy-array der länge 10 -> 8 Tokens und die Eos und SOS tokens -> Source sequence\n",
    "        - Das andere ist Gleiches Array mit Target sequence"
   ],
   "id": "47cb102fcf7d6aad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.174199Z",
     "start_time": "2024-06-13T18:33:49.171525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Anzahl der Elemente in Trainingsdaten:\", len(train_data))\n",
    "\n",
    "print(\"Anzahl der Elemente in einem Trainingssatz\", len(train_data[0]))\n",
    "\n",
    "print(\"Dimension eines der Seqeunzen in einem Trainingssatz\", train_data[0][0].shape)"
   ],
   "id": "965c74e9e4593a9d",
   "execution_count": 136,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "train_dataloader sind die Trainingsdaten gebatched in zufällige batches von 16 unterteilt\n",
    "\n",
    "- liste der länge 562 (9000 / 16 = 562.5 -> der letze \"halbe\" batch wird nicht mehr genommen)\n",
    "    - Jedes Element ist ein numpy-array mit den Dimensionen (16, 2, 10)\n",
    "        - die Jeweiligen 16 Batches sind die 2 sequenzen die zusammengehören (2, 10)\n",
    "        - Die jeweiligen zusammengehörenden Sequenzen sind die insgesamt 10 Tokens"
   ],
   "id": "df9e98f641cb3043"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.211730Z",
     "start_time": "2024-06-13T18:33:49.207026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Anzahl der Batches:\", len(train_dataloader))\n",
    "\n",
    "print(\"Dimensionen in einem Batch:\", train_dataloader[0].shape)"
   ],
   "id": "db75b6d0a083caf1",
   "execution_count": 137,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Modell Definieren",
   "id": "76a3b616f86af53c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 Das Positional Encoding definieren\n",
    "\n",
    "Wird einfach die Sinus-cosinus Formel aus dem Attention is all you need paper in code gepackt\n",
    "\n",
    "Fügt informationen über die Word-order hinzu."
   ],
   "id": "96db558762ac8d9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.298135Z",
     "start_time": "2024-06-13T18:33:49.294801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Encoding - From formula -> This is basically applying the formula for Positional encoding (The one with Sinus and Cosinus)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # Baically a positions list 0, 1, 2, 3, 4, 5, ...\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        #  # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "         # Saving buffer (same as parameter without gradients needed)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Residual connection + pos encoding\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ],
   "id": "7ca765047e7b7f9",
   "execution_count": 138,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Die Transformerstruktur definieren\n",
    "\n",
    "ich versuche die Definition aus dem Pytorch tutorial Für den Guide anzupassen.\n",
    "\n",
    "Beachte: Die Definition der Layer ist hier nicht in der Schönen Gleichen Reihenfolge wie im Guide"
   ],
   "id": "906f9fb760f7a47c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.318079Z",
     "start_time": "2024-06-13T18:33:49.314862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        \n",
    "        # Hier werden glaube ich die Layer definiert. Ist Im guide glaube ich in anderer Reihenfolge -> hab sie jetzt in die gleiche Reihenfolge wie im guide gepackt\n",
    "        \n",
    "        ## Layers des Gesamten Modells\n",
    "        \n",
    "        # Positional Encoding zur Hinzufügung von Positionsinformationen zu den Token-Einbettungen\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        # Token-Einbettung für Quell- und Zielvokabular\n",
    "        # I use a nn.Embedding instead of the self defined TokenEmbedding\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, emb_size) \n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, emb_size)\n",
    "        \n",
    "        # Initialisierung des nn.Transformer Moduls mit den gegebenen Hyperparametern\n",
    "        self.transformer = nn.Transformer(d_model=emb_size,\n",
    "                                          nhead=nhead,\n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers,\n",
    "                                          dim_feedforward=dim_feedforward,\n",
    "                                          dropout=dropout)\n",
    "        \n",
    "        # Linearer Layer zur Projektion der Ausgabedimensionen auf die Zielvokabulargröße\n",
    "        # Generator ist also glaube ich die Outputlayer, die die Ausgabe in die Wahrscheinlichkeiten für die einzelnen Tokens übersetzt\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        \n",
    "        \n",
    "        # Einbettung und Positional Encoding für die Quellsequenz\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        \n",
    "        # Einbettung und Positional Encoding für die Zielsequenz\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        \n",
    "        # Durchführen der Transformationsoperation\n",
    "        # src_emb und tgt_emb sind die eingebetteten Sequenzen mit Positionsinformationen\n",
    "        # src_mask und tgt_mask sind die Masken, die verhindern, dass zukünftige Tokens betrachtet werden\n",
    "        # src_padding_mask, tgt_padding_mask und memory_key_padding_mask sind die Masken für Padding-Tokens\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        \n",
    "        # Projektion der Ausgabe auf die Zielvokabulargröße\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        # Einbettung und Positional Encoding für die Quellsequenz\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        \n",
    "        # Encoder-Durchlauf mit Quellsequenz und Maske\n",
    "        return self.transformer.encoder(src_emb, src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        # Einbettung und Positional Encoding für die Zielsequenz\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        \n",
    "        # Decoder-Durchlauf mit Zielsequenz, Gedächtnis und Maske\n",
    "        return self.transformer.decoder(tgt_emb, memory, tgt_mask)"
   ],
   "id": "8a226abd7fd5b4aa",
   "execution_count": 139,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 Maske definieren\n",
    "\n",
    "- Wir brauchen ein Maske für die masked attention layer(s), sodass das modell beim Training nicht Die zukunft sehen kann (Die zukünftigen Tokens in der Sequenz sieht)\n",
    "- Wir Brauche evtl. eine Maske, die Anzeigt, bei welchen Tokens es sich um padding-tokens handelt\n",
    "\n",
    "Da bei der Makendefinition die Special Tokens wichtig sind, müssen Die special Tokens Genau gekenzeichnet werden. -> Häng von den Daten ab, da dort meißt die special tokens Definiert werden.\n",
    "\n",
    "z.B. bei dem Übersetzungs-pytorch tutorial:\n",
    "- UNK_IDX -> wahrscheinlich für Unknown_Token: This index is returned when the token is not found\n",
    "- PAD_IDX -> Padding_index: Der Token, der Für fehlende Daten genutz wird, sodass die Dimensionen passen\n",
    "- BOS_IDX -> Beginning of Stream Token -> Zeigt an, dass jetzt die Sequenz startet.\n",
    "- EOS_IDX -> End of Stream Token -> Zeigt an, dass stream jetzt zuende ist.\n",
    "\n",
    "```\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)\n",
    "```"
   ],
   "id": "cd547dabba2ae153"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Bei Uns Wurden in der Datengenerierung für die Tokens folgendes gemacht:\n",
    "\n",
    "```\n",
    "SOS_token = np.array([2])\n",
    "EOS_token = np.array([3])\n",
    "```\n",
    "Also sollte der SOS (Start of stream)/ BOS (Beginning of stream) token ein numpy array mit einer 2 sein\n",
    "Der EOS token sollte ein numpy array mit einer 3 sein\n",
    "\n",
    "Also müssen wir unsere Tokens auch definieren:\n"
   ],
   "id": "36971fe323305bf3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.382194Z",
     "start_time": "2024-06-13T18:33:49.380576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "UNK_IDX = 5 #Brauchen wir glaube ich nicht\n",
    "PAD_IDX = 4 #Brauchen wir auch nicht, weil die sequenzen alle genau richtig lang sind\n",
    "BOS_IDX = 2 # So gesetzt wie im Guide beispiel mit den sequenzen\n",
    "EOS_IDX = 3 #auch gesetz wie im guide (hoffe ich)"
   ],
   "id": "8239a97a11e9ce2b",
   "execution_count": 140,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Jetzt die Funktionen für die masken selbst definieren: (Vorlage aus dem übersetzter Tutorial)",
   "id": "db2b873f4c0c4bea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.406006Z",
     "start_time": "2024-06-13T18:33:49.402543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generiert eine Obere Dreiecksmatrix, die in eine Untere Dreiecksmatrix transponiert wird\n",
    "# -> Deckt also nach und nach ein Token auf\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    \n",
    "    # EX for size=5:\n",
    "    # [[0., -inf, -inf, -inf, -inf],\n",
    "    #  [0.,   0., -inf, -inf, -inf],\n",
    "    #  [0.,   0.,   0., -inf, -inf],\n",
    "    #  [0.,   0.,   0.,   0., -inf],\n",
    "    #  [0.,   0.,   0.,   0.,   0.]]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Die Funktion create_mask erstellt sowohl Quell- als auch Ziel-Pad-Masken, indem sie prüft, ob Elemente in der Quell- und Zielsequenz gleich dem Pad-Token sind. Diese Masken werden transponiert, um die richtige Dimension zu erhalten.\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ],
   "id": "ce1a6eb6b73a190",
   "execution_count": 141,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3 Training/Validation definieren",
   "id": "8be22a1d692362be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1 Modell initialisieren\n",
    "\n",
    "Jetzt das Modell initialisieren mit den zu den Daten passenden Hyperparametern\n",
    "\n",
    "Hyperparameter:"
   ],
   "id": "fab7ecb042e98c6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.560454Z",
     "start_time": "2024-06-13T18:33:49.553182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SRC_VOCAB_SIZE = 4 # Ist glaube ich das num_tokens aus dem Guide (Also wie viele Verschiedene Tokens es insgesamt gibt\n",
    "TGT_VOCAB_SIZE = 4 # Auch 4, da die eingabe und zielsequenz die Gleichen möglichkeiten für Tokens haben\n",
    "EMB_SIZE = 8 #die Dimesnion des Modells Die anzahl der Erwarteten features der inputs/outputs also quasi die anzahl der Wörter in einer sequenz glaube ich -> also 8 bei uns (hier werden die spezial-tokens nicht gezählt ?)\n",
    "NHEAD = 2 # Anzahl der heads in einem Attention block\n",
    "FFN_HID_DIM = 512 # Anzahl der hidden layers des Feed-forward networks \n",
    "BATCH_SIZE = 16 # wird nicht ans Modell weitergegeben. evtl für uns nicht wichtig, weil wir die Daten schon gebatcht haben?\n",
    "NUM_ENCODER_LAYERS = 3 # wie viele Encoder blöcke\n",
    "NUM_DECODER_LAYERS = 3 # wie viele Decoder Blöcke"
   ],
   "id": "20ca41c6823f8051",
   "execution_count": 142,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Jetzt das modell selbst initialisieren und optimizer und Kostenfunktion festlegen",
   "id": "4a82f590dd377522"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.742623Z",
     "start_time": "2024-06-13T18:33:49.706199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modell initialisiern\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "# auf GPU laden\n",
    "transformer = transformer.to(DEVICE)\n",
    "# Kostenfunktion als CrossEntropyLoss\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "# Optimizer als Adam optimizer festlegen\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ],
   "id": "99b8b342cab43f49",
   "execution_count": 143,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Trainingsloop definieren\n",
    "\n",
    "Wichtige Unterschiede Zwischen Guide und Übersetzer Tutorial:\n",
    "\n",
    "- Definition 1:\n",
    "    - Führt die Vorhersage mit dem Modell durch und permutiert die Ausgabe, um die Batch-Dimension an die erste Stelle zu setzen (pred = pred.permute(1, 2, 0)).\n",
    "    - Berechnet den Verlust anhand der permutierten Vorhersage und der erwarteten Ausgabe (loss = loss_fn(pred, y_expected)).\n",
    "\n",
    "- Definition 2:\n",
    "\n",
    "    - Führt die Vorhersage mit dem Modell durch, wobei alle Masken übergeben werden (logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)).\n",
    "    - Berechnet den Verlust direkt aus den Vorhersagen (logits) und der Zielsequenz (tgt_out), indem die Dimensionen der Tensoren für die Verlustfunktion angepasst werden (loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1)))."
   ],
   "id": "508c5883ca8a5a43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.892053Z",
     "start_time": "2024-06-13T18:33:49.880160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        # Source und Target aus dem Dataloader nehem\n",
    "        src, tgt = batch[:,0], batch[:,1]\n",
    "        src, tgt = torch.tensor(src).to(DEVICE), torch.tensor(tgt).to(DEVICE)\n",
    "        \n",
    "        # tgt eins nach rechts verschieben, sodass mit dem Beginning of stream token der Token auf position 1 predicted wird\n",
    "        #\n",
    "        # obacht geben, könnte von den Dimensionen der Daten abhängig sein\n",
    "        #\n",
    "        tgt_input = tgt[:,:-1] # die eingehenden daten (Zielsequenz ohne Beginning of stream Token)\n",
    "        tgt_out = tgt[:,1:] # das gleiche wie y_expected (Zielsequenz ohne End of Stream Token)\n",
    "        print(\"tgt_input:\", tgt_input)\n",
    "        print(\"tgt_out:\", tgt_out)\n",
    "        \n",
    "        \n",
    "        # Maske erstellen, um die nächsten wörter zu maskieren\n",
    "        #\n",
    "        # Hier genau schauen, was passiert, ich brauche ja nur eine maske eigentlich\n",
    "        #\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        \n",
    "        # Verlust wird direkt aus den Logits (Vorhersagen) berechnet\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Hier wahrscheinlich loss berechnen und fehler durchpropagieren -> also training\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))"
   ],
   "id": "5685c900b0430005",
   "execution_count": 144,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 Validation Loop definieren",
   "id": "3cf795874123e860"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.985425Z",
     "start_time": "2024-06-13T18:33:49.982396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    \n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        # Source und Target aus dem Dataloader nehem\n",
    "        src, tgt = batch[:,0], batch[:,1]\n",
    "        src, tgt = torch.tensor(src).to(DEVICE), torch.tensor(tgt).to(DEVICE)\n",
    "        \n",
    "        # tgt eins nach rechts verschieben, sodass mit dem Beginning of stream token der Token auf position 1 predicted wird\n",
    "        #\n",
    "        # obacht geben, könnte von den Dimensionen der Daten abhängig sein\n",
    "        #\n",
    "        tgt_input = tgt[:-1,:] # die eingehenden daten\n",
    "        tgt_out = tgt[1:,:] # das gleiche wie y_expected\n",
    "        print(\"tgt_input:\", tgt_input)\n",
    "        print(\"tgt_out:\", tgt_out)\n",
    "        \n",
    "\n",
    "        # Maske erstellen, um die nächsten wörter zu maskieren\n",
    "        #\n",
    "        # Hier genau schauen, was passiert, ich brauche ja nur eine maske eigentlich\n",
    "        #\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        \n",
    "        # ich weiß nicht genau was hier passiert -> maske, etc wird wahrscheinlich ans modell gegeben\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ],
   "id": "420a1fc0085ec763",
   "execution_count": 145,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4 Modell Trainieren\n",
    "\n",
    "Hier sind noch nicht die Erwarteten test und validation loss werte. -> maske oder datenverschiebung evtl falsch.\n",
    "\n",
    "-> erst mal die inferenz abwarten."
   ],
   "id": "4c5d86236639d742"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:51.457129Z",
     "start_time": "2024-06-13T18:33:51.416559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
   ],
   "id": "60fc81da5ee9a790",
   "execution_count": 146,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Inferenz",
   "id": "31fae4ae6d14f0d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:58:38.069612Z",
     "start_time": "2024-06-13T17:57:04.318363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function for Greedy Decoding\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "    \n",
    "    # Memory ist glaube ich einfach die eingabe Encoded\n",
    "    memory = model.encode(src, src_mask)\n",
    "    \n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        print(\"target mask:\", tgt_mask)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(model: torch.nn.Module, src):\n",
    "    model.eval()\n",
    "    \n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(model, src, src_mask, max_len=num_tokens + 2, start_symbol=BOS_IDX).flatten()\n",
    "    \n",
    "    return tgt_tokens\n",
    "\n",
    "# Example usage with test sequences\n",
    "examples = [\n",
    "    torch.tensor([[2, 0, 0, 0, 0, 0, 0, 0, 0, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 0, 1, 3]], dtype=torch.long, device=DEVICE)\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(transformer, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Continuation: {result[1:-1]}\")\n",
    "    print()\n"
   ],
   "id": "31e0e7941f59861e",
   "execution_count": 58,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
