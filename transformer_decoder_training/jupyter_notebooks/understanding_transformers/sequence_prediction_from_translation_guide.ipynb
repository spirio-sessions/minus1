{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Versuch für Sequenze prediction Transformer\n",
    "\n",
    "Ich versuche jetzt das objective aus dem Guide: https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "\n",
    "mit der referenz aus dem offiziellen pytorch Tutorial: https://pytorch.org/tutorials/beginner/translation_transformer.html\n",
    "\n",
    "Die Schritte sind:\n",
    "\n",
    "1. Daten Generieren/Vorbereiten -> Auf die Modellhyperparameter achten -> müssen zu Daten passen\n",
    "    - Daten Sind sentences(sequenzen) zu bestimmten längen (z.B. 8), und sind in Batches vorliegend\n",
    "    - Daten Brauchen Start of Stream/ End of Stream tokens oder beide\n",
    "2. Modell Definieren\n",
    "    - Positional encoding selber definieren (Vorlage nehmen)\n",
    "    - Die Transformerstruktur definieren (Hier viele Bauteile von Pytorch verwenden)\n",
    "    - Das Masking-zeug selber definieren\n",
    "    - Das Padding zeug evtl selber definieren\n",
    "3. Training/Validation definieren\n",
    "    - Modell initialisieren\n",
    "    - Optimizer Festlegen\n",
    "    - Kostenfunktion festlegen\n",
    "    - Trainingsfunktion festlegen (Muss nicht alles in der Trainingsfunktion direkt passieren, aber es muss passieren)\n",
    "        - Es muss beim Training diese Verschiebung der Tokens passieren, dass der nächste output für eine Sequenz ausgegeben wird\n",
    "        - Target-tensor wird während der Prediction ans Modell gegeben\n",
    "        - Target-Maske muss Generiert werden \n",
    "        - Padding maske muss evtl auch generiert werden\n",
    "    - Validationfunktion festlegen\n",
    "        - Ist das gleiche wie im Training, nur werden hier keine Gradienten geupdated oder gelesen\n",
    "4. Training/Validation ausführen\n",
    "5. Inferenz ausführen"
   ],
   "id": "8c1fde1cb5cbe871"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0. Imports",
   "id": "bf34ff3fae0296e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:48.877064Z",
     "start_time": "2024-06-13T18:33:48.869529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ],
   "id": "9b6b7d8b3b7f5846",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Gpu nutzen wenn möglich",
   "id": "2e3f5d778d8a1cb2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.070921Z",
     "start_time": "2024-06-13T18:33:49.069410Z"
    }
   },
   "cell_type": "code",
   "source": "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "id": "5f91607aebd7eb5",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Daten Generieren/Vorbereiten\n",
    "\n",
    "Wir generieren Sequenzes für die Sequenze prediction. Sequenzen sollen so aussehen:\n",
    "\n",
    "- 1, 1, 1, 1, 1, 1, 1, 1 → 1, 1, 1, 1, 1, 1, 1, 1\n",
    "- 0, 0, 0, 0, 0, 0, 0, 0 → 0, 0, 0, 0, 0, 0, 0, 0\n",
    "- 1, 0, 1, 0, 1, 0, 1, 0 → 1, 0, 1, 0, 1, 0, 1, 0\n",
    "- 0, 1, 0, 1, 0, 1, 0, 1 → 0, 1, 0, 1, 0, 1, 0, 1\n",
    "\n",
    "\n",
    "\n",
    "- Alle Sequenzen haben die länge 8 -> kein Padding nötig\n",
    "- Sequenzen werden Zufällig in Batches der Größe 16 eingeteilt\n",
    "- Hier Werden auch die Start of Stream und End of stream Tokens Vorne und hinten and Die Sequenzen gehängt\n",
    "\n",
    "**Obacht: Unsere Daten sind anders angeordnet als die Daten aus dem Übersetzer Tutorial**"
   ],
   "id": "991fa218936d7ea0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.154526Z",
     "start_time": "2024-06-13T18:33:49.104460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_random_data(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "    length = 8\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 1,1,1,1,1,1 -> 1,1,1,1,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 0,0,0,0 -> 0,0,0,0\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1,0,1,0 -> 1,0,1,0,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.zeros(length)\n",
    "        start = random.randint(0, 1)\n",
    "\n",
    "        X[start::2] = 1\n",
    "\n",
    "        y = np.zeros(length)\n",
    "        if X[-1] == 0:\n",
    "            y[::2] = 1\n",
    "        else:\n",
    "            y[1::2] = 1\n",
    "\n",
    "        X = np.concatenate((SOS_token, X, EOS_token))\n",
    "        y = np.concatenate((SOS_token, y, EOS_token))\n",
    "\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
    "    batches = []\n",
    "    for idx in range(0, len(data), batch_size):\n",
    "        # We make sure we dont get the last bit if its not batch_size size\n",
    "        if idx + batch_size < len(data):\n",
    "            # Here you would need to get the max length of the batch,\n",
    "            # and normalize the length with the PAD token.\n",
    "            if padding:\n",
    "                max_batch_length = 0\n",
    "\n",
    "                # Get longest sentence in batch\n",
    "                for seq in data[idx : idx + batch_size]:\n",
    "                    if len(seq) > max_batch_length:\n",
    "                        max_batch_length = len(seq)\n",
    "\n",
    "                # Append X padding tokens until it reaches the max length\n",
    "                for seq_idx in range(batch_size):\n",
    "                    remaining_length = max_batch_length - len(data[idx + seq_idx])\n",
    "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
    "\n",
    "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
    "\n",
    "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "train_data = generate_random_data(9000)\n",
    "val_data = generate_random_data(3000)\n",
    "\n",
    "train_dataloader = batchify_data(train_data)\n",
    "val_dataloader = batchify_data(val_data)"
   ],
   "id": "9e5a330d99b547cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562 batches of size 16\n",
      "187 batches of size 16\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Daten Anschauen\n",
    "\n",
    "train data:\n",
    "- Liste der Länge 9000\n",
    "    - Jedes Element ist weiter liste mit 2 Elementen\n",
    "        - Das eine Element ist Numpy-array der länge 10 -> 8 Tokens und die Eos und SOS tokens -> Source sequence\n",
    "        - Das andere ist Gleiches Array mit Target sequence"
   ],
   "id": "47cb102fcf7d6aad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.174199Z",
     "start_time": "2024-06-13T18:33:49.171525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Anzahl der Elemente in Trainingsdaten:\", len(train_data))\n",
    "\n",
    "print(\"Anzahl der Elemente in einem Trainingssatz\", len(train_data[0]))\n",
    "\n",
    "print(\"Dimension eines der Seqeunzen in einem Trainingssatz\", train_data[0][0].shape)"
   ],
   "id": "965c74e9e4593a9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Elemente in Trainingsdaten: 9000\n",
      "Anzahl der Elemente in einem Trainingssatz 2\n",
      "Dimension eines der Seqeunzen in einem Trainingssatz (10,)\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "train_dataloader sind die Trainingsdaten gebatched in zufällige batches von 16 unterteilt\n",
    "\n",
    "- liste der länge 562 (9000 / 16 = 562.5 -> der letze \"halbe\" batch wird nicht mehr genommen)\n",
    "    - Jedes Element ist ein numpy-array mit den Dimensionen (16, 2, 10)\n",
    "        - die Jeweiligen 16 Batches sind die 2 sequenzen die zusammengehören (2, 10)\n",
    "        - Die jeweiligen zusammengehörenden Sequenzen sind die insgesamt 10 Tokens"
   ],
   "id": "df9e98f641cb3043"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.211730Z",
     "start_time": "2024-06-13T18:33:49.207026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Anzahl der Batches:\", len(train_dataloader))\n",
    "\n",
    "print(\"Dimensionen in einem Batch:\", train_dataloader[0].shape)"
   ],
   "id": "db75b6d0a083caf1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Batches: 562\n",
      "Dimensionen in einem Batch: (16, 2, 10)\n"
     ]
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Modell Definieren",
   "id": "76a3b616f86af53c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 Das Positional Encoding definieren\n",
    "\n",
    "Wird einfach die Sinus-cosinus Formel aus dem Attention is all you need paper in code gepackt\n",
    "\n",
    "Fügt informationen über die Word-order hinzu."
   ],
   "id": "96db558762ac8d9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.298135Z",
     "start_time": "2024-06-13T18:33:49.294801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Encoding - From formula -> This is basically applying the formula for Positional encoding (The one with Sinus and Cosinus)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # Baically a positions list 0, 1, 2, 3, 4, 5, ...\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        #  # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "         # Saving buffer (same as parameter without gradients needed)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Residual connection + pos encoding\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ],
   "id": "7ca765047e7b7f9",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Die Transformerstruktur definieren\n",
    "\n",
    "ich versuche die Definition aus dem Pytorch tutorial Für den Guide anzupassen.\n",
    "\n",
    "Beachte: Die Definition der Layer ist hier nicht in der Schönen Gleichen Reihenfolge wie im Guide"
   ],
   "id": "906f9fb760f7a47c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.318079Z",
     "start_time": "2024-06-13T18:33:49.314862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        \n",
    "        # Hier werden glaube ich die Layer definiert. Ist Im guide glaube ich in anderer Reihenfolge -> hab sie jetzt in die gleiche Reihenfolge wie im guide gepackt\n",
    "        \n",
    "        ## Layers des Gesamten Modells\n",
    "        \n",
    "        # Positional Encoding zur Hinzufügung von Positionsinformationen zu den Token-Einbettungen\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        # Token-Einbettung für Quell- und Zielvokabular\n",
    "        # I use a nn.Embedding instead of the self defined TokenEmbedding\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, emb_size) \n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, emb_size)\n",
    "        \n",
    "        # Initialisierung des nn.Transformer Moduls mit den gegebenen Hyperparametern\n",
    "        self.transformer = nn.Transformer(d_model=emb_size,\n",
    "                                          nhead=nhead,\n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers,\n",
    "                                          dim_feedforward=dim_feedforward,\n",
    "                                          dropout=dropout)\n",
    "        \n",
    "        # Linearer Layer zur Projektion der Ausgabedimensionen auf die Zielvokabulargröße\n",
    "        # Generator ist also glaube ich die Outputlayer, die die Ausgabe in die Wahrscheinlichkeiten für die einzelnen Tokens übersetzt\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        \n",
    "        \n",
    "        # Einbettung und Positional Encoding für die Quellsequenz\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        \n",
    "        # Einbettung und Positional Encoding für die Zielsequenz\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        \n",
    "        # Durchführen der Transformationsoperation\n",
    "        # src_emb und tgt_emb sind die eingebetteten Sequenzen mit Positionsinformationen\n",
    "        # src_mask und tgt_mask sind die Masken, die verhindern, dass zukünftige Tokens betrachtet werden\n",
    "        # src_padding_mask, tgt_padding_mask und memory_key_padding_mask sind die Masken für Padding-Tokens\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        \n",
    "        # Projektion der Ausgabe auf die Zielvokabulargröße\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        # Einbettung und Positional Encoding für die Quellsequenz\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        \n",
    "        # Encoder-Durchlauf mit Quellsequenz und Maske\n",
    "        return self.transformer.encoder(src_emb, src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        # Einbettung und Positional Encoding für die Zielsequenz\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        \n",
    "        # Decoder-Durchlauf mit Zielsequenz, Gedächtnis und Maske\n",
    "        return self.transformer.decoder(tgt_emb, memory, tgt_mask)"
   ],
   "id": "8a226abd7fd5b4aa",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 Maske definieren\n",
    "\n",
    "- Wir brauchen ein Maske für die masked attention layer(s), sodass das modell beim Training nicht Die zukunft sehen kann (Die zukünftigen Tokens in der Sequenz sieht)\n",
    "- Wir Brauche evtl. eine Maske, die Anzeigt, bei welchen Tokens es sich um padding-tokens handelt\n",
    "\n",
    "Da bei der Makendefinition die Special Tokens wichtig sind, müssen Die special Tokens Genau gekenzeichnet werden. -> Häng von den Daten ab, da dort meißt die special tokens Definiert werden.\n",
    "\n",
    "z.B. bei dem Übersetzungs-pytorch tutorial:\n",
    "- UNK_IDX -> wahrscheinlich für Unknown_Token: This index is returned when the token is not found\n",
    "- PAD_IDX -> Padding_index: Der Token, der Für fehlende Daten genutz wird, sodass die Dimensionen passen\n",
    "- BOS_IDX -> Beginning of Stream Token -> Zeigt an, dass jetzt die Sequenz startet.\n",
    "- EOS_IDX -> End of Stream Token -> Zeigt an, dass stream jetzt zuende ist.\n",
    "\n",
    "```\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)\n",
    "```"
   ],
   "id": "cd547dabba2ae153"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Bei Uns Wurden in der Datengenerierung für die Tokens folgendes gemacht:\n",
    "\n",
    "```\n",
    "SOS_token = np.array([2])\n",
    "EOS_token = np.array([3])\n",
    "```\n",
    "Also sollte der SOS (Start of stream)/ BOS (Beginning of stream) token ein numpy array mit einer 2 sein\n",
    "Der EOS token sollte ein numpy array mit einer 3 sein\n",
    "\n",
    "Also müssen wir unsere Tokens auch definieren:\n"
   ],
   "id": "36971fe323305bf3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.382194Z",
     "start_time": "2024-06-13T18:33:49.380576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "UNK_IDX = 5 #Brauchen wir glaube ich nicht\n",
    "PAD_IDX = 4 #Brauchen wir auch nicht, weil die sequenzen alle genau richtig lang sind\n",
    "BOS_IDX = 2 # So gesetzt wie im Guide beispiel mit den sequenzen\n",
    "EOS_IDX = 3 #auch gesetz wie im guide (hoffe ich)"
   ],
   "id": "8239a97a11e9ce2b",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Jetzt die Funktionen für die masken selbst definieren: (Vorlage aus dem übersetzter Tutorial)",
   "id": "db2b873f4c0c4bea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.406006Z",
     "start_time": "2024-06-13T18:33:49.402543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generiert eine Obere Dreiecksmatrix, die in eine Untere Dreiecksmatrix transponiert wird\n",
    "# -> Deckt also nach und nach ein Token auf\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    \n",
    "    # EX for size=5:\n",
    "    # [[0., -inf, -inf, -inf, -inf],\n",
    "    #  [0.,   0., -inf, -inf, -inf],\n",
    "    #  [0.,   0.,   0., -inf, -inf],\n",
    "    #  [0.,   0.,   0.,   0., -inf],\n",
    "    #  [0.,   0.,   0.,   0.,   0.]]\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Die Funktion create_mask erstellt sowohl Quell- als auch Ziel-Pad-Masken, indem sie prüft, ob Elemente in der Quell- und Zielsequenz gleich dem Pad-Token sind. Diese Masken werden transponiert, um die richtige Dimension zu erhalten.\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ],
   "id": "ce1a6eb6b73a190",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3 Training/Validation definieren",
   "id": "8be22a1d692362be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1 Modell initialisieren\n",
    "\n",
    "Jetzt das Modell initialisieren mit den zu den Daten passenden Hyperparametern\n",
    "\n",
    "Hyperparameter:"
   ],
   "id": "fab7ecb042e98c6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.560454Z",
     "start_time": "2024-06-13T18:33:49.553182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SRC_VOCAB_SIZE = 4 # Ist glaube ich das num_tokens aus dem Guide (Also wie viele Verschiedene Tokens es insgesamt gibt\n",
    "TGT_VOCAB_SIZE = 4 # Auch 4, da die eingabe und zielsequenz die Gleichen möglichkeiten für Tokens haben\n",
    "EMB_SIZE = 8 #die Dimesnion des Modells Die anzahl der Erwarteten features der inputs/outputs also quasi die anzahl der Wörter in einer sequenz glaube ich -> also 8 bei uns (hier werden die spezial-tokens nicht gezählt ?)\n",
    "NHEAD = 2 # Anzahl der heads in einem Attention block\n",
    "FFN_HID_DIM = 512 # Anzahl der hidden layers des Feed-forward networks \n",
    "BATCH_SIZE = 16 # wird nicht ans Modell weitergegeben. evtl für uns nicht wichtig, weil wir die Daten schon gebatcht haben?\n",
    "NUM_ENCODER_LAYERS = 3 # wie viele Encoder blöcke\n",
    "NUM_DECODER_LAYERS = 3 # wie viele Decoder Blöcke"
   ],
   "id": "20ca41c6823f8051",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Jetzt das modell selbst initialisieren und optimizer und Kostenfunktion festlegen",
   "id": "4a82f590dd377522"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.742623Z",
     "start_time": "2024-06-13T18:33:49.706199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modell initialisiern\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "# auf GPU laden\n",
    "transformer = transformer.to(DEVICE)\n",
    "# Kostenfunktion als CrossEntropyLoss\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "# Optimizer als Adam optimizer festlegen\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ],
   "id": "99b8b342cab43f49",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Trainingsloop definieren\n",
    "\n",
    "Wichtige Unterschiede Zwischen Guide und Übersetzer Tutorial:\n",
    "\n",
    "- Definition 1:\n",
    "    - Führt die Vorhersage mit dem Modell durch und permutiert die Ausgabe, um die Batch-Dimension an die erste Stelle zu setzen (pred = pred.permute(1, 2, 0)).\n",
    "    - Berechnet den Verlust anhand der permutierten Vorhersage und der erwarteten Ausgabe (loss = loss_fn(pred, y_expected)).\n",
    "\n",
    "- Definition 2:\n",
    "\n",
    "    - Führt die Vorhersage mit dem Modell durch, wobei alle Masken übergeben werden (logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)).\n",
    "    - Berechnet den Verlust direkt aus den Vorhersagen (logits) und der Zielsequenz (tgt_out), indem die Dimensionen der Tensoren für die Verlustfunktion angepasst werden (loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1)))."
   ],
   "id": "508c5883ca8a5a43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.892053Z",
     "start_time": "2024-06-13T18:33:49.880160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        # Source und Target aus dem Dataloader nehem\n",
    "        src, tgt = batch[:,0], batch[:,1]\n",
    "        src, tgt = torch.tensor(src).to(DEVICE), torch.tensor(tgt).to(DEVICE)\n",
    "        \n",
    "        # tgt eins nach rechts verschieben, sodass mit dem Beginning of stream token der Token auf position 1 predicted wird\n",
    "        #\n",
    "        # obacht geben, könnte von den Dimensionen der Daten abhängig sein\n",
    "        #\n",
    "        tgt_input = tgt[:,:-1] # die eingehenden daten (Zielsequenz ohne Beginning of stream Token)\n",
    "        tgt_out = tgt[:,1:] # das gleiche wie y_expected (Zielsequenz ohne End of Stream Token)\n",
    "        print(\"tgt_input:\", tgt_input)\n",
    "        print(\"tgt_out:\", tgt_out)\n",
    "        \n",
    "        \n",
    "        # Maske erstellen, um die nächsten wörter zu maskieren\n",
    "        #\n",
    "        # Hier genau schauen, was passiert, ich brauche ja nur eine maske eigentlich\n",
    "        #\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        \n",
    "        # Verlust wird direkt aus den Logits (Vorhersagen) berechnet\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Hier wahrscheinlich loss berechnen und fehler durchpropagieren -> also training\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))"
   ],
   "id": "5685c900b0430005",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 Validation Loop definieren",
   "id": "3cf795874123e860"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:49.985425Z",
     "start_time": "2024-06-13T18:33:49.982396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    \n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        # Source und Target aus dem Dataloader nehem\n",
    "        src, tgt = batch[:,0], batch[:,1]\n",
    "        src, tgt = torch.tensor(src).to(DEVICE), torch.tensor(tgt).to(DEVICE)\n",
    "        \n",
    "        # tgt eins nach rechts verschieben, sodass mit dem Beginning of stream token der Token auf position 1 predicted wird\n",
    "        #\n",
    "        # obacht geben, könnte von den Dimensionen der Daten abhängig sein\n",
    "        #\n",
    "        tgt_input = tgt[:-1,:] # die eingehenden daten\n",
    "        tgt_out = tgt[1:,:] # das gleiche wie y_expected\n",
    "        print(\"tgt_input:\", tgt_input)\n",
    "        print(\"tgt_out:\", tgt_out)\n",
    "        \n",
    "\n",
    "        # Maske erstellen, um die nächsten wörter zu maskieren\n",
    "        #\n",
    "        # Hier genau schauen, was passiert, ich brauche ja nur eine maske eigentlich\n",
    "        #\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        \n",
    "        # ich weiß nicht genau was hier passiert -> maske, etc wird wahrscheinlich ans modell gegeben\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ],
   "id": "420a1fc0085ec763",
   "outputs": [],
   "execution_count": 145
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4 Modell Trainieren\n",
    "\n",
    "Hier sind noch nicht die Erwarteten test und validation loss werte. -> maske oder datenverschiebung evtl falsch.\n",
    "\n",
    "-> erst mal die inferenz abwarten."
   ],
   "id": "4c5d86236639d742"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T18:33:51.457129Z",
     "start_time": "2024-06-13T18:33:51.416559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
   ],
   "id": "60fc81da5ee9a790",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt_input: tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [2, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [2, 0, 1, 0, 1, 0, 1, 0, 1],\n",
      "        [2, 0, 1, 0, 1, 0, 1, 0, 1],\n",
      "        [2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [2, 0, 1, 0, 1, 0, 1, 0, 1],\n",
      "        [2, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [2, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [2, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [2, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [2, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [2, 1, 0, 1, 0, 1, 0, 1, 0],\n",
      "        [2, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [2, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [2, 1, 0, 1, 0, 1, 0, 1, 0]], device='cuda:0')\n",
      "tgt_out: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 3],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 3],\n",
      "        [0, 1, 0, 1, 0, 1, 0, 1, 3],\n",
      "        [0, 1, 0, 1, 0, 1, 0, 1, 3],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
      "        [0, 1, 0, 1, 0, 1, 0, 1, 3],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 3],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 3],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 3],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 3],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 3],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
      "        [1, 0, 1, 0, 1, 0, 1, 0, 3],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 3],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 3],\n",
      "        [1, 0, 1, 0, 1, 0, 1, 0, 3]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "the batch number of src and tgt must be equal",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[146], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, NUM_EPOCHS\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m      5\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[0;32m----> 6\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[1;32m      8\u001B[0m     val_loss \u001B[38;5;241m=\u001B[39m evaluate(transformer)\n",
      "Cell \u001B[0;32mIn[144], line 27\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, optimizer)\u001B[0m\n\u001B[1;32m     24\u001B[0m src_mask, tgt_mask, src_padding_mask, tgt_padding_mask \u001B[38;5;241m=\u001B[39m create_mask(src, tgt_input)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Verlust wird direkt aus den Logits (Vorhersagen) berechnet\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43msrc_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_padding_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# Hier wahrscheinlich loss berechnen und fehler durchpropagieren -> also training\u001B[39;00m\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[139], line 59\u001B[0m, in \u001B[0;36mSeq2SeqTransformer.forward\u001B[0;34m(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\u001B[0m\n\u001B[1;32m     53\u001B[0m tgt_emb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpositional_encoding(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtgt_tok_emb(trg))\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# Durchführen der Transformationsoperation\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# src_emb und tgt_emb sind die eingebetteten Sequenzen mit Positionsinformationen\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# src_mask und tgt_mask sind die Masken, die verhindern, dass zukünftige Tokens betrachtet werden\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;66;03m# src_padding_mask, tgt_padding_mask und memory_key_padding_mask sind die Masken für Padding-Tokens\u001B[39;00m\n\u001B[0;32m---> 59\u001B[0m outs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m                        \u001B[49m\u001B[43msrc_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;66;03m# Projektion der Ausgabe auf die Zielvokabulargröße\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerator(outs)\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/transformer.py:210\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[1;32m    208\u001B[0m is_batched \u001B[38;5;241m=\u001B[39m src\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;129;01mand\u001B[39;00m src\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m!=\u001B[39m tgt\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[0;32m--> 210\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe batch number of src and tgt must be equal\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;129;01mand\u001B[39;00m src\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m!=\u001B[39m tgt\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[1;32m    212\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe batch number of src and tgt must be equal\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: the batch number of src and tgt must be equal"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Inferenz",
   "id": "31fae4ae6d14f0d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:58:38.069612Z",
     "start_time": "2024-06-13T17:57:04.318363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function for Greedy Decoding\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "    \n",
    "    # Memory ist glaube ich einfach die eingabe Encoded\n",
    "    memory = model.encode(src, src_mask)\n",
    "    \n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        print(\"target mask:\", tgt_mask)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(model: torch.nn.Module, src):\n",
    "    model.eval()\n",
    "    \n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(model, src, src_mask, max_len=num_tokens + 2, start_symbol=BOS_IDX).flatten()\n",
    "    \n",
    "    return tgt_tokens\n",
    "\n",
    "# Example usage with test sequences\n",
    "examples = [\n",
    "    torch.tensor([[2, 0, 0, 0, 0, 0, 0, 0, 0, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=DEVICE),\n",
    "    torch.tensor([[2, 0, 1, 3]], dtype=torch.long, device=DEVICE)\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(transformer, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Continuation: {result[1:-1]}\")\n",
    "    print()\n"
   ],
   "id": "31e0e7941f59861e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target mask: tensor([[False]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 2, 4]' is invalid for input of size 80",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 48\u001B[0m\n\u001B[1;32m     38\u001B[0m examples \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     39\u001B[0m     torch\u001B[38;5;241m.\u001B[39mtensor([[\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m3\u001B[39m]], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39mDEVICE),\n\u001B[1;32m     40\u001B[0m     torch\u001B[38;5;241m.\u001B[39mtensor([[\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m]], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39mDEVICE),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     44\u001B[0m     torch\u001B[38;5;241m.\u001B[39mtensor([[\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m]], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39mDEVICE)\n\u001B[1;32m     45\u001B[0m ]\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, example \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(examples):\n\u001B[0;32m---> 48\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExample \u001B[39m\u001B[38;5;132;01m{\u001B[39;00midx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexample\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mtolist()[\u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[58], line 33\u001B[0m, in \u001B[0;36mpredict\u001B[0;34m(model, src)\u001B[0m\n\u001B[1;32m     31\u001B[0m num_tokens \u001B[38;5;241m=\u001B[39m src\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     32\u001B[0m src_mask \u001B[38;5;241m=\u001B[39m (torch\u001B[38;5;241m.\u001B[39mzeros(num_tokens, num_tokens))\u001B[38;5;241m.\u001B[39mtype(torch\u001B[38;5;241m.\u001B[39mbool)\n\u001B[0;32m---> 33\u001B[0m tgt_tokens \u001B[38;5;241m=\u001B[39m \u001B[43mgreedy_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_tokens\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_symbol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBOS_IDX\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tgt_tokens\n",
      "Cell \u001B[0;32mIn[58], line 15\u001B[0m, in \u001B[0;36mgreedy_decode\u001B[0;34m(model, src, src_mask, max_len, start_symbol)\u001B[0m\n\u001B[1;32m     12\u001B[0m tgt_mask \u001B[38;5;241m=\u001B[39m (generate_square_subsequent_mask(ys\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m))\n\u001B[1;32m     13\u001B[0m             \u001B[38;5;241m.\u001B[39mtype(torch\u001B[38;5;241m.\u001B[39mbool))\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget mask:\u001B[39m\u001B[38;5;124m\"\u001B[39m, tgt_mask)\n\u001B[0;32m---> 15\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m out \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     17\u001B[0m prob \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerator(out[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "Cell \u001B[0;32mIn[49], line 77\u001B[0m, in \u001B[0;36mSeq2SeqTransformer.decode\u001B[0;34m(self, tgt, memory, tgt_mask)\u001B[0m\n\u001B[1;32m     74\u001B[0m tgt_emb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpositional_encoding(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtgt_tok_emb(tgt))\n\u001B[1;32m     76\u001B[0m \u001B[38;5;66;03m# Decoder-Durchlauf mit Zielsequenz, Gedächtnis und Maske\u001B[39;00m\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgt_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/transformer.py:494\u001B[0m, in \u001B[0;36mTransformerDecoder.forward\u001B[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[1;32m    491\u001B[0m tgt_is_causal \u001B[38;5;241m=\u001B[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001B[1;32m    493\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m--> 494\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmod\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mmemory_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mtgt_is_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_is_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mmemory_is_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_is_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    502\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(output)\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/transformer.py:891\u001B[0m, in \u001B[0;36mTransformerDecoderLayer.forward\u001B[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[1;32m    889\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    890\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001B[0;32m--> 891\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mha_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_is_causal\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    892\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm3(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ff_block(x))\n\u001B[1;32m    894\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/transformer.py:909\u001B[0m, in \u001B[0;36mTransformerDecoderLayer._mha_block\u001B[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001B[0m\n\u001B[1;32m    907\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_mha_block\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor, mem: Tensor,\n\u001B[1;32m    908\u001B[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 909\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmultihead_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    910\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    911\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    912\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    913\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout2(x)\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/modules/activation.py:1266\u001B[0m, in \u001B[0;36mMultiheadAttention.forward\u001B[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001B[0m\n\u001B[1;32m   1252\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmulti_head_attention_forward(\n\u001B[1;32m   1253\u001B[0m         query, key, value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads,\n\u001B[1;32m   1254\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1263\u001B[0m         average_attn_weights\u001B[38;5;241m=\u001B[39maverage_attn_weights,\n\u001B[1;32m   1264\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal)\n\u001B[1;32m   1265\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1266\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmulti_head_attention_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1267\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1268\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1269\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_v\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_zero_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1270\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1271\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1272\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1273\u001B[0m \u001B[43m        \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mneed_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1274\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1275\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1276\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[1;32m   1278\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), attn_output_weights\n",
      "File \u001B[0;32m~/Repos/Learn-ml/Transformer-pytorch/venv/lib64/python3.12/site-packages/torch/nn/functional.py:5410\u001B[0m, in \u001B[0;36mmulti_head_attention_forward\u001B[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001B[0m\n\u001B[1;32m   5408\u001B[0m q \u001B[38;5;241m=\u001B[39m q\u001B[38;5;241m.\u001B[39mview(tgt_len, bsz \u001B[38;5;241m*\u001B[39m num_heads, head_dim)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m   5409\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m static_k \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 5410\u001B[0m     k \u001B[38;5;241m=\u001B[39m \u001B[43mk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbsz\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhead_dim\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m   5411\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   5412\u001B[0m     \u001B[38;5;66;03m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001B[39;00m\n\u001B[1;32m   5413\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m static_k\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m==\u001B[39m bsz \u001B[38;5;241m*\u001B[39m num_heads, \\\n\u001B[1;32m   5414\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpecting static_k.size(0) of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbsz\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39mnum_heads\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstatic_k\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: shape '[1, 2, 4]' is invalid for input of size 80"
     ]
    }
   ],
   "execution_count": 58
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
